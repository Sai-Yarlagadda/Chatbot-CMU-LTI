Policy Representation via DiÔ¨Äusion Probability
Model for Reinforcement Learning 1
Long Yang1,‚àó, Zhixiong Huang 2,‚àó, Fenghao Lei2, Yucun Zhong3, Yiming Yang4,
Cong Fang1, Shiting Wen5, Binbin Zhou6, Zhouchen Lin1
1School of ArtiÔ¨Åcial Intelligence, Peking University, Beijing, China
2College of Computer Science and Technology, Zhejiang University, China
3MOE Frontiers Science Center for Brain and Brain-Machine Integration & College of
Computer Science, Zhejiang University, China.
4Institute of Automation Chinese Academy of Sciences Beijing, China
5School of Computer and Data Engineering, NingboTech University, China
6College of Computer Science and Technology, Hangzhou City University, China
{yanglong001,fangcong,zlin}@pku.edu.cn,
{zx.huang,lfh,yucunzhong}@zju.edu.cn, {wensht}@nit.zju.edu.cn
yangyiming2019@ia.ac.cn,bbzhou@hzcu.edu.cn
Code:https://github.com/BellmanTimeHut/DIPO
May 23, 2023
Abstract
Popular reinforcement learning (RL) algorithms tend to produce a unimodal policy
distribution, which weakens the expressiveness of complicated policy and decays the ability
of exploration. The diÔ¨Äusion probability model is powerful to learn complicated multimodal
distributions, which has shown promising and potential applications to RL.
In this paper, we formally build a theoretical foundation of policy representation via
the diÔ¨Äusion probability model and provide practical implementations of diÔ¨Äusion policy
for online model-free RL. Concretely, we character diÔ¨Äusion policy as a stochastic process
induced by stochastic diÔ¨Äerential equations, which is a new approach to representing a
policy. Then we present a convergence guarantee for diÔ¨Äusion policy, which provides a
theory to understand the multimodality of diÔ¨Äusion policy. Furthermore, we propose the
DIPO, which implements model-free online RL with DIÔ¨Äusion POlicy. To the best of our
knowledge, DIPO is the Ô¨Årst algorithm to solve model-free online RL problems with the
diÔ¨Äusion model. Finally, extensive empirical results show the eÔ¨Äectiveness and superiority
of DIPO on the standard continuous control Mujoco benchmark.
1* L.Yang and Z.Huang share equal contributions.
1
arXiv:2305.13122v1  [cs.LG]  22 May 2023
Contents
1
Introduction
3
1.1
Our Main Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2
Paper Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
2
Reinforcement Learning
5
3
Motivation: A View from Policy Representation
5
3.1
Policy Representation for Reinforcement Learning
. . . . . . . . . . . . . . . .
5
3.2
DiÔ¨Äusion Model is Powerful to Policy Representation . . . . . . . . . . . . . . .
6
4
DiÔ¨Äusion Policy
7
4.1
Stochastic Dynamics of DiÔ¨Äusion Policy . . . . . . . . . . . . . . . . . . . . . .
7
4.2
Exponential Integrator Discretization for DiÔ¨Äusion Policy
. . . . . . . . . . . .
8
4.3
Convergence Analysis of DiÔ¨Äusion Policy . . . . . . . . . . . . . . . . . . . . . .
9
5
DIPO: Implementation of DiÔ¨Äusion Policy for Model-Free Online RL
10
5.1
Training Loss of DIPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
5.2
Playing Action of DIPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
5.3
Policy Improvement of DIPO . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
6
Related Work
12
6.1
DiÔ¨Äusion Models for Reinforcement Learning
. . . . . . . . . . . . . . . . . . .
12
6.2
Generative Models for Policy Learning . . . . . . . . . . . . . . . . . . . . . . .
12
7
Experiments
13
7.1
Comparative Evaluation and Illustration . . . . . . . . . . . . . . . . . . . . . .
13
7.2
State-Visiting Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
7.3
Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
8
Conlusion
16
A Review on Notations
25
B Auxiliary Results
26
B.1
DiÔ¨Äusion Probability Model (DPM). . . . . . . . . . . . . . . . . . . . . . . . .
26
B.2
Transition Probability for Ornstein-Uhlenbeck Process . . . . . . . . . . . . . .
26
B.3
Exponential Integrator Discretization . . . . . . . . . . . . . . . . . . . . . . . .
27
B.4
Fokker‚ÄìPlanck Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
B.5
Donsker-Varadhan Representation for KL-divergence . . . . . . . . . . . . . . .
28
B.6
Some Basic Results for DiÔ¨Äusion Policy
. . . . . . . . . . . . . . . . . . . . . .
28
C Implementation Details of DIPO
30
C.1 DIPO: Model-Free Learning with DiÔ¨Äusion Policy . . . . . . . . . . . . . . . . .
31
C.2 Loss Function of DIPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
C.3 Playing Actions of DIPO
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
2
D Time Derivative of KL Divergence Between DifuÔ¨Äusion Policy and True
Reverse Process
35
D.1 Time Derivative of KL Divergence at Reverse Time k = 0 . . . . . . . . . . . .
35
D.2 Auxiliary Results For Reverse Time k = 0 . . . . . . . . . . . . . . . . . . . . .
35
D.3 Proof for Result at Reverse Time k = 0
. . . . . . . . . . . . . . . . . . . . . .
42
D.4 Proof for Result at Arbitrary Reverse Time k . . . . . . . . . . . . . . . . . . .
44
E Proof of Theorem 4.3
45
F Additional Details
48
F.1
Proof of Lemma F.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
F.2
Proof of Lemma D.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
G Details and Discussions for multimodal Experiments
50
G.1 Multimodal Environment
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
G.2 Plots Details of Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
G.3 Results Report . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
H Additional Experiments
55
H.1 Hyper-parameters for MuJoCo
. . . . . . . . . . . . . . . . . . . . . . . . . . .
55
H.2 Additional Tricks for Implementation of DIPO
. . . . . . . . . . . . . . . . . .
56
H.3 Details and Additional Reports for State-Visiting . . . . . . . . . . . . . . . . .
57
H.4 Ablation Study on MLP and VAE
. . . . . . . . . . . . . . . . . . . . . . . . .
60
1
Introduction
Existing policy representations (e.g., Gaussian distribution) for reinforcement learning (RL)
tend to output a unimodal distribution over the action space, which may be trapped in a
locally optimal solution due to its limited expressiveness of complex distribution and may
result in poor performance. DiÔ¨Äusion probability model [Sohl-Dickstein et al., 2015, Ho et al.,
2020, Song et al., 2021] is powerful to learn complicated multimodal distributions, which has
been applied to RL tasks (e.g., [Ajay et al., 2023, Reuss et al., 2023, Chi et al., 2023]).
Although the diÔ¨Äusion model (or diÔ¨Äusion policy) shows its promising and potential
applications to RL tasks, previous works are all empirical or only consider oÔ¨Ñine RL settings.
This raises some fundamental questions: How to character diÔ¨Äusion policy? How to show the
expressiveness of diÔ¨Äusion policy? How to design a diÔ¨Äusion policy for online model-free RL?
Those are the focuses of this paper.
¬Øa0
¬ØaT

Àúa0
ÀúaT
¬Øa1 ‚àº ¬ØœÄ1(¬∑|s)
¬Øat ‚àº ¬ØœÄt(¬∑|s)
¬ØaT ‚àº ¬ØœÄT (¬∑|s) ‚âà N(0, I)
¬Øa0 ‚àº œÄ(¬∑|s)
input
ouput
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑
Àúa0 ‚àº ÀúœÄ0(¬∑|s) = ¬ØœÄT (¬∑|s)
ÀúaT‚àít ‚àº ÀúœÄT‚àít(¬∑|s)
ÀúaT‚àí1 ‚àº ÀúœÄT‚àí1(¬∑|s)
ÀúaT ‚àº ÀúœÄT (¬∑|s)
¬∑ ¬∑ ¬∑
d¬Øat = ‚àí 1
2g(t)¬Øatdt + g(t)dwt
Forward SDE: a ‚àº œÄ(¬∑|s) ‚Üí N(0, I)
dÀúat = 1
2g2(T ‚àí t) [Àúat + 2‚àá log pT‚àít(Àúat)] dt + g(T ‚àí t)d ¬Øwt
Reverse SDE: N(0, I) ‚Üí œÄ(¬∑|s)
Figure 1: DiÔ¨Äusion Policy: Policy Representation via Stochastic Process. For a given state s,
the forward stochastic process {¬Øat|s} maps the input ¬Øa0 =: a ‚àº œÄ(¬∑|s) to be a noise; then we
recover the input by the stochastic process {Àúat|s} that reverses the reversed SDE if we know
the score function ‚àá log pt(¬∑), where pt(¬∑) is the probability distribution of the forward process,
i.e., pt(¬∑) = ¬ØœÄt(¬∑|s).
1.1
Our Main Work
In this paper, we mainly consider diÔ¨Äusion policy from the next three aspects.
Charactering DiÔ¨Äusion Policy as Stochastic Process. We formulate diÔ¨Äusion policy
as a stochastic process that involves two processes induced by stochastic diÔ¨Äerential equations
(SDE), see Figure 1, where the forward process disturbs the input policy œÄ to noise, then the
reverse process infers the policy œÄ according to a corresponding reverse SDE. Although this
view is inspired by the score-based generative model [Song et al., 2021], we provide a brand
new approach to represent a policy: via a stochastic process induced by SDE, neither via value
function nor parametric function. Under this framework, the diÔ¨Äusion policy is Ô¨Çexible to
3
œÄ
data D = {st, at, st+1, rt+1}
policy improvement
e.g.,Q-learning/SAC/PPO
œÄ ‚Üê œÄ
‚Ä≤
œÄ
‚Ä≤
Figure 2: Standard Training Framework for Model-free Online RL.
œÄ
D = {st, at, st+1, rt+1}
data
action gradient
œÄ ‚Üê œÄ
‚Ä≤
œÄ
‚Ä≤
diÔ¨Äusion policy
D
‚Ä≤ = {st, at}
at + Œ∑‚àáaQœÄ(st, at) ‚Üí at
Figure 3: Framework of DIPO: Implementation for Model-free Online RL with DIÔ¨Äusion
POlicy.
generate actions according to numerical SDE solvers.
Convergence Analysis of DiÔ¨Äusion Policy.
Under mild conditions, Theorem 4.3
presents a theoretical convergence guarantee for diÔ¨Äusion policy. The result shows that if the
score estimator is suÔ¨Éciently accurate, then diÔ¨Äusion policy eÔ¨Éciently infers the actions from
any realistic policy that generates the training data. It is noteworthy that Theorem 4.3 also
shows that diÔ¨Äusion policy is powerful to represent a multimodal distribution, which leads to
suÔ¨Écient exploration and better reward performance, Section 3 and Appendix G provide more
discussions with numerical veriÔ¨Åcations for this view.
DiÔ¨Äusion Policy for Model-free Online RL. Recall the standard model-free online
RL framework, see Figure 2, where the policy improvement produces a new policy œÄ
‚Ä≤ ‚™∞ œÄ
according to the data D. However, Theorem 4.3 illustrates that the diÔ¨Äusion policy only Ô¨Åts
the distribution of the policy œÄ but does not improve the policy œÄ. We can not embed the
diÔ¨Äusion policy into the standard RL training framework, i.e., the policy improvement in
Figure 2 can not be naively replaced by diÔ¨Äusion policy. To apply diÔ¨Äusion policy to model-free
online RL task, we propose the DIPO algorithm, see Figure 3. The proposed DIPO considers
a novel way for policy improvement, we call it action gradient that updates each at ‚àà D
along the gradient Ô¨Åeld (over the action space) of state-action value:
at ‚Üê at + Œ∑‚àáaQœÄ(st, at),
where for a given state s, QœÄ(s, a) measures the reward performance over the action space A.
Thus, DIPO improves the policy according to the actions toward to better reward performance.
To the best of our knowledge, this paper Ô¨Årst presents the idea of action gradient, which
provides an eÔ¨Écient way to make it possible to design a diÔ¨Äusion policy for online RL.
1.2
Paper Organization
Section 2 presents the background of reinforcement learning. Section 3 presents our motivation
from the view of policy representation. Section 4 presents the theory of diÔ¨Äusion policy. Section
5 presents the practical implementation of diÔ¨Äusion policy for model-free online reinforcement
learning. Section 7 presents the experiment results.
4
2
Reinforcement Learning
Reinforcement learning (RL)[Sutton and Barto, 2018] is formulated as Markov decision process
M = (S, A, P(¬∑), r, Œ≥, d0), where S is the state space; A ‚äÇ Rp is the continuous action space;
P(s
‚Ä≤|s, a) is the probability of state transition from s to s
‚Ä≤ after playing a; r(s‚Ä≤|s, a) denotes
the reward that the agent observes when the state transition from s to s
‚Ä≤ after playing a;
Œ≥ ‚àà (0, 1) is the discounted factor, and d0(¬∑) is the initial state distribution. A policy œÄ is a
probability distribution deÔ¨Åned on S √ó A, and œÄ(a|s) denotes the probability of playing a
in state s. Let {st, at, st+1, r(st+1|st, at)}t‚â•0 ‚àº œÄ be the trajectory sampled by the policy œÄ,
where s0 ‚àº d0(¬∑), at ‚àº œÄ(¬∑|st), st+1 ‚àº P(¬∑|st, at). The goal of RL is to Ô¨Ånd a policy œÄ such that
œÄ‚ãÜ =: arg maxœÄ EœÄ
P‚àû
t=0 Œ≥tr(st+1|st, at)

.
3
Motivation: A View from Policy Representation
In this section, we clarify our motivation from the view of policy representation: diÔ¨Äusion
model is powerful to policy representation, which leads to suÔ¨Écient exploration and better
reward performance.
3.1
Policy Representation for Reinforcement Learning
Value function and parametric function based are the main two approaches to represent policies,
while diÔ¨Äusion policy expresses a policy via a stochastic process (shown in Figure 1) that is
essentially diÔ¨Écult to the previous representation. In this section, we will clarify this view.
Additionally, we will provide an empirical veriÔ¨Åcation with a numerical experiment.
3.1.1
Policy Representation via Value Function
A typical way to represent policy is œµ-greedy policy [Sutton and Barto, 1998] or energy-based
policy [Sallans and Hinton, 2004, Peters et al., 2010],
œÄ(a|s) =
(
arg maxa‚Ä≤‚ààA QœÄ(s, a
‚Ä≤)
w.p. 1 ‚àí œµ;
randomly play a ‚àà A
w.p. œµ;
or œÄ(a|s) = exp {QœÄ(s, a)}
ZœÄ(s)
,
(1)
where
QœÄ(s, a) =: EœÄ
" ‚àû
X
t=0
Œ≥tr(st+1|st, at)|s0 = s, a0 = a
#
,
the normalization term ZœÄ(s) =
R
Rp exp {QœÄ(s, a)} da, and ‚Äúw.p.‚Äù is short for ‚Äúwith proba-
bility‚Äù. The representation (1) illustrates a connection between policy and value function,
which is widely used in value-based methods (e.g., SASRA [Rummery and Niranjan, 1994],
Q-Learning [Watkins, 1989], DQN [Mnih et al., 2015]) and energy-based methods (e.g., SQL
[Schulman et al., 2017a, Haarnoja et al., 2017, 2018a], SAC [Haarnoja et al., 2018b]).
3.1.2
Policy Representation via Parametric Function
Instead of consulting a value function, the parametric policy is to represent a policy by a
parametric function (e.g., neural networks), denoted as œÄŒ∏, where Œ∏ is the parameter. Policy
gradient theorem [Sutton et al., 1999, Silver et al., 2014] plays a center role to learn Œ∏, which
is fundamental in modern RL (e.g., TRPO [Schulman et al., 2015], DDPG [Lillicrap et al.,
2016], PPO [Schulman et al., 2017b], IMPALA [Espeholt et al., 2018], et al).
5
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
400
300
200
100
0
(a) DiÔ¨Äusion Policy
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
400
300
200
100
0
(b) SAC
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
300
200
100
0
(c) TD3
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
0.49
0.48
0.47
0.46
0.45
(d) PPO
Figure 5: Policy representation comparison of diÔ¨Äerent policies on multimodal environment.
3.1.3
Policy Representation via Stochastic Process
It is diÔ¨Äerent from both value-based and parametric policy representation; the diÔ¨Äusion policy
(see Figure 1) generates an action via a stochastic process, which is a fresh view for the RL
community. The diÔ¨Äusion model with RL Ô¨Årst appears in [Janner et al., 2022], where it
proposes the diÔ¨Äuser that plans by iteratively reÔ¨Åning trajectories, which is an essential oÔ¨Ñine
RL method. Ajay et al. [2023], Reuss et al. [2023] model a policy as a return conditional
diÔ¨Äusion model, Chen et al. [2023a], Wang et al. [2023], Chi et al. [2023] consider to generate
actions via diÔ¨Äusion model. The above methods are all to solve oÔ¨Ñine RL problems. To the
best of our knowledge, our proposed method is the Ô¨Årst diÔ¨Äusion approach to online model-free
reinforcement learning.
3.2
DiÔ¨Äusion Model is Powerful to Policy Representation
A3
A1
A2
A
œÄ(¬∑|s)
A
unimodal
multimodal
Figure 4: Unimodal Distribution
vs Multimodal Distribution.
This section presents the diÔ¨Äusion model is powerful to
represent a complex policy distribution.
by two follow-
ing aspects: 1) Ô¨Åtting a multimodal policy distribution is
eÔ¨Écient for exploration; 2) empirical veriÔ¨Åcation with a
numerical experiment.
The Gaussian policy is widely used in RL, which is
a unimodal distribution, and it plays actions around the
region of its mean center with a higher probability, i.e., the
red region A in Figure 4. The unimodal policy weakens the
expressiveness of complicated policy and decays the agent‚Äôs
ability to explore the environment. While for a multimodal
policy, it plays actions among the diÔ¨Äerent regions: A1‚à™A2‚à™A3. Compared to the unimodal
policy, the multimodal policy is powerful to explore the unknown world, making the agent
6
understand the environment eÔ¨Éciently and make a more reasonable decision.
We compare the ability of policy representation among SAC, TD3 [Fujimoto et al., 2018],
PPO and diÔ¨Äusion policy on the ‚Äúmulti-goal‚Äù environment [Haarnoja et al., 2017] (see Figure
5), where the x-axis and y-axis are 2D states, the four red dots denote the states of the goal
at (0, 5), (0, ‚àí5), (5, 0) and (‚àí5, 0) symmetrically. A reasonable policy should be able to take
actions uniformly to those four goal positions with the same probability, which characters
the capacity of exploration of a policy to understand the environment. In Figure 5, the red
arrowheads represent the directions of actions, and the length of the red arrowheads represents
the size of the actions. Results show that diÔ¨Äusion policy accurately captures a multimodal
distribution landscape, while both SAC, TD3, and PPO are not well suited to capture such a
multimodality. From the distribution of action direction and length, we also know the diÔ¨Äusion
policy keeps a more gradual and steady action size than the SAC, TD3, and PPO to Ô¨Åt the
multimodal distribution. For more details about 2D/3D plots, environment, comparisons, and
discussions, please refer to Appendix G.
4
DiÔ¨Äusion Policy
In this section, we present the details of diÔ¨Äusion policy from the following three aspects: its
stochastic dynamic equation (shown in Figure 1), discretization implementation, and Ô¨Ånite-time
analysis of its performance for the policy representation.
4.1
Stochastic Dynamics of DiÔ¨Äusion Policy
Recall Figure 1, we know diÔ¨Äusion policy contains two processes: forward process and reverse
process. We present its dynamic in this section.
Forward Process. To simplify the expression, we only consider g(t) =
‚àö
2, which is
parallel to the general setting in Figure 1. For any given state s, the forward process produces
a sequence {(¬Øat|s)}t=0:T that starting with ¬Øa0 ‚àº œÄ(¬∑|s), and it follows the Ornstein-Uhlenbeck
process (also known as Ornstein-Uhlenbeck SDE),
d¬Øat = ‚àí¬Øatdt +
‚àö
2dwt.
(2)
Let ¬Øat ‚àº ¬ØœÄt(¬∑|s) be the evolution distribution along the Ornstein-Uhlenbeck Ô¨Çow (2). According
to Proposition B.1 (see Appendix B.2), we know the conditional distribution of ¬Øat|¬Øa0 is
Gaussian,
¬Øat|¬Øa0 ‚àº N
 e‚àít¬Øa0,
 1 ‚àí e‚àí2t
I

.
(3)
That implies the forward process (2) transforms policy œÄ(¬∑|s) to the Gaussian noise N(0, I).
Reverse Process. For any given state s, if we reverse the stochastic process {(¬Øat|s)}t=0:T ,
then we obtain a process that transforms noise into the policy œÄ(¬∑|s). Concretely, we model the
policy as the process {(Àúat|s)}t=0:T according to the next Ornstein-Uhlenbeck process (running
forward in time),
dÀúat = (Àúat + 2‚àá log pT‚àít(Àúat)) dt +
‚àö
2dwt,
(4)
where pt(¬∑) is the probability density function of ¬ØœÄt(¬∑|s). Furthermore, according to [Anderson,
1982], with an initial action Àúa0 ‚àº ¬ØœÄT (¬∑|s), the reverse process {Àúat}t=0:T shares the same
7
Algorithm 1: DiÔ¨Äusion Policy with Exponential Integrator Discretization to Ap-
proximate œÄ(¬∑|s)
1: input: state s, horizon T, reverse length K, step-size h = T
K , score estimators
ÀÜS(¬∑, s, T ‚àí t);
2: initialization: a random action ÀÜa0 ‚àº N(0, I);
3: for k = 0, 1, ¬∑ ¬∑ ¬∑ , K ‚àí 1 do
4:
a random zk ‚àº N(0, I), set tk = hk;
5:
ÀÜatk+1 = ehÀÜatk +
 eh ‚àí 1
 
ÀÜatk + 2ÀÜS(ÀÜatk, s, T ‚àí tk)

+
‚àö
e2h ‚àí 1zk;
6: output: ÀÜatK;
distribution as the time-reversed version of the forward process {¬ØaT‚àít}t=0:T . That also implies
for all t = 0, 1, ¬∑ ¬∑ ¬∑ , T,
ÀúœÄt(¬∑|s) = ¬ØœÄT‚àít(¬∑|s),
if Àúa0 ‚àº ¬ØœÄT (¬∑|s).
(5)
Score Matching. The score function ‚àá log pT‚àít(¬∑) deÔ¨Åned in (4) is not explicit, we
consider an estimator ÀÜS(¬∑, s, T ‚àí t) to approximate the score function at a given state s. We
consider the next problem,
ÀÜS(¬∑, s, T ‚àí t) =: arg min
ÀÜs(¬∑)‚ààF Ea‚àºÀúœÄt(¬∑|s)
hÀÜs(a, s, t) ‚àí ‚àá log pT‚àít(a)
2
2
i
(6)
(5)
= arg min
ÀÜs(¬∑)‚ààF Ea‚àºÀúœÄt(¬∑|s)
hÀÜs(a, s, t) ‚àí ‚àá log ÀúœÄt(a|s)
2
2
i
,
(7)
where F is the collection of function approximators (e.g., neural networks). We will provide
the detailed implementations with a parametric function approximation later; please refer to
Section 5 or Appendix C.2.
4.2
Exponential Integrator Discretization for DiÔ¨Äusion Policy
In this section, we consider the implementation of the reverse process (4) with exponential
integrator discretization [Zhang, 2022, Lee et al., 2023]. Let h > 0 be the step-size, assume
reverse length K = T
h ‚àà N, and tk =: hk, k = 0, 1, ¬∑ ¬∑ ¬∑ , K. Then we give a partition on the
interval [0, T] as follows, 0 = t0 < t1 < ¬∑ ¬∑ ¬∑ < tk < tk+1 < ¬∑ ¬∑ ¬∑ < tK = T. Furthermore, we take
the discretization to the reverse process (4) according to the following equation,
dÀÜat =
 ÀÜat + 2ÀÜS(ÀÜatk, s, T ‚àí tk)

dt +
‚àö
2dwt, t ‚àà [tk, tk+1],
(8)
where it runs initially from ÀÜa0 ‚àº N(0, I). By ItÀÜo integration to the two sizes of (8) on the k-th
interval [tk, tk+1], we obtain the exact solution of the SDE (8), for each k = 0, 1, 2, ¬∑ ¬∑ ¬∑ , K ‚àí 1,
ÀÜatk+1 =ehÀÜatk +

eh ‚àí 1
  ÀÜatk + 2ÀÜS(ÀÜatk, s, T ‚àí tk)

+
p
e2h ‚àí 1zk, zk ‚àº N(0, I).
(9)
For the derivation from the SDE (8) to the iteraion (9), please refer to Appendix B.3, and we
have shown the implementation in Algorithm 1.
8
4.3
Convergence Analysis of DiÔ¨Äusion Policy
In this section, we present the convergence analysis of diÔ¨Äusion policy, we need the following
notations and assumptions before we further analyze. Let œÅ(x) and ¬µ(x) be two smooth
probability density functions on the space Rp, the Kullback‚ÄìLeibler (KL) divergence and
relative Fisher information (FI) from ¬µ(x) to œÅ(x) are deÔ¨Åned as follows,
KL(œÅ‚à•¬µ) =
Z
Rp
œÅ(x) log œÅ(x)
¬µ(x)dx, FI(œÅ‚à•¬µ) =
Z
Rp
œÅ(x)
‚àá log
 œÅ(x)
¬µ(x)

2
2
dx.
Assumption 4.1 (Lipschitz Score Estimator and Policy). The score estimator is Ls-Lipschitz
over action space A, and the policy œÄ(¬∑|s) is Lp-Lipschitz over action space A, i.e., for any a,
a
‚Ä≤ ‚àà A, the following holds,
‚à•ÀÜS(a, s, t) ‚àí ÀÜS(a
‚Ä≤, s, t)‚à• ‚â§ Ls‚à•a ‚àí a
‚Ä≤‚à•, ‚à•‚àá log œÄ(a|s) ‚àí ‚àá log œÄ(a
‚Ä≤|s)‚à• ‚â§ Lp‚à•a ‚àí a
‚Ä≤‚à•.
Assumption 4.2 (Policy with ŒΩ-LSI Setting). The policy œÄ(¬∑|s) satisÔ¨Åes ŒΩ-Log-Sobolev inequal-
ity (LSI) that deÔ¨Åned as follows, there exists constant ŒΩ > 0, for any probability distribution
¬µ(x) such that
KL(¬µ‚à•œÄ) ‚â§ 1
2ŒΩ FI(¬µ‚à•œÄ).
Assumption 4.1 is a standard setting for Langevin-based algorithms (e.g., [Wibisono and
Yang, 2022, Vempala and Wibisono, 2019]), and we extend it with RL notations. Assumption
4.2 presents the policy distribution class that we are concerned, which contains many complex
distributions that are not restricted to be log-concave, e.g. any slightly smoothed bound
distribution admits the condition (see [Ma et al., 2019, Proposition 1]).
Theorem 4.3 (Finite-time Analysis of DiÔ¨Äusion Policy). For a given state s, let {¬ØœÄt(¬∑|s)}t=0:T
and {ÀúœÄt(¬∑|s)}t=0:T be the distributions along the Ô¨Çow (2) and (4) correspondingly, where
{¬ØœÄt(¬∑|s)}t=0:T starts at ¬ØœÄ0(¬∑|s) = œÄ(¬∑|s) and {ÀúœÄt(¬∑|s)}t=0:T starts at ÀúœÄ0(¬∑|s) = ¬ØœÄT (¬∑|s). Let
ÀÜœÄk(¬∑|s) be the distribution of the iteration (9) at the k-th time tk = hk, i.e., ÀÜatk ‚àº ÀÜœÄk(¬∑|s)
denotes the diÔ¨Äusion policy (see Algorithm 1) at the time tk = hk. Let {ÀÜœÄk(¬∑|s)}k=0:K be
starting at ÀÜœÄ0(¬∑|s) = N(0, I), under Assumption 4.1 and 4.2, let the reverse length K ‚â•
T ¬∑ max

œÑ ‚àí1
0 , T‚àí1
0 , 12Ls, ŒΩ
	
, where constants œÑ0 and T0 will be special later. Then the KL-
divergence between diÔ¨Äusion policy ÀÜœÄK(¬∑|s) and input policy œÄ(¬∑|s) is upper-bounded as follows,
KL
 ÀÜœÄK(¬∑|s)‚à•œÄ(¬∑|s)

‚â§ e‚àí 9
4 ŒΩhKKL
 N(0, I)‚à•œÄ(¬∑|s)

|
{z
}
convergence of forward process (2)
+

64pLs
p
5/ŒΩ

h
|
{z
}
errors from discretization (9)
+20
3 œµscore,
where œµscore =
sup
(k,t)‚àà[K]√ó[tk,tk+1]

log Ea‚àºÀúœÄt(¬∑|s)

exp
ÀÜS(a, s, T ‚àí hk) ‚àí ‚àá log ÀúœÄt(a|s)

2
2

|
{z
}
errors from score matching (7)
.
Theorem 4.3 illustrates that the errors involve the following three terms. The Ô¨Årst error
involves KL
 N(0, I)‚à•œÄ(¬∑|s)

that represents how close the distribution of the input policy œÄ
is to the standard Gaussian noise, which is bounded by the exponential convergence rate of
Ornstein-Uhlenbeck Ô¨Çow (2) [Bakry et al., 2014, Wibisono and Jog, 2018, Chen et al., 2023c].
The second error is sourced from exponential integrator discretization implementation (9),
which scales with the discretization step-size h. The discretization error term implies a Ô¨Årst-
order convergence rate with respect to the discretization step-size h and scales polynomially
9
on other parameters. The third error is sourced from score matching (7), which represents how
close the score estimator ÀÜS is to the score function ‚àá log pT‚àít(¬∑) deÔ¨Åned in (4). That implies
for the practical implementation, the error from score matching could be suÔ¨Éciently small if
we Ô¨Ånd a good score estimator ÀÜS.
Furthermore, for any œµ > 0, if we Ô¨Ånd a good score estimator that makes the score
matching error satisfy œµscore <
1
20œµ, the step-size h = O

œµ‚àöŒΩ
pLs

, and reverse length K =
9
4ŒΩh log 3KL(N(0,I)‚à•œÄ(¬∑|s))
œµ
, then Theorem 4.3 implies the output of diÔ¨Äusion policy (ÀÜœÄK(¬∑|s) makes
a suÔ¨Écient close to the input policy œÄ(¬∑|s) with the measurement by KL(ÀÜœÄK(¬∑|s)‚à•œÄ(¬∑|s)) ‚â§ œµ.
5
DIPO: Implementation of DiÔ¨Äusion Policy for Model-Free
Online RL
In this section, we present the details of DIPO, which is an implementation of DIÔ¨Äusion
POlicy for model-free reinforcement learning. According to Theorem 4.3, diÔ¨Äusion policy
only Ô¨Åts the current policy œÄ that generates the training data (denoted as D), but it does
not improve the policy œÄ. It is diÔ¨Äerent from traditional policy-based RL algorithms, we can
not improve a policy according to the policy gradient theorem since diÔ¨Äusion policy is not a
parametric function but learns a policy via a stochastic process. Thus, we need a new way
to implement policy improvement, which is nontrivial. We have presented the framework
of DIPO in Figure 3, and shown the key steps of DIPO in Algorithm 2. For the detailed
implementation, please refer to Algorithm 3 (see Appendix C).
5.1
Training Loss of DIPO
It is intractable to directly apply the formulation (7) to estimate the score function since
‚àá log pt(¬∑) = ‚àá log ¬ØœÄt(¬∑|s) is unknown, which is sourced from the initial distribution ¬Øa0 ‚àº œÄ(¬∑|s)
is unknown. According to denoising score matching [Vincent, 2011, Hyv¬®arinen, 2005], a
practical way is to solve the next optimization problem (10). For any given s ‚àà S,
min
œÜ L(œÜ) = min
ÀÜsœÜ‚ààF
Z T
0
œâ(t)E¬Øa0‚àºœÄ(¬∑|s)E¬Øat|¬Øa0
hÀÜsœÜ(¬Øat, s, t) ‚àí ‚àá log œït(¬Øat|¬Øa0)
2
2
i
dt,
(10)
where œâ(t) : [0, T] ‚Üí R+ is a positive weighting function; œït(¬Øat|¬Øa0) = N
 e‚àít¬Øa0,
 1 ‚àí e‚àí2t
I

denotes the transition kernel of the forward process (3); E¬Øat|¬Øa0[¬∑] denotes the expectation
with respect to œït(¬Øat|¬Øa0); and œÜ is the parameter needed to be learned. Then, according to
Theorem C.1 (see Appendix C.2), we rewrite the objective (10) as follows,
L(œÜ) = Ek‚àºU({1,2,¬∑¬∑¬∑ ,K}),z‚àºN(0,I),(s,a)‚àºD

‚à•z ‚àí œµœÜ
 ‚àö¬ØŒ±ka + ‚àö1 ‚àí ¬ØŒ±kz, s, k

‚à•2
2

,
(11)
where U(¬∑) denotes uniform distribution,
œµœÜ (¬∑, ¬∑, k) = ‚àí‚àö1 ‚àí ¬ØŒ±kÀÜsœÜ (¬∑, ¬∑, T ‚àí tk) ,
and ¬ØŒ±k will be special. The objective (11) provides a way to learn œÜ from samples; see line
14-16 in Algorithm 2.
10
Algorithm 2: (DIPO) Model-Free Reinforcement Learning with DIÔ¨Äusion POlicy
1: initialize œÜ, critic network Qœà; {Œ±i}K
i=0; ¬ØŒ±k = Qk
i=1 Œ±i; step-size Œ∑;
2: repeat
3:
dataset D ‚Üê ‚àÖ; initialize s0 ‚àº d0(¬∑);
4:
#update experience
5:
for t = 0, 1, ¬∑ ¬∑ ¬∑ , T do
6:
play at follows (12); st+1 ‚àº P(¬∑|st, at); D ‚Üê D ‚à™ {st, at, st+1, r(st+1|st, at)};
7:
#update value function
8:
repeat N times
9:
sample (st, at, st+1, r(st+1|st, at)) ‚àº D i.i.d; take gradient descent on ‚ÑìQ(œà) (14);
10:
#action gradient
11:
for t = 0, 1, ¬∑ ¬∑ ¬∑ , T do
12:
replace each action at ‚àà D follows at ‚Üê at + Œ∑‚àáaQœà(st, a)|a=at;
13:
#update policy
14:
repeat N times
15:
sample (s, a) from D i.i.d, sample index k ‚àº U({1, ¬∑ ¬∑ ¬∑ , K}), z ‚àº N(0, I);
16:
take gradient decent on the loss ‚Ñìd(œÜ) = ‚à•z ‚àí œµœÜ(‚àö¬ØŒ±ka + ‚àö1 ‚àí ¬ØŒ±kz, s, k)‚à•2
2;
17: until the policy performs well in the environment.
5.2
Playing Action of DIPO
Replacing the score estimator ÀÜS (deÔ¨Åned in Algorithm 1) according to ÀÜœµœÜ, after some algebras
(see Appendix C.3), we rewrite diÔ¨Äusion policy (i.e., Algorithm 1) as follows,
ÀÜak+1 =
1
‚àöŒ±k

ÀÜak ‚àí 1 ‚àí Œ±k
‚àö1 ‚àí ¬ØŒ±k
œµœÜ(ÀÜak, s, k)

+
r
1 ‚àí Œ±k
Œ±k
zk,
(12)
where k = 0, 1, ¬∑ ¬∑ ¬∑ , K ‚àí 1 runs forward in time, the noise zk ‚àº N(0, I). The agent plays the
last (output) action ÀÜaK.
5.3
Policy Improvement of DIPO
According to (11), we know that only the state-action pairs (s, a) ‚àà D are used to learn a
policy. That inspires us that if we design a method that transforms a given pair (s, a) ‚àà D
to be a ‚Äúbetter‚Äù pair, then we use the ‚Äúbetter‚Äù pair to learn a new diÔ¨Äusion policy œÄ
‚Ä≤, then
œÄ
‚Ä≤ ‚™∞ œÄ. About ‚Äúbetter‚Äù state-action pair should maintain a higher reward performance than
the originally given pair (s, a) ‚àà D. We break our key idea into two steps: 1) Ô¨Årst, we regard
the reward performance as a function with respect to actions, JœÄ(a) = Es‚àºd0(¬∑)[QœÄ(s, a)], which
quantiÔ¨Åes how the action a aÔ¨Äects the performance; 2) then, we update all the actions a ‚àà D
through the direction ‚àáaJœÄ(a) by gradient ascent method:
a ‚Üê a + Œ∑‚àáaJœÄ(a) = a + Œ∑Es‚àºd0(¬∑)[‚àáaQœÄ(s, a)],
(13)
where Œ∑ > 0 is step-size, and we call ‚àáaJœÄ(a) as action gradient. To implement (13) from
samples, we need a neural network Qœà to estimate QœÄ. Recall {st, at, st+1, r(st+1|st, at)}t‚â•0 ‚àº
œÄ, we train the parameter œà by minimizing the following Bellman residual error,
‚ÑìQ(œà) =
 r(st+1|st, at) + Œ≥Qœà(st+1, at+1) ‚àí Qœà(st, at)
2.
(14)
11
Finally, we consider each pair (st, at) ‚àà D, and replace the action at ‚àà D as follows,
at ‚Üê at + Œ∑‚àáaQœà(st, a)|a=at.
(15)
6
Related Work
Due to the diÔ¨Äusion model being a fast-growing Ô¨Åeld, this section only presents the work that
relates to reinforcement learning, a recent work [Yang et al., 2022] provides a comprehensive
survey on the diÔ¨Äusion model. In this section, Ô¨Årst, we review recent advances in diÔ¨Äusion
models with reinforcement learning. Then, we review the generative models for reinforcement
learning.
6.1
DiÔ¨Äusion Models for Reinforcement Learning
The diÔ¨Äusion model with RL Ô¨Årst appears in [Janner et al., 2022], where it proposes the
diÔ¨Äuser that plans by iteratively reÔ¨Åning trajectories, which is an essential oÔ¨Ñine RL method.
Later Ajay et al. [2023] model a policy as a return conditional diÔ¨Äusion model, Chen et al.
[2023a], Wang et al. [2023], Chi et al. [2023] consider to generate actions via diÔ¨Äusion model.
SE(3)-diÔ¨Äusion Ô¨Åelds [Urain et al., 2023] consider learning data-driven SE(3) cost functions
as diÔ¨Äusion models. Pearce et al. [2023] model the imitating human behavior with diÔ¨Äusion
models. Reuss et al. [2023] propose score-based diÔ¨Äusion policies for the goal-conditioned
imitation learning problems. ReorientDiÔ¨Ä [Mishra and Chen, 2023] presents a reorientation
planning method that utilizes a diÔ¨Äusion model-based approach. StructDiÔ¨Äusion [Liu et al.,
2022] is an object-centric transformer with a diÔ¨Äusion model, based on high-level language
goals, which constructs structures out of a single RGB-D image. Brehmer et al. [2023] propose
an equivariant diÔ¨Äuser for generating interactions (EDGI), which trains a diÔ¨Äusion model on an
oÔ¨Ñine trajectory dataset, where EDGI learns a world model and planning in it as a conditional
generative modeling problem follows the diÔ¨Äuser [Janner et al., 2022]. DALL-E-Bot [Kapelyukh
et al., 2022] explores the web-scale image diÔ¨Äusion models for robotics. AdaptDiÔ¨Äuser [Liang
et al., 2023] is an evolutionary planning algorithm with diÔ¨Äusion, which is adapted to unseen
tasks.
The above methods are all to solve oÔ¨Ñine RL problems, to the best of our knowledge, the
proposed DIPO is the Ô¨Årst diÔ¨Äusion approach to solve online model-free RL problems. The
action gradient plays a critical way to implement DIPO, which never appears in existing RL
literature. In fact, the proposed DIPO shown in Figure 3 is a general training framework for
RL, where we can replace the diÔ¨Äusion policy with any function Ô¨Åtter (e.g., MLP or VAE).
6.2
Generative Models for Policy Learning
In this section, we mainly review the generative models, including VAE [Kingma and Welling,
2013], GAN [Goodfellow et al., 2020], Flow [Rezende and Mohamed, 2015], and GFlowNet
[Bengio et al., 2021a,b] for policy learning. Generative models are mainly used in cloning
diverse behaviors [Pomerleau, 1988], imitation learning [Osa et al., 2018], goal-conditioned
imitation learning [Argall et al., 2009], or oÔ¨Ñine RL [Levine et al., 2020], a recent work [Yang
et al., 2023] provides a foundation presentation for the generative models for policy learning.
VAE for Policy Learning. Lynch et al. [2020], Ajay et al. [2021] have directly applied
auto-encoding variational Bayes (VAE) [Kingma and Welling, 2013] and VQ-VAE [Van
Den Oord et al., 2017] model behavioral priors. Mandlekar et al. [2020] design the low-level
12
policy that is conditioned on latent from the CVAE. Pertsch et al. [2021] joint the representation
of skill embedding and skill prior via a deep latent variable model. Mees et al. [2022], Rosete-
Beas et al. [2023] consider seq2seq CVAE [Lynch et al., 2020, Wang et al., 2022] to model of
conditioning the action decoder on the latent plan allows the policy to use the entirety of its
capacity for learning unimodal behavior.
GAN for Imitation Learning. GAIL [Ho and Ermon, 2016] considers the Generative
Adversarial Networks (GANs) [Goodfellow et al., 2020] to imitation learning. These methods
consist of a generator and a discriminator, where the generator policy learns to imitate the
experts‚Äô behaviors, and the discriminator distinguishes between real and fake trajectories,
which models the imitation learning as a distribution matching problem between the expert
policy‚Äôs state-action distribution and the agent‚Äôs policy [Fu et al., 2018, Wang et al., 2021].
For several advanced results and applications, please refer to [Chen et al., 2023b, Deka et al.,
2023, Rafailov et al., Taranovic et al., 2023].
Flow and GFlowNet Model for Policy Learning. Singh et al. [2020] consider normal-
izing Ô¨Çows [Rezende and Mohamed, 2015] for the multi-task RL tasks. Li et al. [2023a] propose
diverse policy optimization, which consider the GFlowNet [Bengio et al., 2021a,b] for the
structured action spaces. Li et al. [2023b] propose CFlowNets that combines GFlowNet with
continuous control. Stochastic GFlowNet [Pan et al., 2023] learns a model of the environment
to capture the stochasticity of state transitions. Malkin et al. [2022] consider training a
GFlowNet with trajectory balance.
Other Methods. Decision Transformer (DT) [Chen et al., 2021] model the oÔ¨Ñine RL
tasks as a conditional sequence problem, which does not learn a policy follows the traditional
methods (e.g., Sutton [1988], Sutton and Barto [1998]). Those methods with DT belong to the
task-agnostic behavior learning methods, which is an active direction in policy learning (e,g.,
[Cui et al., 2023, Brohan et al., 2022, Zheng et al., 2022, Konan et al., 2023, Kim et al., 2023]).
Energy-based models [LeCun et al., 2006] are also modeled as conditional policies [Florence
et al., 2022] or applied to inverse RL [Liu et al., 2021]. Autoregressive model [Vaswani et al.,
2017, Brown et al., 2020] represents the policy as the distribution of action, where it considers
the distribution of the whole trajectory [Reed et al., 2022, ShaÔ¨Åullah et al., 2022].
7
Experiments
In this section, we aim to cover the following three issues: How does DIPO compare to
the widely used RL algorithms (SAC, PPO, and TD3) on the standard continuous control
benchmark? How to show and illustrate the empirical results? How does the diÔ¨Äusion model
compare to VAE [Kingma and Welling, 2013] and multilayer perceptron (MLP) for learning
distribution? How to choose the reverse length K of DIPO for the reverse inference?
7.1
Comparative Evaluation and Illustration
We provide an evaluation on MuJoCo tasks [Todorov et al., 2012]. Figure 6 shows the reward
curves for SAC, PPO, TD3, and DIPO on MuJoCo tasks. To demonstrate the robustness
of the proposed DIPO, we train DIPO with the same hyperparameters for all those 5 tasks,
where we provide the hyperparameters in Table 3, see Appendix H.1. For each algorithm,
we plot the average return of 5 independent trials as the solid curve and plot the standard
deviation across 5 same seeds as the transparent shaded region. We evaluate all the methods
with 106 iterations. Results show that the proposed DIPO achieves the best score across all
those 5 tasks, and DIPO learns much faster than SAC, PPO, and TD3 on the tasks of Ant-3v
13
0.0
0.2
0.4
0.6
0.8
1.0
0
1000
2000
3000
4000
5000
6000
PPO
SAC
TD3
DIPO
(a) Ant-v3
0.0
0.2
0.4
0.6
0.8
1.0
0
2000
4000
6000
8000
10000
12000
PPO
SAC
TD3
DIPO
(b) HalfCheetah-v3
0.0
0.2
0.4
0.6
0.8
1.0
0
500
1000
1500
2000
2500
3000
3500
PPO
SAC
TD3
DIPO
(c) Hopper-v3
0.0
0.2
0.4
0.6
0.8
1.0
0
1000
2000
3000
4000
5000
PPO
SAC
TD3
DIPO
(d) Humanoid-v3
0.0
0.2
0.4
0.6
0.8
1.0
0
1000
2000
3000
4000
5000
PPO
SAC
TD3
DIPO
(e) Walker2d-v3
Figure 6: Average performances on MuJoCo Gym environments with ¬± std shaded, where the
horizontal axis of coordinate denotes the iterations (√ó106), the plots smoothed with a window
of 10.
Ant-v3
DIPO
SAC
TD3
PPO
Figure 7: State-visiting visualization by each algorithm on the Ant-v3 task, where states get
dimension reduction by t-SNE. The points with diÔ¨Äerent colors represent the states visited
by the policy with the style. The distance between points represents the diÔ¨Äerence between
states.
HalfCheetah-v3
DIPO
SAC
Figure 8: State-visiting visualization for comparison between DIPO and SAC on HalfCheetah-
v3.
and Walker2d-3v. Although the asymptotic reward performance of DIPO is similar to baseline
algorithms on other 3 tasks, the proposed DIPO achieves better performance at the initial
iterations, we will try to illustrate some insights for such empirical results of HalfCheetah-v3
in Figure 8, for more discussions, see Appendix H.
7.2
State-Visiting Visualization
From Figure 6, we also know that DIPO achieves the best initial reward performance among
all the 5 tasks, a more intuitive illustration has been shown in Figure 7 and 8, where we only
consider Ant-v3 and HalfCheetah-v3; for more discussions and observations, see Appendix
H.3. We show the state-visiting region to compare both the exploration and Ô¨Ånal reward
performance, where we use the same t-SNE [Van der Maaten and Hinton, 2008] to transfer
the high-dimensional states visited by all the methods for 2D visualization. Results of Figure
7 show that the DIPO explores a wider range of state-visiting, covering TD3, SAC, and PPO.
Furthermore, from Figure 7, we also know DIPO achieves a more dense state-visiting at the
Ô¨Ånal period, which is a reasonable result since after suÔ¨Écient training, the agent identiÔ¨Åes and
14
DIPO
VAE
MLP
SAC
TD3
PPO
algorithm
0
1000
2000
3000
4000
5000
6000
reward
(a) Ant-v3
DIPO
VAE
MLP
SAC
TD3
PPO
algorithm
2000
4000
6000
8000
10000
12000
reward
(b) HalfCheetah-v3
DIPO
VAE
MLP
SAC
TD3
PPO
algorithm
0
500
1000
1500
2000
2500
3000
3500
4000
reward
(c) Hopper-v3
DIPO
VAE
MLP
SAC
TD3
PPO
algorithm
0
1000
2000
3000
4000
5000
6000
reward
(d) Humanoid-v3
DIPO
VAE
MLP
SAC
TD3
PPO
algorithm
0
1000
2000
3000
4000
5000
reward
(e) Walker2d-v3
Figure 9: Reward Performance Comparison to VAE and MLP with DIPO, SAC, PPO and
TD3.
0.0
0.2
0.4
0.6
0.8
1.0
1000
2000
3000
4000
5000
6000
K=20
K=50
K=100
(a) Ant-v3
0.0
0.2
0.4
0.6
0.8
1.0
2000
4000
6000
8000
10000
K=20
K=50
K=100
(b) HalfCheetah-v3
0.0
0.2
0.4
0.6
0.8
1.0
500
1000
1500
2000
2500
3000
3500
K=20
K=50
K=100
(c) Hopper-v3
0.0
0.2
0.4
0.6
0.8
1.0
1000
2000
3000
4000
5000
K=20
K=50
K=100
(d) Humanoid-v3
0.0
0.2
0.4
0.6
0.8
1.0
1000
2000
3000
4000
5000
K=20
K=50
K=100
(e) Walker2d-v3
Figure 10: Learning curves with diÔ¨Äerent reverse lengths K = 20, 50, 100.
Reverse length
Ant-v3
HalfCheetah-v3
Hopper-v3
Humanoid-v3
Walker2d-v3
K = 100
5622.30 ¬± 487.09
10472.31 ¬± 654.96
3123.14 ¬± 636.23
4878.41 ¬± 822.03
4409.18 ¬± 469.06
K = 50
4877.41 ¬± 1010.35
9198.20 ¬± 1738.25
3214.83 ¬± 491.15
4513.39 ¬± 1075.94
4199.34 ¬± 1062.31
K = 20
5288.77 ¬± 970.35
9343.69 ¬± 986.82
2511.63 ¬± 837.03
4294.79 ¬± 1583.48
4467.20 ¬± 368.13
Table 1: Average return over Ô¨Ånal 6E5 iterations with diÔ¨Äerent reverse lengths K = 20, 50, 100,
and maximum value is bolded for each task.
avoids the ‚Äùbad‚Äù states, and plays actions transfer to ‚Äùgood‚Äù states. On the contrary, PPO
shows an aimless exploration in the Ant-v3 task, which partially explains why PPO is not so
good in the Ant-v3 task.
From Figure 8 we know, at the initial time, DIPO covers more regions than SAC in the
HalfCheetah-v3, which results in DIPO obtaining a better reward performance than SAC.
This result coincides with the results of Figure 5, which demonstrates that DIPO is eÔ¨Écient
for exploration, which leads DIPO to better reward performance. While we also know that
SAC starts with a narrow state visit that is similar to the Ô¨Ånal state visit, and SAC performs
with the same reward performance with DIPO at the Ô¨Ånal, which implies SAC runs around
the ‚Äùgood‚Äù region at the beginning although SAC performs a relatively worse initial reward
performance than DIPO. Thus, the result of Figure 8 partially explains why DIPO performs
better than SAC at the initial iterations but performs with same performance with SAC at
the Ô¨Ånal for the HalfCheetah-v3 task.
7.3
Ablation Study
In this section, we consider the ablation study to compare the diÔ¨Äusion model with VAE and
MLP for policy learning, and show a trade-oÔ¨Ä on the reverse length K for reverse inference.
15
7.3.1
Comparison to VAE and MLP
Both VAE and MLP are widely used to learn distribution in machine learning, a fundamental
question is: why must we consider the diÔ¨Äusion model to learn a policy distribution? what the
reward performance is if we use VAE and MLP to model a policy distribution? We show the
answer in Figure 9, where the VAE (or MLP) is the result we replace the diÔ¨Äusion policy of
DIPO (see Figure 3) with VAE (or MLP), i.e., we consider VAE (or MLP)+action gradient
for the tasks. Results show that the diÔ¨Äusion model is more powerful than VAE and MLP for
learning a distribution. This implies the diÔ¨Äusion model is an expressive and Ô¨Çexible family to
model a distribution, which is also consistent with the Ô¨Åeld of the generative model.
7.3.2
Comparison with DiÔ¨Äerent Reverse Lengths
Reverse length K is an important parameter for the diÔ¨Äusion model, which not only aÔ¨Äects the
reward performance but also aÔ¨Äects the training time, we show the results in Figure 10 and
Table 1. The results show that the reverse time K = 100 returns a better reward performance
than other cases (except Hopper-v3 task). Longer reverse length consumes more reverse time
for inference, we hope to use less time for reverse time for action inference. However, a short
reverse length K = 20 decays the reward performance among (except Walker2d-v3 task),
which implies a trade-oÔ¨Ä between reward performance and reverse length K. In practice, we
set K = 100 throughout this paper.
8
Conlusion
We have formally built a theoretical foundation of diÔ¨Äusion policy, which shows a policy
representation via the diÔ¨Äusion probability model and which is a new way to represent a
policy via a stochastic process. Then, we have shown a convergence analysis for diÔ¨Äusion
policy, which provides a theory to understand diÔ¨Äusion policy. Furthermore, we have proposed
an implementation for model-free online RL with a diÔ¨Äusion policy, named DIPO. Finally,
extensive empirical results show the eÔ¨Äectiveness of DIPO among the Mujoco tasks.
References
Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and OÔ¨År Nachum. Opal: OÔ¨Ñine
primitive discovery for accelerating oÔ¨Ñine reinforcement learning. In International Conference
on Learning Representations (ICLR), 2021. (Cited on page 12.)
Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal.
Is conditional generative modeling all you need for decision-making?
In International
Conference on Learning Representations (ICLR), 2023. (Cited on pages 3, 6, and 12.)
Brian DO Anderson. Reverse-time diÔ¨Äusion equation models. Stochastic Processes and their
Applications, 12(3):313‚Äì326, 1982. (Cited on pages 7 and 26.)
Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot
learning from demonstration. Robotics and autonomous systems, 57(5):469‚Äì483, 2009. (Cited
on page 12.)
Dominique Bakry, Ivan Gentil, Michel Ledoux, et al. Analysis and geometry of Markov
diÔ¨Äusion operators, volume 103. Springer, 2014. (Cited on page 9.)
16
Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow
network based generative models for non-iterative diverse candidate generation. Advances in
Neural Information Processing Systems (NeurIPS), 34:27381‚Äì27394, 2021a. (Cited on pages 12
and 13.)
Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J Hu, Mo Tiwari, and Emmanuel Bengio.
GÔ¨Çownet foundations. arXiv preprint arXiv:2111.09266, 2021b. (Cited on pages 12 and 13.)
Johann Brehmer, Joey Bose, Pim De Haan, and Taco Cohen. Edgi: Equivariant diÔ¨Äusion for
planning with embodied agents. In Workshop on Reincarnating Reinforcement Learning at
ICLR, 2023. (Cited on page 12.)
Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea
Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz,
Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry
Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav
Malla, Deeksha Manjunath, Igor Mordatch, OÔ¨År Nachum, Carolina Parada, Jodilyn Peralta,
Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar,
Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan,
Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu,
Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-1: Robotics transformer for real-world
control at scale. In arXiv preprint arXiv:2212.06817, 2022. (Cited on page 13.)
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models
are few-shot learners. Advances in neural information processing systems (NeurIPS), 33:
1877‚Äì1901, 2020. (Cited on page 13.)
Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative
modeling: User-friendly bounds under minimal smoothness assumptions. arXiv preprint
arXiv:2211.01916, 2022. (Cited on page 29.)
Huayu Chen, Cheng Lu, Chengyang Ying, Hang Su, and Jun Zhu. OÔ¨Ñine reinforcement
learning via high-Ô¨Ådelity generative behavior modeling. In International Conference on
Learning Representations (ICLR), 2023a. (Cited on pages 6 and 12.)
Jinyin Chen, Shulong Hu, Haibin Zheng, Changyou Xing, and Guomin Zhang. Gail-pt: An
intelligent penetration testing framework with generative adversarial imitation learning.
Computers & Security, 126:103055, 2023b. (Cited on page 13.)
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter
Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning
via sequence modeling. Advances in neural information processing systems(NeurIPS), 34:
15084‚Äì15097, 2021. (Cited on page 13.)
Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. Sampling is
as easy as learning the score: theory for diÔ¨Äusion models with minimal data assumptions.
In International Conference on Learning Representations (ICLR), 2023c. (Cited on page 9.)
Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin BurchÔ¨Åel, and
Shuran Song. DiÔ¨Äusion policy: Visuomotor policy learning via action diÔ¨Äusion. arXiv
preprint arXiv:2303.04137, 2023. (Cited on pages 3, 6, and 12.)
17
Kai Lai Chung and Ruth J Williams. Introduction to stochastic integration, volume 2. Springer,
1990. (Cited on page 27.)
Zichen JeÔ¨Ä Cui, Yibin Wang, Nur Muhammad, Lerrel Pinto, et al. From play to policy:
Conditional behavior generation from uncurated robot data. International Conference on
Learning Representations (ICLR), 2023. (Cited on page 13.)
Ankur Deka, Changliu Liu, and Katia P Sycara. Arc-actor residual critic for adversarial
imitation learning. In Conference on Robot Learning (CORL), pages 1446‚Äì1456. PMLR,
2023. (Cited on page 13.)
Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov
process expectations for large time. iv. Communications on pure and applied mathematics,
36(2):183‚Äì212, 1983. (Cited on page 28.)
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam
Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-
rl with importance weighted actor-learner architectures. In International conference on
machine learning (ICML), pages 1407‚Äì1416. PMLR, 2018. (Cited on page 5.)
Pete Florence, Corey Lynch, Andy Zeng, Oscar A Ramirez, Ayzaan Wahid, Laura Downs,
Adrian Wong, Johnny Lee, Igor Mordatch, and Jonathan Tompson. Implicit behavioral
cloning. In Conference on Robot Learning (CORL), pages 158‚Äì168. PMLR, 2022. (Cited on
page 13.)
Adriaan Dani¬®el Fokker. Die mittlere energie rotierender elektrischer dipole im strahlungsfeld.
Annalen der Physik, 348(5):810‚Äì820, 1914. (Cited on page 27.)
Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adverserial inverse
reinforcement learning. In International Conference on Learning Representations (ICLR),
2018. (Cited on page 13.)
Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error
in actor-critic methods. In International conference on machine learning (ICML), pages
1587‚Äì1596. PMLR, 2018. (Cited on page 7.)
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of
the ACM, 63(11):139‚Äì144, 2020. (Cited on pages 12 and 13.)
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning
with deep energy-based policies. In International conference on machine learning (ICML),
pages 1352‚Äì1361. PMLR, 2017. (Cited on pages 5, 7, and 50.)
Tuomas Haarnoja, Vitchyr Pong, Aurick Zhou, Murtaza Dalal, Pieter Abbeel, and Sergey
Levine. Composable deep reinforcement learning for robotic manipulation. In 2018 IEEE
international conference on robotics and automation (ICRA), pages 6244‚Äì6251. IEEE, 2018a.
(Cited on page 5.)
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: OÔ¨Ä-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International
conference on machine learning (ICML), pages 1861‚Äì1870. PMLR, 2018b. (Cited on page 5.)
18
Hado Hasselt.
Double q-learning.
Advances in Neural Information Processing Systems
(NeurIPS), 23, 2010. (Cited on page 56.)
Ulrich G Haussmann and Etienne Pardoux. Time reversal of diÔ¨Äusions. The Annals of
Probability, pages 1188‚Äì1205, 1986. (Cited on page 26.)
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in
neural information processing systems (NeurIPS), 29, 2016. (Cited on page 13.)
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diÔ¨Äusion probabilistic models. Advances
in Neural Information Processing Systems (NeurIPS), 33:6840‚Äì6851, 2020. (Cited on pages 3,
26, 33, and 34.)
Aapo Hyv¬®arinen. Estimation of non-normalized statistical models by score matching. Journal
of Machine Learning Research (JMLR), 6(4), 2005. (Cited on page 10.)
Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diÔ¨Äusion for
Ô¨Çexible behavior synthesis. In International Conference on Machine Learning (ICML), 2022.
(Cited on pages 6 and 12.)
Ivan Kapelyukh, Vitalis Vosylius, and Edward Johns. Dall-e-bot: Introducing web-scale
diÔ¨Äusion models to robotics. In CoRL 2022 Workshop on Pre-training Robot Learning, 2022.
(Cited on page 12.)
Changyeon Kim, Jongjin Park, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee.
Preference transformer: Modeling human preferences using transformers for rl. In The
Eleventh International Conference on Learning Representations (ICLR), 2023. (Cited on
page 13.)
Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon.
Soft
truncation: A universal training technique of score-based diÔ¨Äusion model for high precision
score estimation. In Proceedings of the 39th International Conference on Machine Learning
(ICML), volume 162, pages 11201‚Äì11228, 2022. (Cited on page 26.)
Diederik P Kingma and Max Welling.
Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013. (Cited on pages 12 and 13.)
Andrei KolmogoroÔ¨Ä. ¬®Uber die analytischen methoden in der wahrscheinlichkeitsrechnung.
Mathematische Annalen, 104:415‚Äì458, 1931. (Cited on page 27.)
Sachin G Konan, Esmaeil Seraj, and Matthew Gombolay. Contrastive decision transformers.
In Conference on Robot Learning (CORL), pages 2159‚Äì2169. PMLR, 2023. (Cited on page 13.)
Yann LeCun, Sumit Chopra, Raia Hadsell, Marc‚ÄôAurelio Ranzato, and Fu Jie Huang. A
tutorial on energy-based learning. Predicting Structured Data, 1:0, 2006. (Cited on page 13.)
Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for
general data distributions. In International Conference on Algorithmic Learning Theory
(ALT), pages 946‚Äì985. PMLR, 2023. (Cited on page 8.)
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. OÔ¨Ñine reinforcement learning:
Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
(Cited on page 12.)
19
Wenhao Li, Baoxiang Wang, Shanchao Yang, and Hongyuan Zha. Diverse policy optimiza-
tion for structured action space. In Proceedings of the 22th International Conference on
Autonomous Agents and MultiAgent Systems (AAMAS), 2023a. (Cited on page 13.)
Yinchuan Li, Shuang Luo, Haozhi Wang, and Hao Jianye. CÔ¨Çownets: Continuous control
with generative Ô¨Çow networks. In The Eleventh International Conference on Learning
Representations (ICLR), 2023b. (Cited on page 13.)
Zhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni, Masayoshi Tomizuka, and Ping Luo. Adaptdif-
fuser: DiÔ¨Äusion models as adaptive self-evolving planners. arXiv preprint arXiv:2302.01877,
2023. (Cited on page 12.)
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval
Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement
learning. In International Conference on Learning Representations (ICLR), 2016. (Cited on
page 5.)
Minghuan Liu, Tairan He, Minkai Xu, and Weinan Zhang. Energy-based imitation learning.
In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent
Systems (AAMAS), pages 809‚Äì817, 2021. (Cited on page 13.)
Weiyu Liu, Tucker Hermans, Sonia Chernova, and Chris Paxton. StructdiÔ¨Äusion: Object-
centric diÔ¨Äusion for semantic rearrangement of novel objects. In Workshop on Language
and Robotics at CoRL, 2022. (Cited on page 12.)
Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine,
and Pierre Sermanet. Learning latent plans from play. In Conference on robot learning
(CORL), pages 1113‚Äì1132. PMLR, 2020. (Cited on pages 12 and 13.)
Yi-An Ma, Yuansi Chen, Chi Jin, Nicolas Flammarion, and Michael I Jordan. Sampling can
be faster than optimization. Proceedings of the National Academy of Sciences, 116(42):
20881‚Äì20885, 2019. (Cited on page 9.)
Nikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory
balance: Improved credit assignment in gÔ¨Çownets. In Advances in Neural Information
Processing Systems, 2022. (Cited on page 13.)
Ajay Mandlekar, Danfei Xu, Roberto Mart¬¥ƒ±n-Mart¬¥ƒ±n, Silvio Savarese, and Li Fei-Fei. Learning
to generalize across long-horizon tasks from human demonstrations. In Robotics: Science
and Systems (RSS), 2020. (Cited on page 12.)
Oier Mees, Lukas Hermann, and Wolfram Burgard. What matters in language conditioned
robotic imitation learning over unstructured data. IEEE Robotics and Automation Letters,
7(4):11205‚Äì11212, 2022. (Cited on page 13.)
Utkarsh A Mishra and Yongxin Chen. ReorientdiÔ¨Ä: DiÔ¨Äusion model based reorientation for
object manipulation. arXiv preprint arXiv:2303.12700, 2023. (Cited on page 12.)
Diganta Misra. Mish: A self regularized non-monotonic activation function. arXiv preprint
arXiv:1908.08681, 2019. (Cited on page 56.)
20
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
Human-level control through deep reinforcement learning. nature, 518(7540):529‚Äì533, 2015.
(Cited on page 5.)
Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J Andrew Bagnell, Pieter Abbeel, Jan
Peters, et al. An algorithmic perspective on imitation learning. Foundations and Trends¬Æ
in Robotics, 7(1-2):1‚Äì179, 2018. (Cited on page 12.)
Ling Pan, Dinghuai Zhang, Moksh Jain, Longbo Huang, and Yoshua Bengio. Stochastic
generative Ô¨Çow networks. arXiv preprint arXiv:2302.09465, 2023. (Cited on page 13.)
Tim Pearce, Tabish Rashid, Anssi Kanervisto, Dave Bignell, Mingfei Sun, Raluca Georgescu,
Sergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, et al. Imi-
tating human behaviour with diÔ¨Äusion models. In International Conference on Learning
Representations (ICLR), 2023. (Cited on page 12.)
Karl Pertsch, Youngwoon Lee, and Joseph Lim. Accelerating reinforcement learning with
learned skill priors. In Conference on robot learning (CORL), pages 188‚Äì204, 2021. (Cited on
page 13.)
Jan Peters, Katharina Mulling, and Yasemin Altun. Relative entropy policy search. In
Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 24, pages 1607‚Äì1612,
2010. (Cited on page 5.)
VM Planck. ¬®Uber einen satz der statistischen dynamik und seine erweiterung in der quan-
tentheorie. Sitzungsberichte der Preussischen Akademie der Wissenschaften zu Berlin, 24:
324‚Äì341, 1917. (Cited on page 27.)
Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Advances in
neural information processing systems (NeurIPS), 1, 1988. (Cited on page 12.)
Rafael Rafailov, Victor Kolev, Kyle Beltran Hatch, John D Martin, Mariano Phielipp, Jiajun
Wu, and Chelsea Finn. Model-based adversarial imitation learning as online Ô¨Åne-tuning. In
Workshop on Reincarnating Reinforcement Learning at ICLR 2023. (Cited on page 13.)
Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio G¬¥omez Colmenarejo, Alexander Novikov,
Gabriel Barth-maron, Mai Gim¬¥enez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom
Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,
Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. Transactions on
Machine Learning Research (TMLR), 2022. ISSN 2835-8856. (Cited on page 13.)
Moritz Reuss, Maximilian Li, Xiaogang Jia, and Rudolf Lioutikov. Goal-conditioned imitation
learning using score-based diÔ¨Äusion policies. arXiv preprint arXiv:2304.02532, 2023. (Cited
on pages 3, 6, and 12.)
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing Ô¨Çows. In
International conference on machine learning (ICML), pages 1530‚Äì1538, 2015. (Cited on
pages 12 and 13.)
Hannes Risken and Hannes Risken. Fokker-planck equation. Springer, 1996. (Cited on page 27.)
21
Erick Rosete-Beas, Oier Mees, Gabriel Kalweit, Joschka Boedecker, and Wolfram Burgard.
Latent plans for task-agnostic oÔ¨Ñine reinforcement learning. In Conference on Robot Learning
(CORL), pages 1838‚Äì1849, 2023. (Cited on page 13.)
Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems,
volume 37. University of Cambridge, Department of Engineering, 1994. (Cited on page 5.)
Brian Sallans and GeoÔ¨Ärey E Hinton. Reinforcement learning with factored states and actions.
The Journal of Machine Learning Research (JMLR), 5:1063‚Äì1088, 2004. (Cited on page 5.)
Simo S¬®arkk¬®a and Arno Solin. Applied stochastic diÔ¨Äerential equations, volume 10. Cambridge
University Press, 2019. (Cited on pages 26 and 27.)
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust
region policy optimization. In International conference on machine learningn (ICML), pages
1889‚Äì1897. PMLR, 2015. (Cited on page 5.)
John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft
q-learning. arXiv preprint arXiv:1704.06440, 2017a. (Cited on page 5.)
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017b. (Cited on page 5.)
Nur Muhammad ShaÔ¨Åullah, Zichen Cui, Ariuntuya Arty Altanzaya, and Lerrel Pinto. Behavior
transformers: Cloning k modes with one stone. Advances in neural information processing
systems (NeurIPS), 35:22955‚Äì22968, 2022. (Cited on page 13.)
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In International conference on machine learning
(ICML), pages 387‚Äì395. Pmlr, 2014. (Cited on page 5.)
Avi Singh, Huihan Liu, Gaoyue Zhou, Albert Yu, Nicholas Rhinehart, and Sergey Levine.
Parrot: Data-driven behavioral priors for reinforcement learning. In International Conference
on Learning Representations (ICLR), 2020. (Cited on page 13.)
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsu-
pervised learning using nonequilibrium thermodynamics. In International Conference on
Machine Learning (ICML), pages 2256‚Äì2265. PMLR, 2015. (Cited on pages 3 and 26.)
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and
Ben Poole. Score-based generative modeling through stochastic diÔ¨Äerential equations. In
International Conference on Learning Representations (ICLR), 2021. (Cited on pages 3, 26,
and 34.)
Richard S Sutton. Learning to predict by the methods of temporal diÔ¨Äerences. Machine
learning, 3:9‚Äì44, 1988. (Cited on page 13.)
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,
1998. (Cited on pages 5 and 13.)
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,
2018. (Cited on page 5.)
22
Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation.
Advances in neural
information processing systems (NeurIPS), 12, 1999. (Cited on page 5.)
Aleksandar Taranovic, Andras Gabor Kupcsik, Niklas Freymuth, and Gerhard Neumann.
Adversarial imitation learning with preferences. In The Eleventh International Conference
on Learning Representations (ICLR), 2023. (Cited on page 13.)
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based
control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,
pages 5026‚Äì5033. IEEE, 2012. doi: 10.1109/IROS.2012.6386109. (Cited on page 13.)
Julen Urain, Niklas Funk, Georgia Chalvatzaki, and Jan Peters. Se(3)-diÔ¨ÄusionÔ¨Åelds: Learning
cost functions for joint grasp and motion optimization through diÔ¨Äusion. In 2018 IEEE
international conference on robotics and automation (ICRA), 2023. (Cited on page 12.)
Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances
in neural information processing systems (NeurIPS), 30, 2017. (Cited on page 12.)
Laurens Van der Maaten and GeoÔ¨Ärey Hinton. Visualizing data using t-sne. Journal of
machine learning research (JMLR), 9(11), 2008. (Cited on pages 14 and 57.)
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
 Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems (NeurIPS), 30, 2017. (Cited on pages 13 and 56.)
Santosh Vempala and Andre Wibisono. Rapid convergence of the unadjusted langevin algorithm:
Isoperimetry suÔ¨Éces. In Advances in neural information processing systems (NeurIPS),
volume 32, 2019. (Cited on pages 9, 25, and 29.)
Pascal Vincent. A connection between score matching and denoising autoencoders. Neural
computation, 23(7):1661‚Äì1674, 2011. (Cited on page 10.)
Lirui Wang, Xiangyun Meng, Yu Xiang, and Dieter Fox. Hierarchical policies for cluttered-
scene grasping with latent plans. IEEE Robotics and Automation Letters, 7(2):2883‚Äì2890,
2022. (Cited on page 13.)
Yunke Wang, Chang Xu, Bo Du, and Honglak Lee. Learning to weight imperfect demonstrations.
In International Conference on Machine Learning (ICML), pages 10961‚Äì10970. PMLR, 2021.
(Cited on page 13.)
Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. DiÔ¨Äusion policies as an expressive
policy class for oÔ¨Ñine reinforcement learning. In International Conference on Learning
Representations (ICLR), 2023. (Cited on pages 6 and 12.)
Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. PhD thesis, King‚Äôs
College, Cambridge, 1989. (Cited on page 5.)
Andre Wibisono and Varun Jog. Convexity of mutual information along the ornstein-uhlenbeck
Ô¨Çow. In 2018 International Symposium on Information Theory and Its Applications (ISITA),
pages 55‚Äì59. IEEE, 2018. (Cited on page 9.)
23
Andre Wibisono and Kaylee Yingxi Yang.
Convergence in kl divergence of the inexact
langevin algorithm with application to score-based generative models.
arXiv preprint
arXiv:2211.01512, 2022. (Cited on pages 9, 28, and 44.)
Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Yingxia Shao,
Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. DiÔ¨Äusion models: A comprehensive survey
of methods and applications. arXiv preprint arXiv:2209.00796, 2022. (Cited on page 12.)
Sherry Yang, OÔ¨År Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale Schuurmans.
Foundation models for decision making: Problems, methods, and opportunities. arXiv
preprint arXiv:2303.04129, 2023. (Cited on page 12.)
Chen Yongxin Zhang, Qinsheng. Fast sampling of diÔ¨Äusion models with exponential integrator.
In International Conference on Learning Representations (ICLR), 2022. (Cited on page 8.)
Qinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer. In International
Conference on Machine Learning (ICML), pages 27042‚Äì27059. PMLR, 2022. (Cited on page 13.)
24
A
Review on Notations
This section reviews some notations and integration by parts formula, using the notations
consistent with [Vempala and Wibisono, 2019].
Given a smooth function f : Rn ‚Üí R, its gradient ‚àáf : Rn ‚Üí Rn is the vector of partial
derivatives:
‚àáf(x) =
‚àÇf(x)
‚àÇx1
, . . . , ‚àÇf(x)
‚àÇxn
‚ä§
.
The Hessian ‚àá2f : Rn ‚Üí Rn√ón is the matrix of second partial derivatives:
‚àá2f(x) =
‚àÇ2f(x)
‚àÇxixj

1‚â§i,j‚â§n
.
The Laplacian ‚àÜf : Rn ‚Üí R is the trace of its Hessian:
‚àÜf(x) = Tr
 ‚àá2f(x)

=
n
X
i=1
‚àÇ2f(x)
‚àÇx2
i
.
Given a smooth vector Ô¨Åeld p = (p1, . . . , pn): Rn ‚Üí Rn, its divergence is div ¬∑ p: Rn ‚Üí R:
(div ¬∑ p)(x) =
n
X
i=1
‚àÇpi(x)
‚àÇxi
.
(16)
When the variable of a function is clear and without causing ambiguity, we also denote the
above notation as follows,
div ¬∑ (p(x)) =: (div ¬∑ p)(x) =
n
X
i=1
‚àÇpi(x)
‚àÇxi
.
(17)
In particular, the divergence of the gradient is the Laplacian:
(div ¬∑ ‚àáf)(x) =
n
X
i=1
‚àÇ2f(x)
‚àÇx2
i
= ‚àÜf(x).
(18)
Let œÅ(x) and ¬µ(x) be two smooth probability density functions on the space Rp, the Kull-
back‚ÄìLeibler (KL) divergence and relative Fisher information (FI) from ¬µ(x) to œÅ(x) are
deÔ¨Åned as follows,
KL(œÅ‚à•¬µ) =
Z
Rp
œÅ(x) log œÅ(x)
¬µ(x)dx, FI(œÅ‚à•¬µ) =
Z
Rp
œÅ(x)
‚àá log œÅ(x)
¬µ(x)

2
2
dx.
(19)
Before we further analyze, we need the integration by parts formula. For any function
f : Rp ‚Üí R and vector Ô¨Åeld v: Rp ‚Üí Rp with suÔ¨Éciently fast decay at inÔ¨Ånity, we have the
following integration by parts formula:
Z
Rp
‚ü®v(x), ‚àáf(x)‚ü©dx = ‚àí
Z
Rp
f(x)(div ¬∑ v)(x)dx.
(20)
25
B
Auxiliary Results
B.1
DiÔ¨Äusion Probability Model (DPM).
This section reviews some basic background about the diÔ¨Äusion probability model (DPM).
For a given (but unknown) p-dimensional data distribution q(x0), DPM [Sohl-Dickstein et al.,
2015, Ho et al., 2020, Song et al., 2021] is a latent variable generative model that learns a
parametric model to approximate the distribution q(x0). To simplify the presentation in this
section, we only focus on the continuous-time diÔ¨Äusion [Song et al., 2021]. The mechanism of
DPM contains two processes, forward process and reverse process; we present them as follows.
Forward Process. The forward process produces a sequence {xt}t=0:T that perturbs the
initial x0 ‚àº q(¬∑) into a Gaussian noise, which follows the next stochastic diÔ¨Äerential equation
(SDE),
dxt = f(xt, t)dt + g(t)dwt,
(21)
where f(¬∑, ¬∑) is the drift term, g(¬∑) is the diÔ¨Äusion term, and wt is the standard Wiener process.
Reverse Process. According to Anderson [1982], Haussmann and Pardoux [1986], there
exists a corresponding reverse SDE that exactly coincides with the solution of the forward
SDE (21):
dxt =

f(xt, t) ‚àí g2(t)‚àá log pt(xt)

d¬Øt + g(t)d ¬Øwt,
(22)
where d¬Øt is the backward time diÔ¨Äerential, d ¬Øwt is a standard Wiener process Ô¨Çowing backward
in time, and pt(xt)is the marginal probability distribution of the random variable xt at time t.
Once the score function ‚àá log pt(xt) is known for each time t, we can derive the reverse
diÔ¨Äusion process from SDE (22) and simulate it to sample from q(x0) [Song et al., 2021].
B.2
Transition Probability for Ornstein-Uhlenbeck Process
Proposition B.1. Consider the next SDEs,
dxt = ‚àí1
2Œ≤(t)xtdt + g(t)dwt,
where Œ≤(¬∑) and g(¬∑) are real-valued functions. Then, for a given x0, the conditional distribution
of xt|x0 is a Gaussian distribution, i.e.,
xt|x0 ‚àº N (xt|¬µt, Œ£t) ,
where
¬µt = exp

‚àí1
2
Z t
0
Œ≤(s)ds

x0,
Œ£t = exp

‚àí
Z t
0
Œ≤(s)ds
 Z t
0
exp
Z œÑ
0
Œ≤(s)ds

g2(œÑ)dœÑ

I.
Proof. See [Kim et al., 2022, A.1] or [S¬®arkk¬®a and Solin, 2019, Chapter 6.1].
‚ñ°
26
B.3
Exponential Integrator Discretization
The next Proposition B.2 provides a fundamental way for us to derivate the exponential
integrator discretization (9).
Proposition B.2. For a given state s, we consider the following continuous time process, for
t ‚àà [tk, tk+1],
dxt =

xt + 2ÀÜS(xtk, s, T ‚àí tk)

dt +
‚àö
2dwt.
(23)
Then with ItÀÜo integration [Chung and Williams, 1990],
xt ‚àí xtk =
 et‚àítk ‚àí 1
 
xtk + 2ÀÜS(xtk, s, T ‚àí tk)

+
‚àö
2
Z t
tk
et
‚Ä≤‚àítkdwt‚Ä≤,
where t ‚àà [tk, tk+1], and tk = hk.
Proof. See [S¬®arkk¬®a and Solin, 2019, Chapter 6.1].
‚ñ°
Recall SDE (8), according to Proposition B.2, we know the next SDE
dÀÜat =

ÀÜat + 2ÀÜS(ÀÜatk, s, T ‚àí tk)

dt +
‚àö
2dwt, t ‚àà [tk, tk+1]
(24)
formulates the exponential integrator discretization as follows,
ÀÜatk+1 ‚àí ÀÜatk =
 etk+1‚àítk ‚àí 1
 
ÀÜatk + 2ÀÜS(ÀÜatk, s, T ‚àí tk)

+
‚àö
2
Z tk+1
tk
et
‚Ä≤‚àítkdwt‚Ä≤
(25)
=

eh ‚àí 1
 
ÀÜatk + 2ÀÜS(ÀÜatk, s, T ‚àí tk)

+
p
e2h ‚àí 1z,
(26)
where last equation holds due to Wiener process is a stationary process with independent
increments, i.e., the following holds,
‚àö
2
Z tk+1
tk
et
‚Ä≤‚àítkdwt‚Ä≤ =
‚àö
2
Z tk+1
0
et
‚Ä≤‚àítkdwt‚Ä≤ ‚àí
‚àö
2
Z tk
0
et
‚Ä≤‚àítkdwt‚Ä≤
=
p
e2h ‚àí 1z,
where z ‚àº N(0, I).
Then, we rewrite (26) as follows,
ÀÜatk+1 =ehÀÜatk +

eh ‚àí 1
 
ÀÜatk + 2ÀÜS(ÀÜatk, s, T ‚àí tk)

+
p
e2h ‚àí 1z,
which concludes the iteration deÔ¨Åned in (9).
B.4
Fokker‚ÄìPlanck Equation
The Fokker‚ÄìPlanck equation is named after Adriaan Fokker and Max Planck, who described
it in 1914 and 1917 [Fokker, 1914, Planck, 1917]. It is also known as the Kolmogorov forward
equation, after Andrey Kolmogorov, who independently discovered it in 1931 [KolmogoroÔ¨Ä,
1931].
For more history and background about Fokker‚ÄìPlanck equation, please refer to
[Risken and Risken, 1996] or https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_
equation#cite_note-1.
27
For an ItÀÜo process driven by the standard Wiener process wt and described by the stochastic
diÔ¨Äerential equation
dxt = ¬µ(xt, t)dt + Œ£(xt, t)dwt,
(27)
where xt and ¬µ(xt, t) are N-dimensional random vectors, Œ£(xt, t) is an n √ó m matrix and wt
is an m-dimensional standard Wiener process, the probability density p(x, t) for xt satisÔ¨Åes
the Fokker‚ÄìPlanck equation
‚àÇp(x, t)
‚àÇt
= ‚àí
N
X
i=1
‚àÇ
‚àÇxi
[¬µi(x, t)p(x, t)] +
N
X
i=1
N
X
j=1
‚àÇ2
‚àÇxi ‚àÇxj
[Dij(x, t)p(x, t)] ,
(28)
with drift vector ¬µ = (¬µ1, . . . , ¬µN) and diÔ¨Äusion tensor D(x, t) = 1
2Œ£Œ£‚ä§, i.e.
Dij(x, t) = 1
2
M
X
k=1
œÉik(x, t)œÉjk(x, t),
and œÉij denotes the (i, j)-th element of the matrix Œ£.
B.5
Donsker-Varadhan Representation for KL-divergence
Proposition B.3 ([Donsker and Varadhan, 1983]). Let œÅ, ¬µ be two probability distributions
on the measure space (X, F), where X ‚àà Rp. Then
KL(œÅ‚à•¬µ) =
sup
f:X‚ÜíR
Z
Rp œÅ(x)f(x)dx ‚àí log
Z
Rp ¬µ(x) exp(f(x))dx

.
The Donsker-Varadhan representation for KL-divergence implies for any f(¬∑),
KL(œÅ‚à•¬µ) ‚â•
Z
Rp œÅ(x)f(x)dx ‚àí log
Z
Rp ¬µ(x) exp(f(x))dx,
(29)
which is useful later.
B.6
Some Basic Results for DiÔ¨Äusion Policy
Proposition B.4. Let œÄ(¬∑|s) satisfy ŒΩ-Log-Sobolev inequality (LSI) (see Assumption 4.2), the
initial random action ¬Øa0 ‚àº œÄ(¬∑|s), and ¬Øat evolves according to the following Ornstein-Uhlenbeck
process,
d¬Øat = ‚àí¬Øatdt +
‚àö
2dwt.
(30)
Let ¬Øat ‚àº ¬ØœÄt(¬∑|s) be the evolution along the Ornstein-Uhlenbeck Ô¨Çow (30), then ¬ØœÄt(¬∑|s) is ŒΩt-LSI,
where
ŒΩt =
ŒΩ
ŒΩ + (1 ‚àí ŒΩ)e‚àí2t .
Proof. See [Wibisono and Yang, 2022, Lemma 6].
‚ñ°
28
Proposition B.5. Under Assumption 4.1, then ‚àá log ÀúœÄt(¬∑|s) is Lpet-Lipschitz on the time
interval [0, T0], where the policy ÀúœÄt(¬∑|s) is the evolution along the Ô¨Çow (4), and the time T0 is
deÔ¨Åned as follows,
T0 =: sup
t‚â•0

t : 1 ‚àí e‚àí2t ‚â§ e‚àít
Lp

.
Proof. [Chen et al., 2022, Lemma 13].
‚ñ°
The positive scalar T0 is well-deÔ¨Åned, i.e., T0 always exists. In fact, let 1 ‚àí e‚àí2t ‚â§ e‚àít
Lp ‚â• 0,
then the following holds,
e‚àít ‚â•
s
1
L2p
+ 4 ‚àí 1
Lp
,
then
T0 = log
 
1
4
 s
1
L2p
+ 4 + 1
Lp
!!
.
Proposition B.6. ([Vempala and Wibisono, 2019, Lemma 10]) Let œÅ(x) be a probability
distribution function on Rp, and let f(x) = ‚àí log œÅ(x) be a L-smooth, i.e., there exists a
positive constant L such that ‚àíLI ‚™Ø ‚àá2f(x) ‚™Ø LI for all x ‚àà Rp. Furthermore, let œÅ(x)
satisfy the LSI condition with constant ŒΩ > 0, i.e., for any probability distribution ¬µ(x),
KL(¬µ‚à•œÅ) ‚â§ 1
2ŒΩ FI(¬µ‚à•œÅ). Then for any distribution ¬µ(x), the following equation holds,
Ex‚àº¬µ(¬∑)

‚à•‚àá log œÅ(x)‚à•2
2

=
Z
Rp ¬µ(x) ‚à•‚àá log œÅ(x)‚à•2
2 dx ‚â§ 4L2
ŒΩ KL(¬µ‚à•œÅ) + 2pL.
29
C
Implementation Details of DIPO
In this section, we provide all the details of our implementation for DIPO.
Algorithm 3: (DIPO): Model-Free Learning with DiÔ¨Äusion Policy
1: Initialize parameter œÜ, critic networks Qœà, target networks Qœà‚Ä≤, length K;
2: Initialize {Œ≤i}K
i=1; Œ±i =: 1 ‚àí Œ≤i, ¬ØŒ±k =: Qk
i=1 Œ±i, œÉk =:
r1 ‚àí ¬ØŒ±k‚àí1
1 ‚àí ¬ØŒ±k
Œ≤k;
3: Initialize œà
‚Ä≤ ‚Üê œà, œÜ
‚Ä≤ ‚Üê œÜ;
4: repeat
5:
#update experience with diffusion policy
6:
dataset Denv ‚Üê ‚àÖ; initial state s0 ‚àº d0(¬∑);
7:
for t = 0, 1, ¬∑ ¬∑ ¬∑ , T do
8:
initial ÀÜaK ‚àº N(0, I);
9:
for k = K, ¬∑ ¬∑ ¬∑ , 1 do
10:
zk ‚àº N(0, I), if k > 1; else zk = 0;
11:
ÀÜak‚àí1 ‚Üê
1
‚àöŒ±k

ÀÜak ‚àí
Œ≤k
‚àö1 ‚àí ¬ØŒ±k
œµœÜ(ÀÜak, s, k)

+ œÉkzk;
12:
end for
13:
at ‚Üê ÀÜa0; st+1 ‚àº P(¬∑|st, at); Denv ‚Üê Denv ‚à™ {st, at, st+1, r(st+1|st, at)};
14:
end for
15:
#update value function
16:
for each mini-batch data do
17:
sample mini-batch D from Denv with size N, D = {sj, aj, sj+1, r(sj+1|sj, aj)}N
j=1;
18:
take gradient descent as follows
œà ‚Üê œà ‚àí Œ∑œà‚àáœà
1
N
N
X
j=1

r(sj+1|sj, aj) + Œ≥Qœà‚Ä≤(sj+1, aj+1) ‚àí Qœà(sj, aj)
2
;
19:
end for
20:
#improve experience through action
21:
for t = 0, 1, ¬∑ ¬∑ ¬∑ , T do
22:
replace the action at ‚àà Denv as follows
at ‚Üê at + Œ∑a‚àáaQœà(st, a)

a=at;
23:
end for
24:
#update diffusion policy
25:
for each pair do
26:
sample a pair (s, a) ‚àº Denv uniformly; k ‚àº Uniform({1, ¬∑ ¬∑ ¬∑ , K}); z ‚àº N(0, I);
27:
take gradient descent as follows
œÜ ‚Üê œÜ ‚àí Œ∑œÜ‚àáœÜ
z ‚àí œµœÜ
 ‚àö¬ØŒ±ka + ‚àö1 ‚àí ¬ØŒ±kz, s, k
2
2 ;
28:
end for
29:
soft update œà
‚Ä≤ ‚Üê œÅœà
‚Ä≤ + (1 ‚àí œÅ)œà;
30:
soft update œÜ
‚Ä≤ ‚Üê œÅœÜ
‚Ä≤ + (1 ‚àí œÅ)œÜ;
31: until the policy performs well in the real environment.
30
C.1
DIPO: Model-Free Learning with DiÔ¨Äusion Policy
Our source code follows the Algorithm 3.
C.2
Loss Function of DIPO
In this section, we provide the details of #update diffusion policy presented in Algorithm
3. We present the derivation of the loss of score matching (10) and present the details of
updating the diÔ¨Äusion from samples. First, the next Theorem C.1 shows an equivalent version
of the loss deÔ¨Åned in (10), then we present the learning details from samples.
C.2.1
Conditional Sampling Version of Score Matching
Theorem C.1. For give a partition on the interval [0, T], 0 = t0 < t1 < ¬∑ ¬∑ ¬∑ < tk < tk+1 <
¬∑ ¬∑ ¬∑ < tK = T, let Œ±0 = e‚àí2T , Œ±k = e2(‚àítk+1+tk), and ¬ØŒ±k‚àí1 = Qk
k‚Ä≤=0 Œ±k‚Ä≤. Setting œâ(t) according
to the next (36), then the objective (10) follows the next expectation version,
L(œÜ) = Ek‚àºU([K]),zk‚àºN(0,I),¬Øa0‚àºœÄ(¬∑|s)

zk ‚àí œµœÜ
 ‚àö¬ØŒ±k¬Øa0 + ‚àö1 ‚àí ¬ØŒ±kzk, s, k

,
(31)
where [K] =: {1, 2, ¬∑ ¬∑ ¬∑ , K}, U(¬∑) denotes uniform distribution, the parametic funciton
œµœÜ(¬∑, ¬∑, ¬∑) : A √ó S √ó [K] ‚Üí Rp
shares the parameter œÜ according to:
œµœÜ (¬∑, ¬∑, k) = ‚àí‚àö1 ‚àí ¬ØŒ±kÀÜsœÜ (¬∑, ¬∑, T ‚àí tk) .
Proof. According to (3), and Proposition B.1, we know œït(¬Øat|¬Øa0) = N
 e‚àít¬Øa0,
 1 ‚àí e‚àí2t
I

,
then
‚àá log œït(¬Øat|¬Øa0) = ‚àí¬Øat ‚àí e‚àít¬Øa0
1 ‚àí 2e‚àít
= ‚àí
zt
‚àö
1 ‚àí 2e‚àít ,
(32)
where zt ‚àº N(0, I),
Let œÉt =
‚àö
1 ‚àí e‚àí2t, according to (3), we know
¬Øat = e‚àít¬Øa0 +
p
1 ‚àí e‚àí2t

zt = e‚àít¬Øa0 + œÉtzt,
(33)
where zt ‚àº N(0, I).
Recall (10), we obtain
L(œÜ) =
Z
T
0
œâ(t)E¬Øa0‚àºœÄ(¬∑|s)E¬Øat|¬Øa0
h
‚à•ÀÜsœÜ(¬Øat, s, t) ‚àí ‚àá log œït(¬Øat|¬Øa0)‚à•2
2
i
dt
(32)
=
Z
T
0
œâ(t)
œÉ2
t
E¬Øa0‚àºœÄ(¬∑|s)E¬Øat|¬Øa0
h
‚à•œÉtÀÜsœÜ(¬Øat, s, t) + zt‚à•2
2
i
dt
(34)
t‚ÜêT‚àít
=
Z
T
0
œâ(T ‚àí t)
œÉ2
T‚àít
E¬Øa0‚àºœÄ(¬∑|s)E¬ØaT ‚àít|¬Øa0
h
‚à•œÉT‚àítÀÜsœÜ(¬ØaT‚àít, s, T ‚àí t) + zT‚àít‚à•2
2
i
dt.
(35)
Furthermore, we deÔ¨Åne an indicator function It‚Ä≤(t) as follows,
It‚Ä≤(t) =:
 1,
if t
‚Ä≤ = t;
0,
if t
‚Ä≤ Ã∏= t.
31
Let the weighting function be deÔ¨Åned as follows, for any t ‚àà [0, T],
œâ(t) = 1
K
K
X
k=1

1 ‚àí e‚àí2(T‚àít)
IT‚àítk(t),
(36)
where we give a partition on the interval [0, T] as follows,
0 = t0 < t1 < ¬∑ ¬∑ ¬∑ < tk < tk+1 < ¬∑ ¬∑ ¬∑ < tK = T.
Then, we rewrite (34) as follows,
L(œÜ) = 1
K
K
X
k=1
E¬Øa0‚àºœÄ(¬∑|s)E¬ØaT ‚àítk|¬Øa0
h
‚à•œÉT‚àítk ÀÜsœÜ(¬ØaT‚àítk, s, T ‚àí tk) + zT‚àítk‚à•2
2
i
.
(37)
We consider the next term contained in (37)
ÀÜsœÜ (¬ØaT‚àítk, s, T ‚àí tk)
(33)
= ÀÜsœÜ

e‚àí(T‚àítk)¬Øa0 + œÉT‚àítkzT‚àítk, s, T ‚àí tk

=ÀÜsœÜ

e‚àí(T‚àítk)¬Øa0 +
p
1 ‚àí e‚àí2(T‚àítk)zT‚àítk, s, T ‚àí tk

,
where zT‚àítk ‚àº N(0, I), then obtain
E¬ØaT ‚àítk|¬Øa0
h
‚à•ÀÜsœÜ(¬ØaT‚àítk, s, T ‚àí tk) + zT‚àítk‚à•2
2
i
=EzT ‚àítk‚àºN(0,I)
h
œÉT‚àítk ÀÜsœÜ

e‚àí(T‚àítk)¬Øa0 +
p
1 ‚àí e‚àí2(T‚àítk)zT‚àítk, s, T ‚àí tk
i
.
Now, we rewrite (37) as the next expectation version,
L(œÜ) = Ek‚àºU([K]),ztk‚àºN(0,I),¬Øa0‚àºœÄ(¬∑|s)
h
œÉT‚àítk ÀÜsœÜ

e‚àí(T‚àítk)¬Øa0 +
p
1 ‚àí e‚àí2(T‚àítk)zT‚àítk, s, T ‚àí tk
i
(38)
For k = 0, 1, ¬∑ ¬∑ ¬∑ , K, and Œ±0 = e‚àí2T and
Œ±k = e2(‚àítk+1+tk).
(39)
Then we obtain
¬ØŒ±k =
k‚àí1
Y
k‚Ä≤=0
Œ±k‚Ä≤ = e‚àí2(T‚àítk).
With those notations, we rewrite (38) as follows,
L(œÜ) = Ek‚àºU([K]),zk‚àºN(0,I),¬Øa0‚àºœÄ(¬∑|s)
‚àö1 ‚àí ¬ØŒ±kÀÜsœÜ
 ‚àö¬ØŒ±k¬Øa0 + ‚àö1 ‚àí ¬ØŒ±kzk, s, T ‚àí tk

+ zk

. (40)
Finally, we deÔ¨Åne a function œµœÜ(¬∑, ¬∑, ¬∑) : S √ó A √ó [K] ‚Üí Rp, and
œµœÜ
 ‚àö¬ØŒ±k¬Øa0 + ‚àö1 ‚àí ¬ØŒ±kzk, s, k

=: ‚àí‚àö1 ‚àí ¬ØŒ±kÀÜsœÜ
 ‚àö¬ØŒ±k¬Øa0 + ‚àö1 ‚àí ¬ØŒ±kzk, s, T ‚àí tk

,
(41)
i,e. we estimate the score function via an estimator œµœÜ as follows,
ÀÜsœÜ
 ‚àö¬ØŒ±k¬Øa0 + ‚àö1 ‚àí ¬ØŒ±kzk, s, T ‚àí tk

= ‚àíœµœÜ
 ‚àö¬ØŒ±k¬Øa0 + ‚àö1 ‚àí ¬ØŒ±kzk, s, k

‚àö1 ‚àí ¬ØŒ±k
.
(42)
Then we rewrite (40) as follows,
L(œÜ) = Ek‚àºU([K]),zk‚àºN(0,I),¬Øa0‚àºœÄ(¬∑|s)

zk ‚àí œµœÜ
 ‚àö¬ØŒ±k¬Øa0 + ‚àö1 ‚àí ¬ØŒ±kzk, s, k

.
This concludes the proof.
‚ñ°
32
Algorithm 4: DiÔ¨Äusion Policy (A Backward Version [Ho et al., 2020])
1: input state s; parameter œÜ; reverse length K;
2: initialize {Œ≤i}K
i=1; Œ±i =: 1 ‚àí Œ≤i, ¬ØŒ±k =: Qk
i=1 Œ±i, œÉk =:
r1 ‚àí ¬ØŒ±k‚àí1
1 ‚àí ¬ØŒ±k
Œ≤k;
3: initial ÀÜaK ‚àº N(0, I);
4: for k = K, ¬∑ ¬∑ ¬∑ , 1 do
5:
zk ‚àº N(0, I), if k > 1; else zk = 0;
6:
ÀÜak‚àí1 ‚Üê
1
‚àöŒ±k

ÀÜak ‚àí
Œ≤k
‚àö1 ‚àí ¬ØŒ±k
œµœÜ(ÀÜak, s, k)

+ œÉkzk;
7: end for
8: return ÀÜa0
C.2.2
Learning from Samples
According to the expectation version of loss (31), we know, for each pair (s, a) sampled from
experience memory, let k ‚àº Uniform({1, ¬∑ ¬∑ ¬∑ , K}) and z ‚àº N(0, I), the following empirical loss
‚Ñìd(œÜ) = ‚à•z ‚àí œµœÜ
 ‚àö¬ØŒ±ka + ‚àö1 ‚àí ¬ØŒ±kz, s, k

‚à•2
2
is a unbiased estimator of L(œÜ) deÔ¨Åned in (31).
Finally, we learn the parameter œÜ by minimizing the empirical loss ‚Ñìd(œÜ) according to
gradient decent method:
œÜ ‚Üê œÜ ‚àí Œ∑œÜ‚àáœÜ
z ‚àí œµœÜ
 ‚àö¬ØŒ±ka + ‚àö1 ‚àí ¬ØŒ±kz, s, k
2
2 ,
where œµœÜ is the step-size. For the implementation, see lines 25-28 in Algorithm 3.
C.3
Playing Actions of DIPO
In this section, we present all the details of #update experience with diffusion policy
presented in Algorithm 3.
Let Œ≤k = 1 ‚àí Œ±k, then according to Taylar formualtion, we know
‚àöŒ±k = 1 ‚àí 1
2Œ≤k + o(Œ≤k).
(43)
Recall the exponential integrator discretization (25), we know
ÀÜatk+1 ‚àí ÀÜatk =
 etk+1‚àítk ‚àí 1

(ÀÜatk + 2ÀÜsœÜ(ÀÜatk, s, T ‚àí tk)) +
‚àö
2
Z tk+1
tk
et
‚Ä≤‚àítkdwt‚Ä≤,
which implies
ÀÜatk+1 =ÀÜatk +
 etk+1‚àítk ‚àí 1

(ÀÜatk + 2ÀÜsœÜ(ÀÜatk, s, T ‚àí tk)) +
p
e2(tk+1‚àítk) ‚àí 1ztk
(39)
= ÀÜatk +

1
‚àöŒ±k
‚àí 1

(ÀÜatk + 2ÀÜsœÜ(ÀÜatk, s, T ‚àí tk)) +
r
1 ‚àí Œ±k
Œ±k
ztk
(42)
=
1
‚àöŒ±k
ÀÜatk ‚àí 2

1
‚àöŒ±k
‚àí 1

1
‚àö1 ‚àí ¬ØŒ±k
œµœÜ(ÀÜatk, s, k) +
r
1 ‚àí Œ±k
Œ±k
ztk
=
1
‚àöŒ±k
ÀÜatk ‚àí Œ≤k
‚àöŒ±k
¬∑
1
‚àö1 ‚àí ¬ØŒ±k
œµœÜ(ÀÜatk, s, k) +
r
1 ‚àí Œ±k
Œ±k
ztk,
(44)
33
where ztk ‚àº N(0, I), Eq.(44) holds since we use the fact (43), which implies
2

1
‚àöŒ±k
‚àí 1

= 2
1 ‚àí ‚àöŒ±k
‚àöŒ±k

=
Œ≤k
‚àöŒ±k
+ o
 Œ≤k
‚àöŒ±k

.
To simplify the expression, we rewrite (44) as follows,
ÀÜak+1 =
1
‚àöŒ±k
ÀÜak ‚àí Œ≤k
‚àöŒ±k
¬∑
1
‚àö1 ‚àí ¬ØŒ±k
œµœÜ(ÀÜak, s, k) +
r
1 ‚àí Œ±k
Œ±k
zk
(45)
=
1
‚àöŒ±k

ÀÜak ‚àí
Œ≤k
‚àö1 ‚àí ¬ØŒ±k
œµœÜ(ÀÜak, s, k)

+
r
1 ‚àí Œ±k
Œ±k
zk
(46)
=
1
‚àöŒ±k

ÀÜak ‚àí 1 ‚àí Œ±k
‚àö1 ‚àí ¬ØŒ±k
œµœÜ(ÀÜak, s, k)

+
r
1 ‚àí Œ±k
Œ±k
zk,
(47)
where k = 0, 1, ¬∑ ¬∑ ¬∑ , K ‚àí 1 runs forward in time, zk ‚àº N(0, I). The agent plays the last action
ÀÜaK.
Since we consider the SDE of the reverse process (4) that runs forward in time, while most
diÔ¨Äusion probability model literature (e.g., [Ho et al., 2020, Song et al., 2021]) consider the
backward version for sampling. To coordinate the relationship between the two versions, we
also present the backward version in Algorithm 4, which is essentially identical to the iteration
(47) but rewritten in the running in backward time version.
34
D
Time Derivative of KL Divergence Between DifuÔ¨Äusion Pol-
icy and True Reverse Process
In this section, we provide the time derivative of KL divergence between diÔ¨Äusion policy
(Algorithm 1) and true reverse process (deÔ¨Åned in (4)).
D.1
Time Derivative of KL Divergence at Reverse Time k = 0
In this section, we consider the case k = 0 of diÔ¨Äusion policy (see Algorithm 1 or the iteration
(9)). If k = 0, then for 0 ‚â§ t ‚â§ h, the SDE (8) is reduced as follows,
dÀÜat =

ÀÜat + 2ÀÜS(ÀÜa0, s, T)

dt +
‚àö
2dwt, t ‚àà [0, h],
(48)
where wt is the standard Wiener process starting at w0 = 0.
Let the action ÀÜat ‚àº ÀÜœÄt(¬∑|s) follows the process (48). The next Proposition D.1 considers
the distribution diÔ¨Äerence between the diÔ¨Äusion policy ÀÜœÄt(¬∑|s) and the true distribution of
backward process (4) ÀúœÄt(¬∑|s) on the time interval t ‚àà [0, h].
Proposition D.1. Under Assumption 4.1 and 4.2, let ÀúœÄt(¬∑|s) be the distribution at time t
with the process (4), and let ÀÜœÄt(¬∑|s) be the distribution at time t with the process (48). Let
œÑ0 =: sup
(
t : tet ‚â§
‚àö
5ŒΩ
96LsLp
)
, œÑ =: min

œÑ0,
1
12Ls

,
(49)
œµscore =:
sup
(k,t)‚àà[K]√ó[tk,tk+1]

log Ea‚àºÀúœÄt(¬∑|s)

exp
ÀÜS(a, s, T ‚àí hk) ‚àí ‚àá log ÀúœÄt(a|s)

2
2

,
(50)
and 0 ‚â§ t ‚â§ h ‚â§ œÑ, then the following equation holds,
d
dtKL
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

‚â§ ‚àíŒΩ
4KL (ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)) + 5
4ŒΩœµscore + 12pLs
‚àö
5ŒΩt.
(51)
Before we show the details of the proof, we need to deÔ¨Åne some notations, which is useful
later. Let
ÀÜœÄt(ÀÜa|s) =: p(ÀÜat = ÀÜa|s, t)
(52)
denote the distribution of the action ÀÜat = ÀÜa be played at time t along the process (48), where
t ‚àà [0, h]. For each t > 0, let œÅ0,t(ÀÜa0, ÀÜat|s) denote the joint distribution of (ÀÜa0, ÀÜat) conditional
on the state s, which can be written in terms of the conditionals and marginals as follows,
œÅ0|t(ÀÜa0|ÀÜat, s) = œÅ0,t(ÀÜa0, ÀÜat|s)
p(ÀÜat = ÀÜa|s, t) = œÅ0,t(ÀÜa0, ÀÜat|s)
ÀÜœÄt(ÀÜat|s)
.
D.2
Auxiliary Results For Reverse Time k = 0
Lemma D.2. Let ÀÜœÄt(ÀÜa|s) be the distribution at time t along interpolation SDE (48), where
ÀÜœÄt(ÀÜa|s) is short for p(ÀÜat = ÀÜa|s, t), which is the distribution of the action ÀÜat = ÀÜa be played at
time t alongs the process (48) among the time t ‚àà [0, h]. Then its derivation with respect to
time satisÔ¨Åes
‚àÇ
‚àÇt ÀÜœÄt(ÀÜa|s) = ‚àíÀÜœÄt(ÀÜa|s)div ¬∑

ÀÜa + 2EÀÜa0‚àºœÅ0|t(¬∑|ÀÜa,s)
ÀÜS(ÀÜa0, s, T)
ÀÜat = ÀÜa

+ ‚àÜÀÜœÄt(ÀÜa|s).
(53)
35
Before we show the details of the proof, we need to clear the divergence term div. In this
section, all the notation is deÔ¨Åned according to (17), and its value is at the point ÀÜa.
For example, in Eq.(53), the divergence term div is deÔ¨Åned as follows,
div ¬∑

ÀÜa + 2EÀÜa0‚àºœÅ0|t(¬∑|ÀÜa,s)
ÀÜS(ÀÜa0, s, T)
ÀÜat = ÀÜa

= (div ¬∑ p)(ÀÜa),
p(a) = a + 2EÀÜa0‚àºœÅ0|t(¬∑|a,s)
ÀÜS(ÀÜa0, s, T)
ÀÜat = a

.
(54)
For example, in Eq.(58), the divergence term div is deÔ¨Åned as follows,
div ¬∑

p(ÀÜa|ÀÜa0, s, t)

ÀÜa + 2ÀÜS(ÀÜa0, s, T)
 
= (div ¬∑ p)(ÀÜa),
p(a) = p(a|ÀÜa0, s, t)

a + 2ÀÜS(ÀÜa0, s, T)

.
(55)
Similar deÔ¨Ånitions are parallel in Eq.(63), from Eq.(66) to Eq.(69).
Proof. First, for a given state s, conditioning on the initial action ÀÜa0, we introduce a notation
p(¬∑|ÀÜa0, s, t) : Rp ‚Üí [0, 1],
(56)
and each
p(ÀÜa|ÀÜa0, s, t) =: p(ÀÜat = ÀÜa|ÀÜa0, s, t)
that denotes the conditional probability distribution starting from ÀÜa0 to the action ÀÜat = ÀÜa at
time t under the state s. Besides, we also know,
ÀÜœÄt(¬∑|s) = EÀÜa0‚àºN(0,I)[p(¬∑|ÀÜa0, s, t)] =
Z
Rp œÅ0(ÀÜa0)p(¬∑|ÀÜa0, s, t)dÀÜa0,
(57)
where œÅ0(¬∑) = N(0, I) is the initial action distribution for reverse process.
For each t > 0, let œÅ0,t(ÀÜa0, ÀÜat|s) denote the joint distribution of (ÀÜa0, ÀÜat) conditional on the
state s, which can be written in terms of the conditionals and marginals as follows,
œÅ0,t(ÀÜa0, ÀÜat|s) = p(ÀÜa0|s)œÅt|0(ÀÜat|ÀÜa0, s) = p(ÀÜat|s)œÅ0|t(ÀÜa0|ÀÜat, s).
Then we obtain the Fokker‚ÄìPlanck equation for the distribution p(¬∑|ÀÜa0, s, t) as follows,
‚àÇ
‚àÇtp(ÀÜa|ÀÜa0, s, t) = ‚àídiv ¬∑

p(ÀÜa|ÀÜa0, s, t)

ÀÜa + 2ÀÜS(ÀÜa0, s, T)
 
+ ‚àÜp(ÀÜa|ÀÜa0, s, t),
(58)
where the div term is deÔ¨Åned according to (16) and (17) if p(a) = p(a|ÀÜa0, s, t)

a + 2ÀÜS(ÀÜa0, s, T)

.
Furthermore, according to (57), we know
‚àÇ
‚àÇt ÀÜœÄt(ÀÜa|s) = ‚àÇ
‚àÇt
Z
Rp œÅ0(ÀÜa0)p(ÀÜa|ÀÜa0, s, t)dÀÜa0 =
Z
Rp œÅ0(ÀÜa0) ‚àÇ
‚àÇtp(ÀÜa|ÀÜa0, s, t)dÀÜa0
(59)
=
Z
Rp œÅ0(ÀÜa0)

‚àídiv ¬∑

p(ÀÜa|ÀÜa0, s, t)

ÀÜa + 2ÀÜS(ÀÜa0, s, T)
 
+ ‚àÜp(ÀÜa|ÀÜa0, s, t)

dÀÜa0
(60)
= ‚àí ÀÜœÄt(ÀÜa|s)div ¬∑ ÀÜa ‚àí 2div ¬∑

ÀÜœÄt(ÀÜa|s)EÀÜa0‚àºœÅ0|t(¬∑|ÀÜa,s)
ÀÜS(ÀÜa0, s, T)
ÀÜat = ÀÜa

+ ‚àÜÀÜœÄt(ÀÜa|s)
(61)
= ‚àí ÀÜœÄt(ÀÜa|s)div ¬∑

ÀÜa + 2EÀÜa0‚àºœÅ0|t(¬∑|ÀÜa,s)
ÀÜS(ÀÜa0, s, T)
ÀÜat = ÀÜa

+ ‚àÜÀÜœÄt(ÀÜa|s),
(62)
36
where Eq.(61) holds since: with the deÔ¨Ånition of ÀÜœÄt(ÀÜa|s) =: p(ÀÜa|s, t), we obtain
Z
Rp œÅ0(ÀÜa0)

‚àídiv ¬∑

p(ÀÜa|ÀÜa0, s, t)ÀÜa

dÀÜa0 = ‚àíÀÜœÄt(ÀÜa|s)div ¬∑ ÀÜa;
(63)
recall
ÀÜœÄt(ÀÜa|s) =: p(ÀÜat = ÀÜa|s, t),
(64)
we know
œÅ0(ÀÜa0)p(ÀÜa|ÀÜa0, s, t) = p(ÀÜa, ÀÜa0|s, t),
‚ñ∂ Bayes‚Äô theorem
p(ÀÜa, ÀÜa0|s, t) =p(ÀÜa|s, t)p(ÀÜa0|ÀÜat = ÀÜa, s, t) = ÀÜœÄt(ÀÜa|s)p(ÀÜa0|ÀÜat = ÀÜa, s, t),
(65)
then we obtain
‚àí
Z
Rp œÅ0(ÀÜa0)div ¬∑

p(ÀÜa|ÀÜa0, s, t)ÀÜS(ÀÜa0, s, T)

dÀÜa0
(66)
= ‚àí
Z
Rp div ¬∑

p(ÀÜa, ÀÜa0|s, t)ÀÜS(ÀÜa0, s, T)

dÀÜa0
(67)
= ‚àí
Z
Rp div ¬∑

ÀÜœÄt(ÀÜa|s)p(ÀÜa0|ÀÜat = ÀÜa, s, t)ÀÜS(ÀÜa0, s, T)

dÀÜa0
‚ñ∂ see Eq.(65)
= ‚àí div ¬∑

ÀÜœÄt(ÀÜa|s)
Z
Rp p(ÀÜa0|ÀÜat = ÀÜa, s, t)ÀÜS(ÀÜa0, s, T)dÀÜa0

(68)
= ‚àí div ¬∑

ÀÜœÄt(ÀÜa|s)EÀÜa0‚àºœÅ0|t(¬∑|ÀÜa,s)
ÀÜS(ÀÜa0, s, T)
ÀÜat = ÀÜa

,
(69)
where the last equation holds since
Z
Rp p(ÀÜa0|ÀÜat = ÀÜa, s, t)ÀÜS(ÀÜa0, s, T)dÀÜa0 = EÀÜa0‚àºœÅ0|t(¬∑|ÀÜa,s)
h
ÀÜS(ÀÜa0, s, T)|ÀÜat = ÀÜa
i
.
(70)
Finally, consider (60) with (63) and (69), we conclude the Lemma D.2.
‚ñ°
We consider the time derivative of KL-divergence between the distribution ÀÜœÄt(¬∑|s) and
ÀúœÄt(¬∑|s), and decompose it as follows.
Lemma D.3. The time derivative of KL-divergence between the distribution ÀÜœÄt(¬∑|s) and ÀúœÄt(¬∑|s)
can be decomposed as follows,
d
dtKL
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

=
Z
Rp
‚àÇÀÜœÄt(a|s)
‚àÇt
log ÀÜœÄt(a|s)
ÀúœÄt(a|s)da ‚àí
Z
Rp
ÀÜœÄt(a|s)
ÀúœÄt(a|s)
‚àÇÀúœÄt(a|s)
‚àÇt
da.
(71)
Proof. We consider the time derivative of KL-divergence between the distribution ÀÜœÄt(¬∑|s) and
ÀúœÄt(¬∑|s), and we know
d
dtKL
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

= d
dt
Z
Rp
ÀÜœÄt(a|s) log ÀÜœÄt(a|s)
ÀúœÄt(a|s)da
=
Z
Rp
‚àÇÀÜœÄt(a|s)
‚àÇt
log ÀÜœÄt(a|s)
ÀúœÄt(a|s)da +
Z
Rp
ÀúœÄt(a|s) ‚àÇ
‚àÇt
 ÀÜœÄt(a|s)
ÀúœÄt(a|s)

da
=
Z
Rp
‚àÇÀÜœÄt(a|s)
‚àÇt
log ÀÜœÄt(a|s)
ÀúœÄt(a|s)da +
Z
Rp
HHHHH
‚àÇÀÜœÄt(a|s)
‚àÇt
‚àí ÀÜœÄt(a|s)
ÀúœÄt(a|s)
‚àÇÀúœÄt(a|s)
‚àÇt

da
=
Z
Rp
‚àÇÀÜœÄt(a|s)
‚àÇt
log ÀÜœÄt(a|s)
ÀúœÄt(a|s)da ‚àí
Z
Rp
ÀÜœÄt(a|s)
ÀúœÄt(a|s)
‚àÇÀúœÄt(a|s)
‚àÇt
da,
(72)
37
where the last equation holds since
Z
Rp
‚àÇÀÜœÄt(a|s)
‚àÇt
da = d
dt
Z
Rp
ÀÜœÄt(a|s)da
|
{z
}
=1
= 0.
That concludes the proof.
‚ñ°
The relative entropy and relative Fisher information FI
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

can be rewritten
as follows.
Lemma D.4. The relative entropy and relative Fisher information FI
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

can be
rewritten as the following identity,
FI
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

=
Z
Rp

‚àáÀÜœÄt(a|s), ‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s)

‚àí

‚àá ÀÜœÄt(a|s)
ÀúœÄt(a|s), ‚àáÀúœÄt(a|s)

da.
Proof. We consider the following identity,
Z
Rp

‚àá ÀÜœÄt(a|s)
ÀúœÄt(a|s), ‚àáÀúœÄt(a|s)

‚àí

‚àáÀÜœÄt(a|s), ‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s)

da
=
Z
Rp
  ÀúœÄt(a|s)‚àáÀÜœÄt(a|s) ‚àí ÀÜœÄt(a|s)‚àáÀúœÄt(a|s)
ÀúœÄt(a|s)
, ‚àá log ÀúœÄt(a|s)

‚àí ÀÜœÄt(a|s)

‚àá log ÀÜœÄt(a|s), ‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s)
 !
da
=
Z
Rp
ÀÜœÄt(a|s)

‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s), ‚àá log ÀúœÄt(a|s)

da ‚àí
Z
Rp
ÀÜœÄt(a|s)

‚àá log ÀÜœÄt(a|s), ‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s)

da
= ‚àí
Z
Rp
ÀÜœÄt(a|s)

‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s), ‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s)

da
= ‚àí
Z
Rp
ÀÜœÄt(a|s)
‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s)

2
2
da =: ‚àíFI
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

,
which concludes the proof.
‚ñ°
Lemma D.4 implies the following identity, which is useful later,
Z
Rp

‚àí

‚àá ÀÜœÄt(a|s)
ÀúœÄt(a|s), ‚àáÀúœÄt(a|s)

‚àí

‚àáÀÜœÄt(a|s), ‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s)

da
=
Z
Rp

‚àá ÀÜœÄt(a|s)
ÀúœÄt(a|s), ‚àáÀúœÄt(a|s)

‚àí

‚àáÀÜœÄt(a|s), ‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s)

‚àí 2

‚àá ÀÜœÄt(a|s)
ÀúœÄt(a|s), ‚àáÀúœÄt(a|s)

da
= ‚àí FI
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

‚àí 2
Z
Rp
ÀÜœÄt(a|s)

‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s), ‚àá log ÀúœÄt(a|s)

da.
(73)
Lemma D.5. The time derivative of KL-divergence between the distribution ÀÜœÄt(¬∑|s) and ÀúœÄt(¬∑|s)
can be further decomposed as follows,
d
dtKL
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

= ‚àíFI
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

(74)
+ 2
Z
Rp
Z
Rp
œÅ0,t(ÀÜa0, a|s)

‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s), ÀÜS(ÀÜa0, s, T) ‚àí ‚àá log ÀúœÄt(a|s)

dadÀÜa0.
(75)
38
Proof. According to Lemma D.3, we need to consider the two terms in (72) correspondingly.
First term in (72). Recall Lemma D.2, we know
‚àÇ
‚àÇt ÀÜœÄt(a|s) = ‚àí ÀÜœÄt(a|s)div ¬∑

a + 2EÀÜa0‚àºœÅ0|t(¬∑|a,s)
ÀÜS(ÀÜa0, s, T)
ÀÜat = a

+ ‚àÜÀÜœÄt(a|s)
(18)
= div ¬∑

‚àí

a + 2EÀÜa0‚àºœÅ0|t(¬∑|a,s)
ÀÜS(ÀÜa0, s, T)
ÀÜat = a

ÀÜœÄt(a|s) + ‚àáÀÜœÄt(a|s)

.
To short the expression, we deÔ¨Åne a notation gt(¬∑, ¬∑) : S √ó A ‚Üí Rp as follows,
gt(s, a) =: a + 2EÀÜa0‚àºœÅ0|t(¬∑|a,s)
ÀÜS(ÀÜa0, s, T)
ÀÜat = a

,
then we rewrite the distribution at time t along interpolation SDE (48) as follows,
‚àÇ
‚àÇt ÀÜœÄt(a|s) = div ¬∑

‚àí gt(s, a)ÀÜœÄt(a|s) + ‚àáÀÜœÄt(a|s)

.
We consider the Ô¨Årst term in (72), according to integration by parts formula (20), we know
Z
Rp
‚àÇÀÜœÄt(a|s)
‚àÇt
log ÀÜœÄt(a|s)
ÀúœÄt(a|s)da =
Z
Rp
div ¬∑

‚àí gt(s, a)ÀÜœÄt(a|s) + ‚àáÀÜœÄt(a|s)

log ÀÜœÄt(a|s)
ÀúœÄt(a|s)da
(20)
=
Z
Rp

gt(s, a)ÀÜœÄt(a|s) ‚àí ‚àáÀÜœÄt(a|s), ‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s)

da
=
Z
Rp
ÀÜœÄt(a|s)

gt(s, a), ‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s)

da
‚àí
Z
Rp

‚àáÀÜœÄt(a|s), ‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s)

da.
(76)
Second term in (72). According to the Kolmogorov backward equation, we know
‚àÇÀúœÄt(a|s)
‚àÇt
= ‚àídiv ¬∑ (ÀúœÄt(a|s)a) ‚àí ‚àÜÀúœÄt(a|s) = ‚àídiv ¬∑

ÀúœÄt(a|s)a + ‚àáÀúœÄt(a|s)

,
(77)
then we obtain
Z
Rp
ÀÜœÄt(a|s)
ÀúœÄt(a|s)
‚àÇÀúœÄt(a|s)
‚àÇt
da = ‚àí
Z
Rp
ÀÜœÄt(a|s)
ÀúœÄt(a|s)div ¬∑

ÀúœÄt(a|s)a + ‚àáÀúœÄt(a|s)

da
(20)
=
Z
Rp

‚àá ÀÜœÄt(a|s)
ÀúœÄt(a|s), ÀúœÄt(a|s)a + ‚àáÀúœÄt(a|s)

da
=
Z
Rp
ÀúœÄt(a|s)

‚àá ÀÜœÄt(a|s)
ÀúœÄt(a|s), a

da +
Z
Rp

‚àá ÀÜœÄt(a|s)
ÀúœÄt(a|s), ‚àáÀúœÄt(a|s)

da.
(78)
Time derivative of KL-divergence. We consider the next identity,
Z
Rp

ÀÜœÄt(a|s)

gt(s, a), ‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s)

‚àí ÀúœÄt(a|s)

‚àá ÀÜœÄt(a|s)
ÀúœÄt(a|s), a

da
39
=
Z
Rp

ÀÜœÄt(a|s)

gt(s, a), ‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s)

‚àí ÀÜœÄt(a|s) ÀúœÄt(a|s)
ÀÜœÄt(a|s)

‚àá ÀÜœÄt(a|s)
ÀúœÄt(a|s), a

da
=2
Z
Rp
ÀÜœÄt(a|s)

EÀÜa0‚àºœÅ0|t(¬∑|a,s)
ÀÜS(ÀÜa0, s, T)
ÀÜat = a

, ‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s)

da,
then according to (72), and with the results (76), (78), we obtain
d
dtKL
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

= ‚àíFI
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

(79)
+ 2
Z
Rp
ÀÜœÄt(a|s)

‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s), EÀÜa0‚àºœÅ0|t(¬∑|a,s)
ÀÜS(ÀÜa0, s, T)
ÀÜat = a

‚àí ‚àá log ÀúœÄt(a|s)

da.
Furthermore, we consider
Z
Rp
ÀÜœÄt(a|s)

‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s), EÀÜa0‚àºœÅ0|t(¬∑|a,s)
ÀÜS(ÀÜa0, s, T)
ÀÜat = a

‚àí ‚àá log ÀúœÄt(a|s)

da
=
Z
Rp
Z
Rp
œÅ0,t(ÀÜa0, a|s)

‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s), ÀÜS(ÀÜa0, s, T) ‚àí ‚àá log ÀúœÄt(a|s)

dadÀÜa0,
(80)
where Eq.(80) holds due to œÅ0,t(ÀÜa0, ÀÜat|s) denotes the joint distribution of (ÀÜa0, ÀÜat) conditional
on the state s, which can be written in terms of the conditionals and marginals as follows,
œÅ0|t(ÀÜa0|ÀÜat, s) = œÅ0,t(ÀÜa0, ÀÜat|s)
pt(ÀÜat|s)
= œÅ0,t(ÀÜa0, ÀÜat|s)
ÀÜœÄt(ÀÜat|s)
;
(81)
and in Eq.(80), we denote ÀÜat = a.
Finally, combining (79) and (80), we obtain the following equation,
d
dtKL
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

= ‚àíFI
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

(82)
+ 2
Z
Rp
Z
Rp
œÅ0,t(ÀÜa0, a|s)

‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s), ÀÜS(ÀÜa0, s, T) ‚àí ‚àá log ÀúœÄt(a|s)

dadÀÜa0,
(83)
which concludes the proof.
‚ñ°
Lemma D.6. The time derivative of KL-divergence between the distribution ÀÜœÄt(¬∑|s) and ÀúœÄt(¬∑|s)
is bounded as follows,
d
dtKL
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

‚â§ ‚àí 3
4FI
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

+ 4
Z
Rp
Z
Rp
œÅ0,t(ÀÜa0, a|s)
ÀÜS(ÀÜa0, s, T) ‚àí ‚àá log ÀúœÄt(a|s)

2
2dadÀÜa0.
(84)
Proof. First, we consider
Z
Rp
Z
Rp
œÅ0,t(ÀÜa0, a|s)

‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s), ÀÜS(ÀÜa0, s, T) ‚àí ‚àá log ÀúœÄt(a|s)

dadÀÜa0
40
‚â§
Z
Rp
Z
Rp
œÅ0,t(ÀÜa0, a|s)
 
2
ÀÜS(ÀÜa0, s, T) ‚àí ‚àá log ÀúœÄt(a|s)

2
2 + 1
8
‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s)

2
2
!
dadÀÜa0
(85)
=2
Z
Rp
Z
Rp
œÅ0,t(ÀÜa0, a|s)
ÀÜS(ÀÜa0, s, T) ‚àí ‚àá log ÀúœÄt(a|s)

2
2dadÀÜa0 + 1
8FI
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

,
(86)
where Eq.(85) holds since we consider ‚ü®a, b‚ü© ‚â§ 2‚à•a‚à•2 + 1
8‚à•b‚à•2.
Then, according to Lemma D.5, we obtain
d
dtKL
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

= ‚àíFI
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

+ 2
Z
Rp
Z
Rp
œÅ0,t(ÀÜa0, a|s)

‚àá log ÀÜœÄt(a|s)
ÀúœÄt(a|s), ÀÜS(ÀÜa0, s, T) ‚àí ‚àá log ÀúœÄt(a|s)

dadÀÜa0
‚â§ ‚àí 3
4FI
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

+ 4
Z
Rp
Z
Rp
œÅ0,t(ÀÜa0, a|s)
ÀÜS(ÀÜa0, s, T) ‚àí ‚àá log ÀúœÄt(a|s)

2
2dadÀÜa0,
(87)
which concludes the proof.
‚ñ°
Before we provide further analysis to show the boundedness of (87)., we need to consider
SDE (8). Let h > 0 be the step-size, assume K = T
h ‚àà N, and tk =: hk, k = 0, 1, ¬∑ ¬∑ ¬∑ , K. SDE
(8) considers as follows, for t ‚àà [hk, h(k + 1)],
dÀÜat =

ÀÜat + 2ÀÜS(ÀÜatk, s, T ‚àí tk)

dt +
‚àö
2dwt,
(88)
Recall the SDE (88), in this section, we only consider k = 0, and we obtain the following
SDE,
dÀÜat =

ÀÜat + 2ÀÜS(ÀÜa0, s, T)

dt +
‚àö
2dwt,
(89)
where wt is the standard Wiener process starting at w0 = 0, and t is from 0 to h.
Integration with (89), we obtain
ÀÜat ‚àí ÀÜa0 = (et ‚àí 1)

ÀÜa0 + 2ÀÜS(ÀÜa0, s, T)

+
‚àö
2
Z t
0
etdwt,
(90)
which implies
ÀÜat = etÀÜa0 + 2(et ‚àí 1)ÀÜS(ÀÜa0, s, T) +
p
et ‚àí 1z, z ‚àº N(0, I).
(91)
Lemma D.7. Under Assumption 4.1, for all 0 ‚â§ t ‚â§
1
12Ls , then the following holds,
Z
Rp
Z
Rp
œÅ0,t(ÀÜa0, a|s)
ÀÜS(ÀÜa0, s, T) ‚àí ‚àá log ÀúœÄt(a|s)

2
2dadÀÜa0
‚â§36pt(1 + t)L2
s + 144t2L2
s
Z
Rp
ÀÜœÄt(a|s)
ÀÜS(a, s, T) ‚àí ‚àá log ÀúœÄt(a|s)

2
2 +
‚àá log ÀúœÄt(a|s)

2
2

da,
where ÀÜat updated according to (91).
Proof. See Section F.2.
‚ñ°
41
D.3
Proof for Result at Reverse Time k = 0
Proof. According to the deÔ¨Ånition of diÔ¨Äusion policy, we know ÀúœÄt(¬∑|s) = ¬ØœÄT‚àít(¬∑|s). Then
according to Proposition B.4, we know ÀúœÄt(¬∑|s) is ŒΩT‚àít-LSI, where
ŒΩT‚àít =
ŒΩ
ŒΩ + (1 ‚àí ŒΩ)e‚àí2(T‚àít) .
Since we consider the time-step 0 ‚â§ t ‚â§ T, then
ŒΩT‚àít =
ŒΩ
ŒΩ + (1 ‚àí ŒΩ)e‚àí2(T‚àít) ‚â• 1, ‚àÄt ‚àà [0, T].
(92)
According to Proposition B.5, we know under Assumption 4.1, ‚àá log ÀúœÄt(¬∑|s) is Lpet-Lipschitz
on the time interval [0, T0], where
T0 =: sup
t‚â•0

t : 1 ‚àí e‚àí2t ‚â§ et
Lp

.
Then according to Proposition B.6, we obtain
Z
Rp
ÀÜœÄt(a|s)
‚àá log ÀúœÄt(a|s)

2
2da ‚â§ 4L2
pe2t
ŒΩT‚àít
KL (ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)) + 2pLpet.
(93)
Furthermore, according to Donsker-Varadhan representation (see Section B.5), let
f(a) =: Œ≤t
ÀÜS(a, s, T) ‚àí ‚àá log ÀúœÄt(a|s)

2
2,
the positive constant Œ≤t will be special later, see Eq.(98). With the result (29), we know
KL
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

‚â•
Z
Rp ÀÜœÄt(a|s)f(a)da ‚àí log
Z
Rp ÀúœÄt(a|s) exp(f(a))da,
which implies
Z
Rp
ÀÜœÄt(a|s)
ÀÜS(a, s, T) ‚àí ‚àá log ÀúœÄt(a|s)

2
2da
‚â§ 1
Œ≤t
KL
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

+ 1
Œ≤t
log
Z
Rp
ÀúœÄt(a|s) exp
ÀÜS(a, s, T) ‚àí ‚àá log ÀúœÄt(a|s)

2
2

da
= 1
Œ≤t
KL
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

+ 1
Œ≤t
log Ea‚àºÀúœÄt(¬∑|s)

exp
ÀÜS(a, s, T) ‚àí ‚àá log ÀúœÄt(a|s)

2
2

.
(94)
Finally, according to Lemma D.6-D.7, Eq.(93)-(94), we obtain
d
dtKL
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

(87)
‚â§ ‚àí 3
4FI
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

+ 4
Z
Rp
Z
Rp
œÅ0,t(ÀÜa0, a|s)
ÀÜS(ÀÜa0, s, T) ‚àí ‚àá log ÀúœÄt(a|s)

2
2dadÀÜa0
Lemma D.7
‚â§
‚àí 3
4FI
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

+ 576t2L2
s
Z
Rp
ÀÜœÄt(a|s)
‚àá log ÀúœÄt(a|s)

2
2da
42
+ 576t2L2
s
Z
Rp
ÀÜœÄt(a|s)
ÀÜS(a, s, T) ‚àí ‚àá log ÀúœÄt(a|s)

2
2da
‚â§ ‚àí 3
4FI
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

+ 576t2L2
s
 
4L2
pe2t
ŒΩT‚àít
KL (ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)) + 2pLpet
!
‚ñ∂ due to Eq.(93)
+ 576t2L2
s
Œ≤t

KL
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

+ log Ea‚àºÀúœÄt(¬∑|s)

exp
ÀÜS(a, s, T) ‚àí ‚àá log ÀúœÄt(a|s)

2
2
 
‚ñ∂ due to Eq.(94)
= ‚àí 3
4FI
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

+ 576t2L2
s
 
4L2
pe2t
ŒΩT‚àít
+ 1
Œ≤t
!
KL (ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s))
+ 576t2L2
s
Œ≤t
log Ea‚àºÀúœÄt(¬∑|s)

exp
ÀÜS(a, s, T) ‚àí ‚àá log ÀúœÄt(a|s)

2
2

+ 1152t2pL2
sLpet
‚â§
 
576t2L2
s
 
4L2
pe2t
ŒΩT‚àít
+ 1
Œ≤t
!
‚àí 3
2ŒΩ
!
KL (ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)) + 576t2L2
s
Œ≤t
œµscore + 1152t2pL2
sLpet
‚ñ∂ due to Assumption 4.2
=

576t2L2
s

4ct + 1
Œ≤t

‚àí 3
2ŒΩ

KL (ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)) + 576t2L2
s
Œ≤t
œµscore + 1152t2pL2
sLpet
‚ñ∂ due to
L2
pe2t
ŒΩT ‚àít =: ct
(98)
= ‚àí ŒΩ
4KL (ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)) + 576t2L2
s
Œ≤t
œµscore + 1152t2pL2
sLpet
(95)
(99)
‚â§ ‚àí ŒΩ
4KL (ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)) + 5
4ŒΩœµscore + 1152t2pL2
sLpet
(96)
where
œµscore =
sup
(k,t)‚àà[K]√ó[kh,(k+1)h]

log Ea‚àºÀúœÄt(¬∑|s)

exp
ÀÜS(a, s, T ‚àí hk) ‚àí ‚àá log ÀúœÄt(a|s)

2
2

;
(97)
Eq.(95) holds since we set Œ≤t as follows, we set 576t2L2
s

4ct + 1
Œ≤t

= 5ŒΩ
4 , i.e,
1
Œ≤t
=
5ŒΩ
2304t2L2s
‚àí 4ct;
(98)
where Eq.(96) holds since
576t2L2
s
Œ≤t
= 576t2L2
s

5ŒΩ
2304t2L2s
‚àí 4ct

‚â§ 5ŒΩ
4 .
(99)
Now, we consider the time-step t keeps the constant Œ≤t positive, it is suÔ¨Écient to consider
the next condition due to the property (92),
5ŒΩ
2304t2L2s
‚â• 4L2
pe2t,
(100)
which implies tet ‚â§
‚àö
5ŒΩ
96LsLp
.
43
Formally, we deÔ¨Åne a notation
œÑ0 =: sup
(
t : tet ‚â§
‚àö
5ŒΩ
96LsLp
)
,
(101)
œÑ =: min

œÑ0, T0,
1
12Ls

.
(102)
Then with result of Eq.(100), if 0 ‚â§ t ‚â§ h ‚â§ œÑ, we rewrite Eq.(96) as follows,
d
dtKL
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

‚â§ ‚àí ŒΩ
4KL (ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)) + 5
4ŒΩœµscore + 12pLs
‚àö
5ŒΩt
= ‚àí ŒΩ
4KL (ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)) + 12pLs
‚àö
5ŒΩt
+5
4ŒΩ
sup
(k,t)‚àà[K]√ó[tk,tk+1]

log Ea‚àºÀúœÄt(¬∑|s)

exp
ÀÜS(a, s, T ‚àí hk) ‚àí ‚àá log ÀúœÄt(a|s)

2
2

,
(103)
which concludes the proof.
‚ñ°
Remark D.8. The result of Proposition B.5 only depends on Assumption 4.1, thus the result
(93) does not depend on additional assumption of the uniform L-smooth of log ÀúœÄt on the time
interval [0, T], e.g., Wibisono and Yang [2022]. Instead of the uniform L-smooth of log ÀúœÄt, we
consider the Lpet-Lipschitz on the time interval [0, T0], which is one of the diÔ¨Äerence between
our proof and [Wibisono and Yang, 2022]. Although we obtain a similar convergence rate from
the view of Langevin-based algorithms, we need a weak condition.
D.4
Proof for Result at Arbitrary Reverse Time k
Proposition D.9. Under Assumption 4.1 and 4.2. Let ÀúœÄk(¬∑|s) be the distribution at the time
t = hk along the process (4) that starts from ÀúœÄ0(¬∑|s) = ¬ØœÄT (¬∑|s), then ÀúœÄk(¬∑|s) = ¬ØœÄT‚àíhk(¬∑|s).
Let ÀÜœÄk(¬∑|s) be the distribution of the iteration (9) at the k-the time tk = hk, starting from
ÀÜœÄ0(¬∑|s) = N(0, I). Let 0 < h ‚â§ œÑ, then for all k = 0, 1, ¬∑ ¬∑ ¬∑ , K ‚àí 1,
KL
 ÀÜœÄk+1(¬∑|s)‚à•ÀúœÄk+1(¬∑|s)

‚â§e‚àí 1
4 ŒΩhKL
 ÀÜœÄk(¬∑|s)‚à•ÀúœÄk(¬∑|s)

+ 5
4ŒΩœµscoreh + 12pLs
‚àö
5ŒΩh2,
where œÑ is deÔ¨Åned in (102).
Proof. Recall Proposition D.1, we know for any 0 ‚â§ t ‚â§ h ‚â§ œÑ, the following holds
d
dtKL
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

‚â§ ‚àíŒΩ
4KL (ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)) + 5
4ŒΩœµscore + 12pLs
‚àö
5ŒΩh,
(104)
where comparing to (51), we use the condition t ‚â§ h.
We rewrite (104) as follows,
d
dt

e
1
4 ŒΩtKL
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

‚â§ e
1
4 ŒΩt
5
4ŒΩœµscore + 12pLs
‚àö
5ŒΩh

.
Then, on the interval [0, h], we obtain
Z
h
0
d
dt

e
1
4 ŒΩtKL
 ÀÜœÄt(¬∑|s)‚à•ÀúœÄt(¬∑|s)

dt ‚â§
Z
h
0
e
1
4 ŒΩt
5
4ŒΩœµscore + 12pLs
‚àö
5ŒΩh

dt,
44
which implies
e
1
4 ŒΩhKL
 ÀÜœÄh(¬∑|s)‚à•ÀúœÄh(¬∑|s)

‚â§ KL
 ÀÜœÄ0(¬∑|s)‚à•ÀúœÄ0(¬∑|s)

+ 4
ŒΩ

e
1
4 ŒΩh ‚àí 1
 5
4ŒΩœµscore + 12pLs
‚àö
5ŒΩh

.
Furthermore, we obtain
KL
 ÀÜœÄh(¬∑|s)‚à•ÀúœÄh(¬∑|s)

‚â§e‚àí 1
4 ŒΩhKL
 ÀÜœÄ0(¬∑|s)‚à•ÀúœÄ0(¬∑|s)

+ 4
ŒΩ

1 ‚àí e‚àí 1
4 ŒΩh 5
4ŒΩœµscore + 12pLs
‚àö
5ŒΩh

‚â§e‚àí 1
4 ŒΩhKL
 ÀÜœÄ0(¬∑|s)‚à•ÀúœÄ0(¬∑|s)

+ 5
4ŒΩœµscoreh + 12pLs
‚àö
5ŒΩh2,
(105)
where last equation holds since we use 1 ‚àí e‚àíx ‚â§ x, if x ‚â• 0.
Recall ÀúœÄk(¬∑|s) is the distribution at the time t = hk along the process (4) that starts from
ÀúœÄ0(¬∑|s) = ¬ØœÄT (¬∑|s), then ÀúœÄk(¬∑|s) = ¬ØœÄT‚àíhk(¬∑|s).
Recall ÀÜœÄk(¬∑|s) is the distribution of the iteration (9) at the k-the time tk = hk, starting
from ÀÜœÄ0(¬∑|s) = N(0, I).
According to (105), we rename the ÀúœÄ0(¬∑|s) with ÀúœÄk(¬∑|s), ÀúœÄh(¬∑|s) with ÀúœÄk+1(¬∑|s), ÀÜœÄ0(¬∑|s) with
ÀÜœÄk(¬∑|s) and ÀÜœÄh(¬∑|s) with ÀÜœÄk+1(¬∑|s), then we obtain
KL
 ÀÜœÄk+1(¬∑|s)‚à•ÀúœÄk+1(¬∑|s)

‚â§e‚àí 1
4 ŒΩhKL
 ÀÜœÄk(¬∑|s)‚à•ÀúœÄk(¬∑|s)

+ 5
4ŒΩœµscoreh + 12pLs
‚àö
5ŒΩh2,
which concludes the result.
‚ñ°
E
Proof of Theorem 4.3
Theorem 4.3 (Finite-time Analysis of DiÔ¨Äusion Policy).
For a given state s, let {¬ØœÄt(¬∑|s)}t=0:T
and {ÀúœÄt(¬∑|s)}t=0:T be the distributions along the Ornstein-Uhlenbeck Ô¨Çow (2) and (4) corre-
spondingly, where {¬ØœÄt(¬∑|s)}t=0:T starts at ¬ØœÄ0(¬∑|s) = œÄ(¬∑|s) and {ÀúœÄt(¬∑|s)}t=0:T starts at ÀúœÄ0(¬∑|s) =
¬ØœÄT (¬∑|s). Let ÀÜœÄk(¬∑|s) be the distribution of the exponential integrator discretization iteration (9)
at the k-the time tk = hk, i.e., ÀÜatk ‚àº ÀÜœÄk(¬∑|s) denotes the distribution of the diÔ¨Äusion policy
(see Algorithms 1) at the time tk = hk. Let {ÀÜœÄk(¬∑|s)}k=0:K be starting at ÀÜœÄ0(¬∑|s) = N(0, I),
under Assumption 4.1 and 4.2, let the reverse length K satisfy
K ‚â• T ¬∑ max
 1
œÑ0
, 1
T0
, 12Ls, ŒΩ

,
where
œÑ0 =: sup
t‚â•0
(
t : tet ‚â§
‚àö
5ŒΩ
96LsLp
)
, T0 =: sup
t‚â•0

t : 1 ‚àí e‚àí2t ‚â§ et
Lp

.
Then the KL-divergence between the diÔ¨Äusion policy ÀÜaK ‚àº ÀÜœÄK(¬∑|s) and input policy œÄ(¬∑|s) is
upper-bounded as follows,
KL
 ÀÜœÄK(¬∑|s)‚à•œÄ(¬∑|s)

‚â§ e‚àí 9
4 ŒΩhKKL
 N(0, I)‚à•œÄ(¬∑|s)

|
{z
}
convergence of forward process
+
64pLs
r
5
ŒΩ ¬∑ T
K
|
{z
}
errors from discretization
+ 20
3
sup
(k,t)‚àà[K]√ó[tk,tk+1]

log Ea‚àºÀúœÄt(¬∑|s)

exp
ÀÜS(a, s, T ‚àí hk) ‚àí ‚àá log ÀúœÄt(a|s)

2
2

|
{z
}
errors from score matching
.
45
Proof. Recall ÀúœÄk(¬∑|s) = ¬ØœÄT‚àíhk(¬∑|s), then we know
ÀúœÄK(¬∑|s) = ¬ØœÄT‚àíhK(¬∑|s) = ¬ØœÄ0(¬∑|s) = œÄ(¬∑|s),
(106)
then according to Proposition D.9, we know
KL
 ÀÜœÄK(¬∑|s)‚à•œÄ(¬∑|s)
 (106)
= KL
 ÀÜœÄK(¬∑|s)‚à•ÀúœÄK(¬∑|s)

‚â§e‚àí 1
4 ŒΩKKL
 ÀÜœÄ0(¬∑|s)‚à•ÀúœÄ0(¬∑|s)

+
K‚àí1
X
j=0
e‚àí 1
4 ŒΩhj
5
4ŒΩœµscoreh + 12pLs
‚àö
5ŒΩh2

‚â§e‚àí 1
4 ŒΩKKL
 ÀÜœÄ0(¬∑|s)‚à•ÀúœÄ0(¬∑|s)

+
1
1 ‚àí e‚àí 1
4 ŒΩh
5
4ŒΩœµscoreh + 12pLs
‚àö
5ŒΩh2

‚â§e‚àí 1
4 ŒΩKKL
 ÀÜœÄ0(¬∑|s)‚à•ÀúœÄ0(¬∑|s)

+ 16
3ŒΩh
5
4ŒΩœµscoreh + 12pLs
‚àö
5ŒΩh2

(107)
=e‚àí 1
4 ŒΩKKL
 ÀÜœÄ0(¬∑|s)‚à•ÀúœÄ0(¬∑|s)

+ 20
3 œµscore + 64
r
5
ŒΩ pLsh,
(108)
where Eq.(107) holds since we consider the
1 ‚àí e‚àíx ‚â• 3
4x, if 0 < x ‚â§ 1
4,
(109)
and we set the step-size h satisÔ¨Åes the next condition:
hŒΩ ‚â§ 1, i.e., h ‚â§ 1
ŒΩ .
Let Œæ(¬∑) be standard Gaussian distribution on Rp, i.e., Œæ(¬∑) ‚àº N(0, I), then we obtain the
following result: for a given state s,
d
dtKL (Œæ(¬∑)‚à•¬ØœÄt(¬∑|s)) = d
dt
Z
Rp
Œæ(a) log
Œæ(a)
¬ØœÄt(a|s)da
= ‚àí
Z
Rp
Œæ(a)
¬ØœÄt(a|s)
‚àÇ¬ØœÄt(a|s)
‚àÇt
da
= ‚àí
Z
Rp
Œæ(a)
¬ØœÄt(a|s)

div ¬∑

¬ØœÄt(a|s)‚àá log ¬ØœÄt(a|s)
Œæ(a)

da
‚ñ∂ Fokker‚ÄìPlanck Equation
=
Z
Rp

‚àá Œæ(a)
¬ØœÄt(a|s), ¬ØœÄt(a|s)‚àá log ¬ØœÄt(a|s)
Œæ(a)

da
‚ñ∂ Integration by Parts
=
Z
Rp
 Œæ(a)
¬ØœÄt(a|s)‚àá log
Œæ(a)
¬ØœÄt(a|s), ¬ØœÄt(a|s)‚àá log ¬ØœÄt(a|s)
Œæ(a)

da
=
Z
Rp
Œæ(a)

‚àá log
Œæ(a)
¬ØœÄt(a|s), ‚àá log ¬ØœÄt(a|s)
Œæ(a)

da
= ‚àí
Z
Rp
Œæ(a)
‚àá log
Œæ(a)
¬ØœÄt(a|s)

2
2
= ‚àíEa‚àºŒæ(¬∑)
"‚àá log
Œæ(a)
¬ØœÄt(a|s)

2
2
#
46
= ‚àíFI (Œæ(¬∑)‚à•¬ØœÄt(¬∑|s))
‚â§ ‚àí2ŒΩtKL (Œæ(¬∑)‚à•¬ØœÄt(¬∑|s))
‚ñ∂ Assumption 4.2 and Proposition B.4
= ‚àí
2ŒΩ
ŒΩ + (1 ‚àí ŒΩ)e‚àí2t KL (Œæ(¬∑)‚à•¬ØœÄt(¬∑|s))
‚â§ ‚àí2ŒΩKL (Œæ(¬∑)‚à•¬ØœÄt(¬∑|s)) ,
(110)
where the last equation holds since e‚àít ‚â§ 1 with t ‚â• 0.
Eq.(110) implies
d
dt log KL (Œæ(¬∑)‚à•¬ØœÄt(¬∑|s)) ‚â§ ‚àí2ŒΩ,
integrating both sides of above equation on the interval [0, T], we obtain
KL (Œæ(¬∑)‚à•¬ØœÄT (¬∑|s)) ‚â§ e‚àí2ŒΩT KL (Œæ(¬∑)‚à•¬ØœÄ0(¬∑|s)) .
(111)
According to deÔ¨Ånition of diÔ¨Äusion policy, since: ÀÜa0 ‚àº N(0, I), and ÀúœÄ0(¬∑|s)
(5)
= ¬ØœÄT (¬∑|s), then we
know
KL
 ÀÜœÄ0(¬∑|s)‚à•ÀúœÄ0(¬∑|s)

= KL
 Œæ(¬∑)‚à•¬ØœÄT (¬∑|s)

,
(112)
which implies
KL
 ÀÜœÄ0(¬∑|s)‚à•ÀúœÄ0(¬∑|s)
 (112)
= KL
 Œæ(¬∑)‚à•¬ØœÄT (¬∑|s)
 (111)
‚â§ e‚àí2ŒΩT KL (Œæ(¬∑)‚à•¬ØœÄ0(¬∑|s)) .
(113)
Combining (108) and (113), we obtain
KL
 ÀÜœÄK(¬∑|s)‚à•œÄ(¬∑|s)

‚â§e‚àí 1
4 ŒΩhK‚àíT KL (Œæ(¬∑)‚à•¬ØœÄ0(¬∑|s)) + 20
3 œµscore + 64
r
5
ŒΩ pLsh
(106)
= e‚àí 9
4 ŒΩhKKL
 N(0, I)‚à•œÄ(¬∑|s)

+ 20
3 œµscore + 64
r
5
ŒΩ pLsh.
(114)
Recall the following conditions (101), (102), and (109) on the step-size h,
h ‚â§ min

œÑ0, T0,
1
12Ls
, 1
ŒΩ

,
which implies the reverse length K satisfy the following condition
K = T
h ‚â• T ¬∑ max
 1
œÑ0
, 1
T0
, 12Ls, ŒΩ

.
Finally, recall the deÔ¨Ånition of œµ (97), we rewrite (114) as follows
KL
 ÀÜœÄK(¬∑|s)‚à•œÄ(¬∑|s)

‚â§ e‚àí 9
4 ŒΩhKKL
 N(0, I)‚à•œÄ(¬∑|s)

+ 64pLs
r
5
ŒΩ ¬∑ T
K
+ 20
3
sup
(k,t)‚àà[K]√ó[tk,tk+1]

log Ea‚àºÀúœÄt(¬∑|s)

exp
ÀÜS(a, s, T ‚àí hk) ‚àí ‚àá log ÀúœÄt(a|s)

2
2

,
which concludes the proof.
‚ñ°
47
F
Additional Details
F.1
Proof of Lemma F.1
Lemma F.1. Under Assumption 4.1, for all 0 ‚â§ t
‚Ä≤ ‚â§ T, if t ‚â§
1
12Ls , then for any given state
s
ÀÜS

ÀÜat, s, t
‚Ä≤
‚àí ÀÜS

ÀÜa0, s, t
‚Ä≤ ‚â§ 3Lst ‚à•ÀÜa0‚à• + 6Lst
ÀÜS

ÀÜat, s, t
‚Ä≤ + 3Ls
‚àö
t‚à•z‚à•,
and
ÀÜS

ÀÜat, s, t
‚Ä≤
‚àí ÀÜS

ÀÜa0, s, t
‚Ä≤
2
2 ‚â§ 36L2
st2 ‚à•ÀÜa0‚à•2
2 + 72L2
st2 ÀÜS

ÀÜat, s, t
‚Ä≤
2
2 + 36L2
st‚à•z‚à•2
2,
(115)
where ÀÜat updated according to (91).
Proof. (of Lemma F.1). First, we consider
ÀÜS

ÀÜat, s, t
‚Ä≤
‚àí ÀÜS

ÀÜa0, s, t
‚Ä≤ ‚â§Ls ‚à•ÀÜat ‚àí ÀÜa0‚à•
=
(et ‚àí 1)ÀÜa0 + 2(et ‚àí 1)ÀÜS(ÀÜa0, s, t
‚Ä≤) +
p
et ‚àí 1z

‚â§2Lst ‚à•ÀÜa0‚à•2
2 + 4Lst
ÀÜS(ÀÜa0, s, t
‚Ä≤)
 + 2Ls
‚àö
t‚à•z‚à•,
(116)
where the last equation holds due to et ‚àí 1 ‚â§ 2t.
Furthermore, we consider the case with t ‚â§
1
12Ls , then we obtain the boundedness of the
term
ÀÜS

ÀÜa0, s, t
‚Ä≤ ‚â§
ÀÜS

ÀÜat, s, t
‚Ä≤ + Ls ‚à•ÀÜat ‚àí ÀÜa0‚à•
‚â§
ÀÜS

ÀÜat, s, t
‚Ä≤ + 2Lst ‚à•ÀÜa0‚à• + 4Lst
ÀÜS(ÀÜa0, s, t
‚Ä≤)
 + 2Ls
‚àö
t‚à•z‚à•
‚â§
ÀÜS

ÀÜat, s, t
‚Ä≤ + 2Lst ‚à•ÀÜa0‚à• + 1
3
ÀÜS(ÀÜa0, s, t
‚Ä≤)
 + 2Ls
‚àö
t‚à•z‚à•,
which implies
ÀÜS

ÀÜa0, s, t
‚Ä≤ ‚â§ 3
2
ÀÜS

ÀÜat, s, t
‚Ä≤ + 3Lst ‚à•ÀÜa0‚à•2
2 + 3Ls
‚àö
t‚à•z‚à•.
(117)
Taking Eq.(117) into Eq.(116), and with t ‚â§
1
12Ls , we obtain
ÀÜS

ÀÜat, s, t
‚Ä≤
‚àí ÀÜS

ÀÜa0, s, t
‚Ä≤ ‚â§ 3Lst ‚à•ÀÜa0‚à• + 6Lst
ÀÜS

ÀÜat, s, t
‚Ä≤ + 3Ls
‚àö
t‚à•z‚à•.
Finally, we know
ÀÜS

ÀÜat, s, t
‚Ä≤
‚àí ÀÜS

ÀÜa0, s, t
‚Ä≤
2
2 ‚â§ 36L2
st2 ‚à•ÀÜa0‚à•2
2 + 72L2
st2 ÀÜS

ÀÜat, s, t
‚Ä≤
2
2 + 36L2
st‚à•z‚à•2
2,
(118)
which concludes the proof of Lemma F.1.
‚ñ°
48
F.2
Proof of Lemma D.7
Proof.(Lemma D.7) Recall the update rule of ÀÜat (91),
ÀÜat = etÀÜa0 + 2(et ‚àí 1)ÀÜS(ÀÜa0, s, T) +
p
et ‚àí 1z, z ‚àº N(0, I).
To simplify the expression, in this section, we introduce the following notation
z ‚àº œÅz(¬∑), where œÅz(¬∑) = N(0, I).
(119)
According to the deÔ¨Ånition of œÅ0,t(ÀÜa0, ÀÜat|s) (81), we denote ÀÜat = a, then we know,
Z
Rp
Z
Rp
œÅ0,t(ÀÜa0, a|s)
ÀÜS(ÀÜa0, s, T) ‚àí ‚àá log ÀúœÄt(a|s)

2
2dadÀÜa0
‚â§2
Z
Rp
Z
Rp
œÅ0,t(ÀÜa0, a|s)
ÀÜS(ÀÜa0, s, T) ‚àí ÀÜS(ÀÜat, s, T)

2
2 +
ÀÜS(ÀÜat, s, T) ‚àí ‚àá log ÀúœÄt(a|s)

2
2

dadÀÜa0
=2
Z
Rp
Z
Rp
œÅ0,t(ÀÜa0, a|s)
ÀÜS(ÀÜa0, s, T) ‚àí ÀÜS(a, s, T)

2
2 +
ÀÜS(a, s, T) ‚àí ‚àá log ÀúœÄt(a|s)

2
2

dadÀÜa0
Recall Lemma F.1, we know
Z
Rp
Z
Rp
œÅ0,t(ÀÜa0, a|s)
ÀÜS(ÀÜa0, s, T) ‚àí ÀÜS(a, s, T)

2
2dadÀÜa0
(115)
‚â§
Z
Rp
Z
Rp
Z
Rp
œÅ0,t(ÀÜa0, a|s)œÅz(z)

36L2
st2 ‚à•ÀÜa0‚à•2
2 + 72L2
st2 ÀÜS (a, s, T)

2
2 + 36L2
st‚à•z‚à•2
2

dadÀÜa0dz
=
Z
Rp
Z
Rp
Z
Rp
œÅ0,t(ÀÜa0, a|s)œÅz(z)

36L2
st2 ‚à•ÀÜa0‚à•2
2 + 72L2
st2 ÀÜS (a, s, T)

2
2

dadÀÜa0dz
+ 36L2
st
Z
Rp
Z
Rp
Z
Rp
œÅ0,t(ÀÜa0, a|s)œÅz(z)‚à•z‚à•2
2dadÀÜa0dz
=36L2
st2
Z
Rp
ÀÜœÄ0(ÀÜa0|s) ‚à•ÀÜa0‚à•2
2 dÀÜa0 + 72L2
st2
Z
Rp
ÀÜœÄt(a|s)
ÀÜS (a, s, T)

2
2 da + 36L2
spt
(120)
=36L2
spt2 + 72L2
st2
Z
Rp
ÀÜœÄt(a|s)
ÀÜS (a, s, T)

2
2 da + 36L2
spt
(121)
‚â§36pt(1 + t)L2
s + 144t2L2
s
Z
Rp
ÀÜœÄt(a|s)
ÀÜS(a, s, T) ‚àí ‚àá log ÀúœÄt(a|s)

2
2 +
‚àá log ÀúœÄt(a|s)

2
2

da,
(122)
where the Ô¨Årst term in Eq.(120) holds since:
Z
Rp
Z
Rp œÅ0,t(ÀÜa0, a|s)œÅz(z)dadz = ÀÜœÄ0(ÀÜa0|s);
the second term in Eq.(120) holds since:
Z
Rp
Z
Rp
Z
Rp œÅ0,t(ÀÜa0, a|s)œÅz(z)dÀÜa0dz = ÀÜœÄt(a|s);
49
the third term in Eq.(120) holds since: z ‚àº N(0, I), then ‚à•z‚à•2
2 ‚àº œá2(p)-distribution with p
degrees of freedom, then
Z
Rp
Z
Rp
Z
Rp œÅ0,t(ÀÜa0, a|s)œÅz(z)‚à•z‚à•2
2dadÀÜa0dz = p;
(123)
Eq.(121) holds with the same analysis of (123), since ÀÜa0 ‚àº N(0, I), then ‚à•ÀÜa0‚à•2
2 ‚àº œá2(p), which
implies
Z
Rp ÀÜœÄ0(ÀÜa0|s) ‚à•ÀÜa0‚à•2
2 dÀÜa0 = p;
Eq.(122) holds since we use the fact: ‚à•‚ü®Œ± + Œ≤‚ü©‚à•2
2 ‚â§ 2‚à•Œ±‚à•2
2 + 2‚à•Œ≤‚à•2
2.
‚ñ°
G
Details and Discussions for multimodal Experiments
In this section, we present all the implementation details and the plots of both 2D and 3D
Visualization. Then we provide additional discussions for empirical results of the task of the
multimodal environment in Section 3.2.
G.1
Multimodal Environment
In this section, we clarify the task and reward of the multimodal environment.
G.1.1
Task
We design a simple ‚Äúmulti-goal‚Äù environment according to the Didactic Example [Haarnoja
et al., 2017], in which the agent is a 2D point mass on the 7 √ó 7 plane, and the agent tries to
reach one of four points (0, 5), (0, ‚àí5), (5, 0) and (‚àí5, 0) symmetrically placed goals.
G.1.2
Reward
The reward is deÔ¨Åned according to the following three parts:
R = r1 + r2 + r3,
where
‚Ä¢ r1 ‚àù ‚àí‚à•a‚à•2
2 if agent plays the action a;
‚Ä¢ r2 = ‚àí min{‚à•(x, y) ‚àí target‚à•2
2}, target denotes one of the target points (0, 5), (0, ‚àí5),
(5, 0), and (‚àí5, 0);
‚Ä¢ if the agent reaches one of the targets among {(0, 5), (0, ‚àí5), (5, 0), (‚àí5, 0)}, then it
receives a reward r3 = 10.
Since the goal positions are symmetrically distributed at the four points (0, 5), (0, ‚àí5), (5, 0)
and (‚àí5, 0), a reasonable policy should be able to take actions uniformly to those four goal
positions with the same probability, which characters the capacity of exploration of a policy
to understand the environment. Furthermore, we know that the shape of the reward curve
should be symmetrical with four equal peaks.
50
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
0.08
0.06
0.04
0.02
0.00
0.02
0.04
(a) 1E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
60
50
40
30
20
10
0
(b) 2E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
120
100
80
60
40
20
(c) 3E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
250
200
150
100
50
0
(d) 4E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
300
200
100
0
(e) 5E3 iterations
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
400
300
200
100
0
(f) 6E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
400
300
200
100
0
(g) 7E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
400
300
200
100
0
(h) 8E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
400
300
200
100
0
(i) 9E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
400
300
200
100
0
(j) 10E3 iterations
Figure 11: Policy representation comparison of diÔ¨Äusion policy with diÔ¨Äerent iterations.
G.2
Plots Details of Visualization
This section presents all the details of the 2D and 3D visualization for the multi-goal task. At
the end of this section, we present the shape of the reward curve.
G.2.1
2D Visualization
For the 2D visualization, the red arrowheads denote actions learned by the corresponding RL
algorithms, where each action starts at one of the totals of 7 √ó 7 = 49 points (corresponding to
all the states) with horizontal and vertical coordinates ranges among {‚àí3, ‚àí2, ‚àí1, 0, 1, 2, 3} √ó
{‚àí3, ‚àí2, ‚àí1, 0, 1, 2, 3}. The length of the red arrowheads denotes the length of the action
vector, and the direction of the red arrowheads denotes the direction of actions. This is to say;
for each Ô¨Ågure, we plot all the actions starting from the same coordinate points.
51
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
0.8
0.6
0.4
0.2
0.0
(a) 1E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
140
120
100
80
60
(b) 2E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
275
250
225
200
175
150
125
100
(c) 3E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
350
300
250
200
150
100
(d) 4E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
400
300
200
100
0
(e) 5E3 iterations
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
500
400
300
200
100
0
(f) 6E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
400
300
200
100
0
(g) 7E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
400
300
200
100
0
(h) 8E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
400
300
200
100
0
(i) 9E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
400
300
200
100
0
(j) 10E3 iterations
Figure 12: Policy representation comparison of SAC with diÔ¨Äerent iterations.
G.2.2
3D Visualization
For the 3D visualization, we provide a decomposition of the the region [‚àí7, 7] √ó [‚àí7, 7] into
100 √ó 100 = 10000 points, each point (x, y) ‚àà [‚àí7, 7] √ó [‚àí7, 7] denotes a state. For each state
(x, y), a corresponding action is learned by its corresponding RL algorithms, denoted as a.
Then according to the critic neural network, we obtain the state-action value function Q value
of the corresponding point ((x, y), a). The 3D visualization shows the state-action Q (for PPO,
is value function V ) with respect to the states.
G.2.3
Shape of Reward Curve
Since the shape of the reward curve is symmetrical with four equal peaks, the 2D visualization
presents the distribution of actions toward those four equal peaks. A good algorithm should
take actions with a uniform distribution toward those four points (0, 5), (0, ‚àí5), (5, 0), and
(‚àí5, 0) on the 2D visualization. The 3D visualization presents the learned shape according to
the algorithm during the learning process. A good algorithm should Ô¨Åt the symmetrical reward
52
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
1.0
0.8
0.6
0.4
0.2
(a) 1E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
150
100
50
0
(b) 2E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
250
200
150
100
50
0
(c) 3E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
300
200
100
0
(d) 4E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
300
200
100
0
(e) 5E3 iterations
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
400
300
200
100
0
(f) 6E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
400
300
200
100
0
(g) 7E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
300
200
100
0
(h) 8E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
300
200
100
0
(i) 9E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
300
200
100
0
(j) 10E3 iterations
Figure 13: Policy representation comparison of TD3 with diÔ¨Äerent iterations.
shape with four equal peaks. A multimodal policy distribution is eÔ¨Écient for exploration,
which may lead an agent to learn a good policy and perform better. Thus, both 2D and
3D visualizations character the algorithm‚Äôs capacity to represent the multimodal policy
distribution.
G.3
Results Report
We have shown all the results in Figure 11 (for diÔ¨Äusion policy), 12 (for SAC), 13 (for TD3)
and 14 (for PPO), where we train the policy with a total 10000 iterations, and show the 2D
and 3D visualization every 1000 iteration.
Figure 11 shows that the diÔ¨Äusion policy accurately captures a multimodal distribution
landscape of reward, while from Figure 12, 13, and 14, we know that both SAC, TD3, and
PPO are not well suited to capture such multimodality. Comparing Figure 11 to Figure 12
and 13, we know that although SAC and TD3 share a similar best reward performance, where
both diÔ¨Äusion policy and SAC and TD3 keep the highest reward around ‚àí20, diÔ¨Äusion policy
matches the real environment and performance shape.
53
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
0.0245
0.0240
0.0235
0.0230
0.0225
(a) 1E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
0.90
0.88
0.86
0.84
0.82
(b) 2E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
1.42
1.40
1.38
1.36
1.34
1.32
1.30
(c) 3E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
0.62
0.61
0.60
0.59
0.58
0.57
(d) 4E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
1.30
1.28
1.26
1.24
1.22
1.20
(e) 5E3 iterations
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
6
4
2
0
2
4
6
x
6
4
2
0
2
4
6
y
0.1
0.1
0.1
0.1
0.2
0.2
0.2
0.2
0.2
0.3
0.3
0.3
0.3
0.3
0.5
0.5
0.5
0.5
0.5
0.6
0.6
0.6
0.6
0.7
0.7
0.7
0.7
0.8
0.8
0.8
0.8
0.9
0.9
0.9
0.9
1.0
1.0
1.0
1.0
1.2
1.2
1.2
1.2
1.3
1.3
1.3
1.3
1.4
1.4
1.4
1.4
1.5
1.5
1.5
1.5
1.6
1.6
1.6
1.6
1.7
1.7
1.7
1.7
1.9
1.9
1.9
1.9
2.0
2.0
2.0
2.0
2.1
2.1
2.1
2.1
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
0.57
0.56
0.55
0.54
0.53
(f) 6E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
0.61
0.60
0.59
0.58
0.57
0.56
(g) 7E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
0.405
0.400
0.395
0.390
0.385
0.380
0.375
(h) 8E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
0.51
0.50
0.49
0.48
0.47
(i) 9E3 iterations
x
6
4
2 0
2
4
6
y
6
4
2
0
2
4
6
Value Function
0.52
0.51
0.50
0.49
0.48
(j) 10E3 iterations
Figure 14: Policy representation comparison of PPO with diÔ¨Äerent iterations.
From Figure 14, we also Ô¨Ånd PPO always runs around at the initial value, and it does not
improve the reward performance, which implies PPO fails to Ô¨Åt multimodality. It does not
learn any information about multimodality.
From the distributions of action directions and lengths, we also know the diÔ¨Äusion policy
keeps a more gradual and steady action size than the SAC, TD3, and PPO to learn the
multimodal reward performance. Thus, the diÔ¨Äusion model is a powerful policy representation
that leads to a more suÔ¨Écient exploration and better performance, which is our motivation to
consider representing policy via the diÔ¨Äusion model.
54
H
Additional Experiments
In this section, we provide additional details about the experiments, including Hyper-parameters
of all the algorithms; additional tricks for implementation of DIPO; details and additional
reports for state-visiting; and ablation study on MLP and VAE.
The Python code for our implementation of DIPO is provided along with this submission
in the supplementary material. SAC: https://github.com/toshikwa/soft-actor-critic.
pytorch PPO: https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail TD3: https:
//github.com/sfujim/TD3, which were oÔ¨Écial code library.
H.1
Hyper-parameters for MuJoCo
Common Hyper-parameters:
Hyperparameter
DIPO
SAC
TD3
PPO
No. of hidden layers
2
2
2
2
No. of hidden nodes
256
256
256
256
Activation
mish
relu
relu
tanh
Batch size
256
256
256
256
Discount for reward Œ≥
0.99
0.99
0.99
0.99
Target smoothing coeÔ¨Écient œÑ
0.005
0.005
0.005
0.005
Learning rate for actor
3 √ó 10‚àí4
3 √ó 10‚àí4
3 √ó 10‚àí4
7 √ó 10‚àí4
Learning rate for critic
3 √ó 10‚àí4
3 √ó 10‚àí4
3 √ó 10‚àí4
7 √ó 10‚àí4
Actor Critic grad norm
2
N/A
N/A
0.5
Memeroy size
1 √ó 106
1 √ó 106
1 √ó 106
1 √ó 106
Entropy coeÔ¨Écient
N/A
0.2
N/A
0.01
Value loss coeÔ¨Écient
N/A
N/A
N/A
0.5
Exploration noise
N/A
N/A
N(0, 0.1)
N/A
Policy noise
N/A
N/A
N(0, 0.2)
N/A
Noise clip
N/A
N/A
0.5
N/A
Use gae
N/A
N/A
N/A
True
Table 2: Hyper-parameters for algorithms.
Additional Hyper-parameters of DIPO:
Hyperparameter
Hopper-v3
Walker2d-v3
Ant-v3
HalfCheetah-v3
Humanoid-v3
Learning rate for action
0.03
0.03
0.03
0.03
0.03
Actor Critic grad norm
1
2
0.8
2
2
Action grad norm ratio
0.3
0.08
0.1
0.08
0.1
Action gradient steps
20
20
20
40
20
DiÔ¨Äusion inference timesteps
100
100
100
100
100
DiÔ¨Äusion beta schedule
cosine
cosine
cosine
cosine
cosine
Update actor target every
1
1
1
2
1
Table 3: Hyper-parameters of DIPO.
55
DIPO
SAC
TD3
PPO
Figure 15: State-visiting distribution of Humanoid-v3, where states get dimension reduction
by t-SNE. The points with diÔ¨Äerent colors represent the states visited by the policy with the
style. The distance between points represents the diÔ¨Äerence between states.
H.2
Additional Tricks for Implementation of DIPO
We have provided the additional details for the Algorithm 3.
H.2.1
Double Q-learning for Estimating Q-Value
We consider the double Q-learning [Hasselt, 2010] to update the Q value. We consider the two
critic networks Qœà1, Qœà2, two target networks Qœà‚Ä≤
1, Qœà‚Ä≤
2. Let Bellman residual be as follows,
LQ(œà) = E(st,at,st+1,at+1)
"

r(st+1|st, at) + Œ≥ min
i=1,2 Qœà‚Ä≤
i(st+1, at+1)

‚àí Qœà(st, at)

2#
.
Then, we update œài as follows, for i ‚àà {1, 2}
œài ‚Üê œài ‚àí Œ∑‚àáLQ(œài).
Furthermore, we consider the following soft update rule for œà
‚Ä≤
i as follows,
œà
‚Ä≤
i ‚Üê œÅœà
‚Ä≤
i + (1 ‚àí œÅ)œài.
Finally, for the action gradient step, we consider the following update rule: replacing each
action at ‚àà Denv as follows
at ‚Üê at + Œ∑a‚àáa

min
i=1,2 {Qœài(st, a)}
 
a=at
.
H.2.2
Critic and DiÔ¨Äusion Model
We use a four-layer feedforward neural network of 256 hidden nodes, with activation function
Mish [Misra, 2019] between each layer, to design the two critic networks Qœà1, Qœà2 two target
networks Qœà‚Ä≤
1, Qœà‚Ä≤
2, and the noise term œµœÜ. We consider gradient normalization for critic and
œµœÜ to stabilize the training process.
For each reverse time k ‚àà [K], we consider the sinusoidal positional encoding [Vaswani
et al., 2017] to encode each k ‚àà [K] into a 32-dimensional vector.
56
DIPO
SAC
TD3
PPO
Figure 16: State-visiting distribution of Walker2d-v3, where states get dimension reduction
by t-SNE. The points with diÔ¨Äerent colors represent the states visited by the policy with the
style. The distance between points represents the diÔ¨Äerence between states.
Ant-v3
DIPO
SAC
TD3
PPO
Figure 17: State-visiting visualization by each algorithm on the Ant-v3 task, where states get
dimension reduction by t-SNE. The points with diÔ¨Äerent colors represent the states visited
by the policy with the style. The distance between points represents the diÔ¨Äerence between
states.
H.3
Details and Additional Reports for State-Visiting
In this section, we provide more details for Section 7.1, including the implementation details
(see Appendix H.3.1), more comparisons and more insights for the empirical results. We
provide the main discussions cover the following three observations:
‚Ä¢ poor exploration results in poor initial reward performance;
‚Ä¢ good Ô¨Ånal reward performance along with dense state-visiting;
‚Ä¢ a counterexample: PPO violates the above two observations.
H.3.1
Implementation Details for 2D State-Visiting
We save the parameters for each algorithm during the training for each 1E5 iteration. Then
we run the model with an episode with ten random seeds to compare fairly; those ten random
seeds are the same among diÔ¨Äerent algorithms. Thus, we collect a state set with ten episodes
for each algorithm. Finally, we convert high-dimensional state data into two-dimensional state
data by t-SNE [Van der Maaten and Hinton, 2008], and we show the visualization according to
the open implementation https://scikit-learn.org/stable/auto_examples/manifold/
plot_t_sne_perplexity.html where we set the parameters as follows,
perpexity = 50, early exaggeration = 12, random state = 33.
We have shown all the results in Figure 15 (for Humanoid); Figure 16 (for Walker2d);
Figure 17 (for Ant); Figure 18 (for HalfCheetah); and Figure 19 (for Hopper), where we polt
the result after each E5 iterations.
57
DIPO
SAC
TD3
PPO
Figure 18: The state-visiting visualization by each algorithm on the HalfCheetah-v3 task,
where states get dimension reduction by t-SNE. The points with diÔ¨Äerent colors represent
the states visited by the policy with the style. The distance between points represents the
diÔ¨Äerence between states.
DIPO
SAC
TD3
PPO
Figure 19: State-visiting distribution of Hopper-v3, where states get dimension reduction by
t-SNE. The points with diÔ¨Äerent colors represent the states visited by the policy with the
style. The distance between points represents the diÔ¨Äerence between states.
H.3.2
Observation 1: Poor Exploration Result in Poor Initial Reward Perfor-
mance
From Figure 6, we know TD3 and PPO reach a worse initial reward performance than DIPO
and SAC for the Hopper task, which coincides with the results appear in Figure 19. At the
initial interaction, TD3 and PPO explore within a very sparse state-visiting region, which
decays the reward performance. Such an empirical result also appears in the Walker2d task
for PPO (see Figure 16), Humanoid task for TD3 and SAC (see Figure 15), where a spare
state-visiting is always accompanied by a worse initial reward performance. Those empirical
results once again conÔ¨Årm a common sense: poor exploration results in poor initial reward
performance.
Conversely, from Figure 6, we know DIPO and SAC obtain a better initial reward perfor-
mance for the Hopper task, and Figure 19 shows that DIPO and SAC explore a wider range
of state-visiting that covers than TD3 and PPO. That implies that a wide state visit leads to
better initial reward performance. Such an empirical result also appears in the Walker2d task
for DIPO, SAC, and TD3 (see Figure 16), Humanoid task for DIPO (see Figure 15), where
the agent runs with a wider range state-visiting, which is helpful to the agent obtains a better
initial reward performance.
In summary, poor exploration could make the agent make a poor decision and cause a
poor initial reward performance. While if the agent explores a wider range of regions to visit
more states, which is helpful for the agent to understand the environment and could lead to
better initial reward performance.
58
0.0
0.2
0.4
0.6
0.8
1.0
0
1000
2000
3000
4000
5000
6000
PPO
SAC
TD3
MLP
VAE
DIPO
(a) Ant-v3
0.0
0.2
0.4
0.6
0.8
1.0
0
2000
4000
6000
8000
10000
12000
PPO
SAC
TD3
MLP
VAE
DIPO
(b) HalfCheetah-v3
0.0
0.2
0.4
0.6
0.8
1.0
0
500
1000
1500
2000
2500
3000
3500
PPO
SAC
TD3
MLP
VAE
DIPO
(c) Hopper-v3
0.0
0.2
0.4
0.6
0.8
1.0
0
1000
2000
3000
4000
5000
PPO
SAC
TD3
MLP
VAE
DIPO
(d) Humanoid-v3
0.0
0.2
0.4
0.6
0.8
1.0
0
1000
2000
3000
4000
5000
PPO
SAC
TD3
MLP
VAE
DIPO
(e) Walker2d-v3
Figure 20: Average performances on MuJoCo Gym environments with ¬± std shaded, where
the horizontal axis of coordinate denotes the iterations (√ó106), the plots smoothed with a
window of 10.
H.3.3
Observation 2: Good Final Reward Performance along with Dense State-
Visiting
From Figure 19, we know DIPO, SAC, and TD3 achieve a more dense state-visiting for the
Hopper task at the Ô¨Ånal iterations. Such an empirical result also appears in the Walker2d
and Humanoid tasks for DIPO, SAC, and TD3 (see Figure 15 and 16). This is a reasonable
result since after suÔ¨Écient training, the agent identiÔ¨Åes and avoids the ‚Äùbad‚Äù states, and plays
actions to transfer to ‚Äùgood‚Äù states. Besides, this observation is also consistent with the result
that appears in Figure 6, the better algorithm (e.g., the proposed DIPO) usually visits a more
narrow and dense state region at the Ô¨Ånal iterations. On the contrary, PPO shows an aimless
exploration among the Ant-v3 task (see Figure 7) and HalfCheetah (see Figure 8), which
provides a partial explanation for why PPO is not so good in the Ant-v3 and HalfCheetah
task. This is a natural result for RL since a better algorithm should keep a better exploration
at the beginning and a more suÔ¨Écient exploitation at the Ô¨Ånal iterations.
H.3.4
Observation 3: PPO Violates above Two Observations
From all of those 5 tasks (see Figure 15 to 19), we also Ô¨Ånd PPO violates the common sense of
RL, where PPO usual with a narrow state-visiting at the beginning and wide state-visiting at
the Ô¨Ånal iteration. For example, from Figure 6 and 19, we know PPO achieves an asymptotic
reward performance as DIPO for the Hopper-v3, while the state-visiting distribution of PPO is
fundamentally diÔ¨Äerent from DIPO. DIPO shows a wide state-visiting region gradually turns
into a narrow state-visiting region, while PPO shows a narrow state-visiting region gradually
turns into a wide state-visiting region. We show the fair visualization with t-SNE by the same
setting for all of those 5 tasks, the abnormal empirical results show that PPO may Ô¨Ånd some
59
new views diÔ¨Äerent from DIPO/TD3/SAC to understand the environment.
H.4
Ablation Study on MLP and VAE
A fundamental question is why must we consider the diÔ¨Äusion model to learn a policy
distribution. In fact, Both VAE and MLP are widely used to learn distribution in machine
learning, can we replace the diÔ¨Äusion model with VAE and MLP in DIPO? In this section, we
further analyze the empirical reward performance among DIPO, MLP, and VAE.
We show the answer in Figure 9 and Figure 20, where the VAE (or MLP) is the result we
replace the diÔ¨Äusion policy of DIPO (see Figure 3) with VAE (or MLP), i.e., we consider VAE
(or MLP)+action gradient (15) for the tasks.
Results of Figure 20 show that the diÔ¨Äusion model achieves the best reward performance
among all 5 tasks. This implies the diÔ¨Äusion model is an expressive and Ô¨Çexible family to
model a distribution, which is also consistent with the Ô¨Åeld of the generative model.
Additionally, from the results of Figure 20 we know MLP with action gradient also performs
well among all 5 tasks, which implies the action gradient is a very promising way to improve
reward performance. For example, Humanoid-v3 is the most challenging task among Mujoco
tasks, MLP achieves a Ô¨Ånal reward performance near the PPO, SAC, DIPO, and TD3. We
all know that these algorithms (PPO, SAC, DIPO, and TD3) are meticulously constructed
mathematically, while MLP with action gradient is a simple model, but it achieves so good
reward performance, which is a direction worth further in-depth research to search simple but
eÔ¨Écient RL algorithm.
60

