ONE MODEL TO RULE THEM ALL ? TOWARDS END-TO-END JOINT SPEAKER
DIARIZATION AND SPEECH RECOGNITION
Samuele Cornell1,2, Jee-weon Jung2, Shinji Watanabe2, Stefano Squartini1
1Universit`a Politecnica delle Marche, Italy 2Carnegie Mellon University, USA
ABSTRACT
This paper presents a novel framework for joint speaker diariza-
tion (SD) and automatic speech recognition (ASR), named SLIDAR
(sliding-window diarization-augmented recognition). SLIDAR can
process arbitrary length inputs and can handle any number of speak-
ers, effectively solving “who spoke what, when” concurrently. SLI-
DAR leverages a sliding window approach and consists of an end-to-
end diarization-augmented speech transcription (E2E DAST) model
which provides, locally, for each window: transcripts, diarization
and speaker embeddings. The E2E DAST model is based on an
encoder-decoder architecture and leverages recent techniques such
as serialized output training and “Whisper-style” prompting. The
local outputs are then combined to get the final SD+ASR result by
clustering the speaker embeddings to get global speaker identities.
Experiments performed on monaural recordings from the AMI cor-
pus confirm the effectiveness of the method in both close-talk and
far-field speech scenarios.
Index Terms— conversational speech recognition, conversa-
tion transcription, speaker diarization, multi-talker automatic speech
recognition, end-to-end
1. INTRODUCTION
Speaker diarization (SD) and automatic speech recognition (ASR)
are both essential tasks in many applications, such as meeting
transcription, speech analytics, automatic captioning, and speech-
enabled virtual assistants to name a few [1–5]. These tasks, be-
ing mutually complementary, have often been performed together
(SD+ASR). This integration stems from the fact that, in actual
use-cases, it is often necessary to obtain a “who spoke what,
when” diarization-augmented speech transcription (DAST) [1,6–8]:
i.e. a “rich” speech transcription with segmentation and speaker-
attribution at the utterance or word-level.
Such DAST output for long-form audio recordings is generally
achieved by using a complex SD+ASR pipeline which comprises of,
generally: SD, speech separation/enhancement (SSE) and ASR com-
ponents [7,9–13]. However, such an approach has an inherent draw-
back: it is difficult to optimize jointly all the modules. This leads
to sub-optimal overall performance and the need for lots of hyper-
parameter tuning including, but not limited to, SD output smooth-
ing [7,9,13]. This is also especially true when dealing with monaural
recordings (which are also more commonly available), as it makes
separation and diarization way more challenging.
Recent works have aimed to reduce the components within the
SD+ASR pipeline, for example, by trying to combine closely re-
lated tasks such as SD and speech separation [14]. Indeed, some
S. Cornell was partially supported by the “Miracle” project POR
MARCHE FESR 2014-2020.
works [15–17] try to replace SD altogether with SSE via continu-
ous source separation (CSS) [15]. However, such approaches are
not suitable for long-form audio. CSS will lose the speaker tracking
when two or more speakers are not active within a CSS window over-
lap and additional SD is thus necessary [11]. As such e.g. [16, 18]
were only evaluated on utterance groups separately or [17] focused
only on 2-speakers scenarios and relatively short recordings. The
same problem also applies to all ASR-based systems that rely only
on permutation invariant training (PIT) [19], SOT [20] or other local-
only strategies to perform multi-speaker ASR [20–24].
Target-speaker approaches do not suffer from such a problem
but require a prior SD output. As such, target-speaker SSE meth-
ods [13, 25, 26], while being effective, do not really simplify the
pipeline. Instead, ASR-based target-speaker methods [27–31] can
reduce, in principle, the components only to SD+ASR with the ASR
doing implicitly speech separation.
For example, Transcribe-to-
diarize (T2D) [32] simplifies considerably the pipeline as it needs
only voice activity detection (VAD), clustering-based SD and an
E2E speaker-attributed ASR model [27]. This latter utilizes pre-
estimated speaker embeddings obtained from the clustering-based
SD step to generate a DAST with consistent speaker-ids across all
the recordings. Contrary to aforementioned works on target-speaker
ASR [27–29, 31] here the ASR model does not only do speaker-
attribution but also outputs a refined diarization output.
In this work, we propose a relatively simple yet effective frame-
work that solves the problem of “who spoke what, when” simultane-
ously. We call this approach SLIDAR as it jointly performs speaker-
attribution, speech segmentation, and recognition in a “quasi” end-
to-end (E2E) manner, as it uses a sliding window and clustering
mechanism to keep the computational cost linear with the length of
the recording. SLIDAR first adopts an E2E DAST attention-based
encoder/decoder (AED) [3] model to derive local transcriptions plus
SD as well as speaker embeddings. Following, all results from each
window are combined and final global speaker labels are assigned
by clustering all the local speaker embeddings. Unlike T2D, which
is the most closely related work to ours, SLIDAR does not require
any prior VAD or clustering-based diarization output. This frame-
work relies on multiple novel contributions: (i) E2E joint SD+ASR
modeling with serialized output training (SOT) [20], enabled by the
addition of several special tokens; (ii) a sliding-window based in-
ference mechanism suitable for multi-talker recordings; (iii) joint
speaker embeddings estimation via the SOT SD+ASR output; (iv)
new “Whisper-style” multi-task learning tasks for multi-talker long-
form recordings.
Experiments and ablation studies (Sec. 4) on real-world monau-
ral long-form multi-talker recordings i.e.
AMI headset-mix and
far-field microphone signals indicate promising potential for this
method, with comparable or better performance with respect to
state-of-the-art methods such as T2D.
arXiv:2310.01688v1  [eess.AS]  2 Oct 2023
M2T 
Model
time
avg
<|spk0|><|time0|> how you doing <|time240|>
<|spk1|><time100|> hi <|time130|> 
<|spk1|><|time280|> doing <|trunc|> 
E2E  DAST 
model
window 2
window 1
local DAST
clustering and aggregation 
spk-id output (D x T)
time
avg
local 
diarization
spk-id vectors (D)
<|spk0|><|time0|> doing good you <|time140|>
<|spk1|><time160|> fine thanks <|time210|> 
<|spk0|><|time220|> yeah been so long <|time300|> 
Fig. 1. Proposed SLIDAR framework on a two speakers toy example.
2. SLIDAR FRAMEWORK
Figure 1 illustrates our proposed framework, SLIDAR. SLIDAR em-
ploys a sliding window mechanism on an input long-form record-
ing, where each window is processed independently by a local E2E
DAST model. Such a mechanism ensures that our approach can
be applied to an arbitrary length input while keeping the memory
requirements constant, enabling the utilization of large pre-trained
models (these usually employ self-attention, e.g.
WavLM [33]).
However, this also requires to have a technique that keeps track of
the speaker identities globally. In this framework, the E2E DAST
model is tasked with estimating, for each window, a “local” DAST
output as well as speaker embeddings of speakers. The “global”
transcription is derived by aggregating local transcriptions; global
speaker labels are computed by clustering all speaker embeddings.
2.1. Local E2E DAST Model
The local E2E DAST model is based on AED and adopts a SOT
strategy. It predicts, for each utterance in the current input, special
tokens for onset and offset timestamps, the words and a speaker tag
token for the speaker to which the utterance belongs: e.g. <|spk0|>
<|time10|> hello world <|time200|>, where <|time10|> and
<|time200|> are respectively the onset and offset timestamp tokens
of the utterance.
Except for these changes, described in detail below, the model
is trained following the standard SOT [20] framework using teacher
forcing at the attention-based decoder output yi ∈ [y1, .., yM]:
LSOT =
M
X
i=1
CE(yi, lΨ
i ).
(1)
Where M is the target length. Ψ defines the particular permuta-
tion of the li ∈ [l1, .., lM] target sequence, i.e. the target tokens se-
quence permutation for which each utterance is ordered by its start-
ing time, in a first-in first-out (FIFO) manner (tokens from utterances
in Fig. 1 plus an end-of-sequence <|eos|> token). CE denotes the
cross-entropy loss function between output distribution and target
labels, again as in [20].
Timestamp Tokens. We use a fixed set of special tokens that rep-
resents timestamps, identical to Whisper [34]. The amount of these
timestamp tokens depends on their resolution (e.g. 100 ms) as well
as the maximum window length we want the model to support in
inference. A <|nospeech|> special token is used when the current
window does not contain any speech.
Speaker Tag Tokens. Our approach can be considered an exten-
sion of [20, 24, 32].
Instead of adopting a speaker change sym-
bol <|sc|>, we use N different speaker tag tokens: <|spk0|>,
<|spk1|>,. . . ,<|spkN|> with N chosen a-priori as the maximum
number of speakers we expect in the local window. Unlike earlier
studies that rely on a speaker inventory [24] or clustering-based di-
arization output [32], here, even at the local level, we have an ar-
bitrary mapping problem between reference speaker names and the
estimated speaker tags. To overcome this ambiguity, we simply en-
force for the speaker tags a FIFO naming convention: within each
window, the first speaker to appear is labeled as <|spk0|>, the sec-
ond as <|spk1|> and so on (see Fig. 1). Note that such speaker tag
consistency is only enforced within the current window.
Truncation Token. Since the model is applied (both in inference
and training) on finite audio chunks from e.g. a meeting scenario, it
can happen that some utterances are truncated from such windowing
approach. To handle such cases, we introduce an additional special
truncation token (<|trunc|>) (see Fig. 1) which is used in place of
timestamps when an utterance is truncated: e.g. if truncated at the
beginning it replaces the onset timestamp, if truncated at its end,
the offset timestamp. Such <|trunc|> token is key for fully local
E2E DAST as it allows the model also to take over the VAD role.
This allows training the model on randomly sampled windows but
also requires world-level segmentation in order to split utterances
that are truncated. Such segmentation can be obtained in a scalable
way only via forced alignment (FA). Yet FA can be unreliable in the
presence of overlapped speech if per-speaker close-talk microphones
are not available (e.g. web videos etc.). Thus, here, we also explore
a different strategy, where, as in [35], we estimate word boundaries
from utterance boundaries by counting the characters in each word
and assuming these latter isochronal. Our experiments (see Tab. 2)
show that this form of “weak supervision” can work reasonably well.
2.2. Windowing and Decoding
As mentioned, the inference phase employs a sliding window mech-
anism on the whole recording. As such, we have to take into account
the fact that the window could truncate the final utterance of multi-
ple speakers at the end-point of the window. I.e., for some speak-
ers, in their last utterance, instead of an offset token, a <|trunc|>
is encountered. When this occurs, we enforce the start of the next
window at the rightmost silence segment (no speaker active) from
the leftmost utterance that was truncated in the previous decoding
window (e.g. for two speakers, see Fig. 1). This assures that no trun-
cation at utterances onset is encountered during inference. Instead,
when no utterances are truncated at the offset, we simply start the
next window at the end of the current one (no overlap).
Within each window, during decoding and at each autoregres-
sive iteration, we exclude inadmissible hypotheses from the beam
search stack. This is crucial for achieving satisfactory performance.
For example, we enforce rules such as: (i) timestamp tokens should
be monotonically increasing for each speaker (ii) after a timestamp
token, the next one can be either a speaker token or a subword unit
token; and so on, so that all hypotheses are well formed.
2.3. Speaker Vectors Estimation and Clustering
Together with the local E2E DAST output, the model is also tasked to
estimate speaker embeddings for each window, to be used to assign
global speaker labels and merge all predictions. In detail, (see Fig 1),
we require the local E2E DAST model encoder also to output frame-
level speaker discriminative features via another output head E(·):
hi = E(xj)
(2)
Where xj ∈ RDf ×K is the input feature vectors sequence to the
encoder with length K and size Df each: xj = [x1, . . . , xK].
While hi ∈ RDh×M , hi = [h1, . . . , hM] is the corresponding
output sequence with length M and size Df each. We use the local
DAST diarization information estimated by the decoder via SOT to
compute the local speaker embedding vectors es ∈ RDe×N , es =
[e1, . . . , eN] at each window using time-averaging:
ˆes =
1
n(Θs)
X
i∈Θs
hi, s ∈ 1, . . . , N local
(3)
where Θs ⊆ {x ∈ N | 1 <= x <= M} is the set of frames
for which only the s-th local speaker is active (thus without consid-
ering overlapped speech parts), n(Θs) is the cardinality of the set
and N local ≤ N is the number of local speakers in the current win-
dow (see Sec. 2.1). Note that this is equivalent to applying a binary
mask and it is a key difference from [36] where instead a sigmoid
mask is employed. We train such speaker embedding E(·) encoder
head jointly with the local DAST objective LSOT by using the Lspeaker
speaker embedding loss from EEND-VC [36,37]:
Lspeaker
=
1
N local
Nlocal
X
s=1
lspeaker (σs, ˆes) .
(4)
Where σs is the s-th target speaker identity index over all speakers in
the training set and lspeaker is defined exactly as in [36]; an embedding
dictionary for all training speakers is learned jointly during training.
Note that, in training, we use oracle diarization for time-averaging
in Eq. 3 instead of the estimated DAST output, as this latter would
require decoding. Also, compared to [36], here we have no permu-
tation ambiguity since the local speaker order is FIFO-based.
Since inference is performed on sliding windows that can over-
lap, we use the overlapped part information, again as in EEND-
VC, to impose cannot-link constraints in the clustering phase. We
chose in this work constrained agglomerative hierarchical cluster-
ing (AHC) [37] and impose constraints only when the overlap is
greater than 5 s.
2.4. Multi-Task Training
Since the proposed framework is fundamentally built upon AED
ASR (it uses SOT), we can use the same ASR multi-task learn-
ing framework introduced in Whisper [34]. In particular, here we
experiment with two additional tasks/modes on top of the local
DAST prediction: (i) <|OD|>, where the model is prompted with
oracle diarization information for the current window and (ii) a
<|vanillaASR|> task, to allow the model to be trained on non-
conversational ASR corpora (which are arguably larger) and also
cover single-speaker pre-segmented inputs. On top of these two,
we also experiment with Whisper previous decoding window output
conditioning (<|prev|>) for the current prediction.
3. EXPERIMENTAL SETUP
3.1. Dataset: AMI Corpus
The AMI Corpus [38] consists of over 100 h real-world meetings
between 3 to 5 participants recorded by a variety of devices, includ-
ing distant microphone arrays and close-talk lapel and headset mi-
crophones. It is partitioned [39] into train (80.2 h), dev (9.7 h),
and test (9.1 h) sets. To compare with [32], we experiment with
two conditions: single distant microphone (SDM) and headset-mix
(IHM-MIX) recordings. Importantly, AMI offers reliable word-level
segmentation annotation obtained via FA on the worn headset mics,
allowing us to compare our SLIDAR strategy with and without the
utterance truncation approximation explained in Sec. 2.1.
3.2. Model Architecture
Our framework can employ any AED ASR model with minor mod-
ifications (e.g.
including Whisper).
In this work, we adopt the
WavLM-supported Transformer AED model introduced in IRIS [40]
but without any connectionist temporal classification (CTC) [41]
loss on the encoder. Instead, the encoder here has a separate lin-
ear layer to output 256 dimensional speaker discriminative features
(hi, see Sec. 2.3). Compared to [40] we use 2 encoder and 6 de-
coder layers. Each an internal representation of size 512, 4 attention
heads, 0.2 dropout probability and 1024 as the hidden dimension of
the feed-forward layer. We use SentencePiece [42] tokenization and
set the vocabulary size to 500 non-special tokens. The timestamps
special tokens have a time resolution of 100 ms and a max duration
of 20 s. The number of speaker tag tokens is 5.
3.3. Training and Inference Details
We train the model by using the full AMI training corpus (all mi-
crophones, including all arrays), CHiME-6 [5] (all far-field micro-
phones only) and 5k h of simulated 3 to 5 speakers meetings ob-
tained using LibriSpeech [44], SINS [45], and Pyroomacoustics [46]
by using NVIDIA NeMo synthetic data simulator [47]. For these, we
randomly sampled room size between 20−50 m2 and T60 reverber-
ation time between 0.3−0.7 s. The microphone position is randomly
sampled inside the room and then kept fixed while speaker positions
are sampled at each new utterance.
During training, we randomly sample 20 s windows from
each recording and use with, respectively 0.5 and 0.1 probability,
<|prev|> and <|OD|> multi-task prompt conditioning modes. For
truncation, we use the approximation outlined in Sec. 2.1 from AMI,
CHiME-6, and LibriSpeech manual segmentation unless stated oth-
erwise. When multi-task training with <|vanillaASR|>, we use
additional data from the full Mixer 6 Speech [48] training set as
well as pre-segmented utterances from all the previously mentioned
corpora.
We use a dynamic batch size with a total of 900 s of input audio
examples, Adam optimizer [49], a learning rate of 1e−3 and 8 GPUs.
The model is trained for 200k steps, keeping WavLM frozen. After
Table 1. DER (%) and cpWER (%) on the AMI corpus dev and eval sets. DER is computed without any collar as in [32];
For SLIDAR we also report the results obtained with oracle diarization prompting (<|OD|> mode) during inference (see Sec. 2.4).
SC: speaker confusion (%) , MS: missed detection (%) , FA: false alarm (%).
Mic
System
Configuration
dev
eval
SC / MS / FA / DER
cpWER
SC / MS / FA / DER
cpWER
VBx [39]
oracle VAD
2.88 / 13.45 / 0.00 / 16.33
-
4.43 / 14.56 / 0.00 / 18.99
-
T2D [32]
3.05 / 11.46 / 9.00 / 23.51
15.9
2.47 / 14.24 / 7.72 / 24.43
16.4
IHM-MIX
T2D [32]
oracle VAD
2.83 / 9.69 / 3.46 / 15.98
16.3
1.78 / 11.71 / 3.10 / 16.58
15.1
SLIDAR
4.02 / 7.39 / 13.81 / 25.22
14.2
3.46 / 5.74 / 17.90 / 27.10
15.6
SLIDAR
oracle diarization
-
10.8
-
11.5
VBx [39]‡
-
-
4.83 / 18.15 / 3.24 / 26.22
-
SC + OVL [43]‡
-
-
6.67 / 9.63 / 7.39 / 23.69
-
T2D
3.48 / 15.93 / 7.17 / 26.58
22.6
2.86 / 19.20 / 6.07 / 28.12
24.9
SDM
T2D
oracle VAD
3.38 / 10.62 / 3.28 / 17.27
21.5
2.69 / 12.82 / 3.04 / 18.54
22.2
SLIDAR
4.92 / 14.53 / 9.83 / 29.28
21.8
3.79 / 19.73 / 8.01 / 31.52
24.5
SLIDAR
oracle diarization
-
19.4
-
21.1
‡ obtained from github.com/desh2608/diarizer.
this, we fine-tune the AED model and WavLM together with a learn-
ing rate of 1e−6 for additional 20k steps, using gradient checkpoint-
ing to not incur in out-of-memory issues. We use adaptive SpecAug-
ment [50] on the WavLM extracted features as in [40], clip gradi-
ents l2 norm to 5, and adopt a linear learning rate schedule with 20k
warm-up steps and then decay. In inference, we use 20 s sliding win-
dow (hop size variable see Sec. 2.2) and beam-size 10 for decoding.
4. EXPERIMENTAL RESULTS
We evaluate our proposed framework in terms of diarization error
rate (DER) and concatenated minimum permutation word error rate
(cpWER) [5] to assess both SD and ASR performance. We adopt
the same evaluation protocol and AMI ground truth segmentation as
used in [32,39].
In Table 1, we compare, on AMI IHM-MIX and SDM record-
ings, SLIDAR with T2D. Besides being the current state-of-the-art
on such data it is also, among all previous works, the closest to our
own. Again, it consists of separate VAD, clustering-based diariza-
tion, and a speaker-conditioned E2E joint SD+ASR model. It also
has been pre-trained with significantly more supervised data (includ-
ing 75k [27] proprietary data). Instead, our model mostly leverages
the WavLM-large self-supervised pre-training. We include in the
comparison also two different clustering-based diarization methods:
VBx [39] and overlap-aware spectral clustering [43].
Regarding cpWER, we can see that SLIDAR, compared to
T2D, in general obtains slightly better figures. We also report, for
SLIDAR, figures obtained by using oracle diarization as a prompt
(<|OD|> mode, see Sec. 2.4) for the current window prediction. We
can see that, if oracle diarization is made available, cpWER can
be reduced quite dramatically on SLIDAR. Further work could ex-
plore a two-pass decoding approach, using the previously estimated
diarization output in place of oracle one.
In general, SLIDAR has worse performance than T2D regarding
diarization. SLIDAR tends to produce higher false alarms (FA) com-
pared to T2D as it over-estimates utterance boundaries. This may be
due to the fact that SLIDAR has been trained with manual anno-
tations from a variety of corpora on top of AMI. These may have
mismatched segmentation compared to the one used here for eval-
uation on AMI (which, again, has no DER forgiveness collar). For
example, LibriSpeech and CHiME-6 manual segmentation is gener-
ally less tight.
Table 2. DER (%) and cpWER (%) on the AMI corpus IHM-MIX
recordings for dev and eval sets. We study the impact of accurate
vs. approximate word-level segmentation for truncation (Sec. 2.1)
and of the different multi-task training strategies (Sec. 2.4).
Configuration
dev
eval
DER cpWER DER cpWER
M2T (FA)
22.8
16.5
24.8
17.9
M2T (approximate)
26.7
16.7
28.1
18.2
+ <|vanillaASR|>
25.8
15.1
27.3
15.9
+ <|prev|>
25.4
14.4
26.8
15.5
+ <|OD|> (train-only) 25.2
14.2
27.1
15.6
In Table 2, we report an ablation study performed on AMI IHM-
MIX. We quantify: (i) the impact of various multi-task/prompting
strategies; (ii) having approximate vs. using FA-obtained segmenta-
tion for the purpose of utterance truncation (see Sec. 2.1) and as ut-
terance segmentation ground truth. Training with FA-obtained seg-
mentation has little impact on cpWER but has a larger impact on the
DER, which decreases considerably. Again, this may be because it
reduces the training versus evaluation segmentation annotation mis-
match. Among the multi-task objectives, <|vanillaASR|> has the
most impact, as additional (non multi-speaker annotated, see [7])
Mixer 6 data can be leveraged. Adding <|OD|> in training appears
to degrade performance negligibly but, as a trade-off, we gain addi-
tional model flexibility.
5. CONCLUSIONS
In this work, we presented SLIDAR, a framework for “quasi” E2E
joint speaker diarization and ASR (SD+ASR) for long-form record-
ings.
This framework employs a sliding window strategy where
a fully E2E SD+ASR local model is applied on each window and
the global speaker tracking is demanded to a clustering mechanism.
Our experiments performed on AMI IHM-MIX and SDM mix show
promising results. SLIDAR compares favorably with state-of-the-art
methods such as Transcribe-to-Diarize, despite using significantly
less supervised training data and offering a much more streamlined,
almost E2E pipeline. Future research directions include modify-
ing this framework for streamable inference and trying to devise a
way for fully E2E long-form DAST. This latter requires to devise
a memory-efficient E2E learnable mechanism for long-term speaker
tracking.
6. REFERENCES
[1] J. S. Garofolo, J. G. Fiscus, A. F. Martin et al., “Nist rich transcription
2002 evaluation: A preview.,” in LREC, 2002.
[2] T. J. Park, N. Kanda, D. Dimitriadis et al., “A review of speaker di-
arization: Recent advances with deep learning,” Computer Speech &
Language, vol. 72, pp. 101317, 2022.
[3] R. Prabhavalkar, T. Hori, T. N. Sainath et al.,
“End-to-end speech
recognition: A survey,” arXiv preprint arXiv:2303.03329, 2023.
[4] J. Barker, S. Watanabe, E. Vincent and J. Trmal, “The fifth CHiME
speech separation and recognition challenge: Dataset, task and base-
lines,” in Proc. InterSpeech, 2018.
[5] S. Watanabe, M. Mandel, J. Barker et al., “CHiME-6 challenge: Tack-
ling multispeaker speech recognition for unsegmented recordings,” in
CHiME Workshop, 2020.
[6] J. G. Fiscus, J. Ajot and J. S. Garofolo, “The rich transcription 2007
meeting recognition evaluation,” in International Evaluation Workshop
on Rich Transcription. Springer, 2007, pp. 373–389.
[7] S. Cornell, M. Wiesner, S. Watanabe et al., “The chime-7 dasr chal-
lenge: Distant meeting transcription with multiple devices in diverse
scenarios,” CHiME Workshop, 2023.
[8] F. Yu, S. Zhang, Y. Fu et al.,
“M2MeT: The ICASSP 2022 multi-
channel multi-party meeting transcription challenge,” in Proc. ICASSP,
2022.
[9] I. Medennikov, M. Korenevsky, T. Prisyach et al., “The stc system for
the chime-6 challenge,” in CHiME Workshop, 2020.
[10] J. Du, Y.-H. Tu, L. Sun et al., “The ustc-nelslip systems for chime-6
challenge,” in CHiME Workshop, 2020.
[11] D. Raj, P. Denisov, Z. Chen et al., “Integration of speech separation,
diarization, and recognition for multi-speaker meetings: System de-
scription, comparison, and analysis,” in IEEE SLT, 2021.
[12] F. Yu, S. Zhang, P. Guo et al., “Summary on the icassp 2022 multi-
channel multi-party meeting transcription grand challenge,” in Proc.
ICASSP. IEEE, 2022, pp. 9156–9160.
[13] C. Boeddeker, A. S. Subramanian, G. Wichern et al., “Ts-sep: Joint
diarization and separation conditioned on estimated speaker embed-
dings,” arXiv preprint arXiv:2303.03849, 2023.
[14] S. Maiti, Y. Ueda, S. Watanabe et al., “EEND-SS: Joint end-to-end
neural speaker diarization and speech separation for flexible number of
speakers,” in IEEE SLT. IEEE, 2023, pp. 480–487.
[15] Z. Chen, T. Yoshioka, L. Lu et al., “Continuous speech separation:
Dataset and analysis,” in Proc. ICASSP, 2020.
[16] T. Yoshioka, X. Wang, D. Wang et al., “VarArray: Array-geometry-
agnostic continuous speech separation,” in Proc. ICASSP, 2022.
[17] G. Morrone, S. Cornell, D. Raj et al., “Low-latency speech separation
guided diarization for telephone conversations,” in 2022 IEEE Spoken
Language Technology Workshop (SLT). IEEE, 2023, pp. 641–646.
[18] N. Kanda, J. Wu, X. Wang et al., “VarArray meets t-SOT: Advancing
the state of the art of streaming distant conversational speech recogni-
tion,” ArXiv, 2022.
[19] M. Kolbæk, D. Yu, Z.-H. Tan and J. Jensen, “Multitalker speech sepa-
ration with utterance-level permutation invariant training of deep recur-
rent neural networks,” IEEE/ACM Transactions on Audio, Speech, and
Language Processing, vol. 25, no. 10, pp. 1901–1913, 2017.
[20] N. Kanda, Y. Gaur, X. Wang et al., “Serialized output training for end-
to-end overlapped speech recognition,” in Proc. InterSpeech, 2020.
[21] X. Chang, W. Zhang, Y. Qian et al., “End-to-end multi-speaker speech
recognition with transformer,” in Proc. ICASSP. IEEE, 2020, pp. 6134–
6138.
[22] L. Lu, N. Kanda, J. Li and Y. Gong, “Streaming end-to-end multi-talker
speech recognition,” IEEE SPL, vol. 28, 2021.
[23] D. Raj, L. Lu, Z. Chen et al., “Continuous streaming multi-talker asr
with dual-path transducers,” in Proc. ICASSP. IEEE, 2022, pp. 7317–
7321.
[24] N. Kanda, J. Wu, Y. Wu et al., “Streaming multi-talker ASR with token-
level serialized output training,” in Proc. InterSpeech, 2022.
[25] C. Boeddeker, J. Heitkaemper, J. Schmalenstroeer et al., “Front-end
processing for the CHiME-5 dinner party scenario,” in CHiME5 Work-
shop, 2018.
[26] M. Delcroix, K. Zmolikova, K. Kinoshita et al., “Single channel tar-
get speaker extraction and recognition with speaker beam,” in Proc.
ICASSP, 2018.
[27] N. Kanda, Y. Gaur, X. Wang et al., “Joint speaker counting, speech
recognition, and speaker identification for overlapped speech of any
number of speakers,” in Proc. InterSpeech, 2020.
[28] N. Kanda, X. Chang, Y. Gaur et al.,
“Investigation of end-to-end
speaker-attributed asr for continuous multi-talker recordings,” in Proc.
SLT, 2021.
[29] N. Kanda, J. Wu, Y. Wu et al., “Streaming speaker-attributed asr with
token-level speaker embeddings,” Proc. InterSpeech, 2022.
[30] T. Moriya, H. Sato, T. Ochiai et al.,
“Streaming end-to-end target
speaker ASR,” in Proc. InterSpeech, 2022.
[31] Z. Huang, D. Raj, P. Garc´ıa and S. Khudanpur,
“Adapting self-
supervised models to multi-talker speech recognition using speaker em-
beddings,” in Proc. ICASSP, 2023.
[32] N. Kanda, X. Xiao, Y. Gaur et al.,
“Transcribe-to-diarize: Neural
speaker diarization for unlimited number of speakers using end-to-end
speaker-attributed ASR,”
in Proc. ICASSP. IEEE, 2022, pp. 8082–
8086.
[33] S. Chen, C. Wang, Z. Chen et al., “WavLM: Large-scale self-supervised
pre-training for full stack speech processing,” IEEE Journal of Selected
Topics in Signal Processing, vol. 16, no. 6, 2022.
[34] A. Radford, J. W. Kim, T. Xu et al., “Robust speech recognition via
large-scale weak supervision,” ArXiv, 2022.
[35] T. von Neumann, C. Boeddeker, M. Delcroix and R. Haeb-Umbach,
“Meeteval: A toolkit for computation of word error rates for meeting
transcription systems,” arXiv preprint arXiv:2307.11394, 2023.
[36] K. Kinoshita, M. Delcroix and N. Tawara, “Integrating end-to-end neu-
ral and clustering-based diarization: Getting the best of both worlds,”
in Proc. ICASSP, 2021.
[37] K. Kinoshita, M. Delcroix and N. Tawara, “Advances in integration
of end-to-end neural and clustering-based diarization for real conversa-
tional speech,” Proc. InterSpeech, 2021.
[38] J. Carletta, S. Ashby, S. Bourban et al., “The AMI meeting corpus:
A pre-announcement,” in International workshop on machine learning
for multimodal interaction, 2005.
[39] F. Landini, J. Profant, M. Diez and L. Burget, “Bayesian hmm cluster-
ing of x-vector sequences (vbx) in speaker diarization: theory, imple-
mentation and analysis on standard tasks,” Computer Speech & Lan-
guage, vol. 71, pp. 101254, 2022.
[40] X. Chang, T. Maekaku, Y. Fujita and S. Watanabe, “End-to-end inte-
gration of speech recognition, speech enhancement, and self-supervised
learning representation,” in Proc. InterSpeech, 2022.
[41] A. Graves, S. Fern´andez, F. Gomez and J. Schmidhuber, “Connection-
ist temporal classification: labelling unsegmented sequence data with
recurrent neural networks,” in ICML, 2006.
[42] T. Kudo and J. Richardson, “Sentencepiece: A simple and language in-
dependent subword tokenizer and detokenizer for neural text process-
ing,” in Proceedings of the 2018 Conference on Empirical Methods
in Natural Language Processing: System Demonstrations, 2018, pp.
66–71.
[43] D. Raj, Z. Huang and S. Khudanpur, “Multi-class spectral clustering
with overlaps for speaker diarization,” in Proc. SLT, 2021.
[44] V. Panayotov, G. Chen, D. Povey and S. Khudanpur, “LibriSpeech: an
ASR corpus based on public domain audio books,” in Proc. ICASSP,
2015.
[45] G. Dekkers, S. Lauwereins, B. Thoen et al., “The sins database for
detection of daily activities in a home environment using an acoustic
sensor network,” DCASE Workshop, pp. 1–5, 2017.
[46] R. Scheibler, E. Bezzam and I. Dokmani´c,
“Pyroomacoustics: A
python package for audio room simulation and array processing algo-
rithms,” in Proc. of ICASSP. IEEE, 2018, pp. 351–355.
[47] O. Kuchaiev et al., “NeMo: a toolkit for building AI applications using
neural modules,” in Proc. Systems for ML Worshop, NeurIPS, 2019.
[48] L. Brandschain, D. Graff, C. Cieri et al., “The mixer 6 corpus: Re-
sources for cross-channel and text independent speaker recognition,”
in LREC, 2010.
[49] D. P. Kingma and J. Ba, “Adam: a method for stochastic optimization,”
in ICLR, 2014.
[50] D. S. Park, Y. Zhang, C.-C. Chiu et al., “Specaugment on large scale
datasets,” in Proc. of ICASSP. IEEE, 2020, pp. 6879–6883.

