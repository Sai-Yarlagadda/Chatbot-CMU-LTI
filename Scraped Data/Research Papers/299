arXiv:2307.12767v1  [eess.AS]  24 Jul 2023
Integration of Frame- and Label-synchronous Beam Search for
Streaming Encoder‚Äìdecoder Speech Recognition
Emiru Tsunoo1, Hayato Futami1, Yosuke Kashiwagi1, Siddhant Arora2, Shinji Watanabe2
1Sony Group Corporation, Japan
2Carnegie Mellon University, U.S.A.
emiru.tsunoo@sony.com
Abstract
Although frame-based models, such as CTC and transduc-
ers, have an afÔ¨Ånity for streaming automatic speech recogni-
tion, their decoding uses no future knowledge, which could
lead to incorrect pruning.
Conversely, label-based attention
encoder‚Äìdecoder mitigates this issue using soft attention to
the input, while it tends to overestimate labels biased towards
its training domain, unlike CTC. We exploit these comple-
mentary attributes and propose to integrate the frame- and
label-synchronous (F-/L-Sync) decoding alternately performed
within a single beam-search scheme. F-Sync decoding leads
the decoding for block-wise processing, while L-Sync decod-
ing provides the prioritized hypotheses using look-ahead future
frames within a block. We maintain the hypotheses from both
decoding methods to perform effective pruning. Experiments
demonstrate that the proposed search algorithm achieves lower
error rates compared to the other search methods, while being
robust against out-of-domain situations.
Index Terms: speech recognition, beam search, attention-based
encoder‚Äìdecoder, CTC
1. Introduction
Streaming style automatic speech recognition (ASR) is essential
for better user experiences. Among end-to-end ASR models,
connectionist temporal classiÔ¨Åcation (CTC) [1‚Äì4] and trans-
ducers [5‚Äì7] successfully model the temporal phenomenon of
speech in frame-by-frame computation. These models are re-
ferred to as frame-synchronous (F-Sync) models and they have
an afÔ¨Ånity for streaming processing [8‚Äì10]. Conversely, the out-
put of ASR is a label-base, which is suitable for modeling with
label-synchronous (L-Sync) models, such as attention-based de-
coders (AttDecs) [11, 12] and language models (LMs) [13, 14].
L-Sync models estimate labels in an autoregressive manner with
the given context of the previously estimated output. Generally,
L-Sync models have a strong ability to model label sequences,
and they are used to improve the ASR performance through fu-
sion or rescoring [15‚Äì18].
During decoding, F-Sync models expand the hypotheses
at each time frame. F-Sync decoding can be efÔ¨Åciently com-
puted in a beam search by maintaining scores of hypotheses
ending with a blank token and non-blank tokens, respectively
[1]. However, the hypotheses are pruned at each time frame by
using only partial information from the input speech. This limi-
tation sometimes causes unreliable pruning. Conversely, during
L-Sync decoding, the hypotheses are extended token-by-token
by making soft attention to all the input and the previous out-
put sequence. Thus, this approach has an advantage in pruning
over F-Sync decoding without future information. However, it
has a label bias problem [19‚Äì21], where once the model over-
estimates a label, it is difÔ¨Åcult to downgrade it by the sufÔ¨Åx
distribution. Furthermore, it is difÔ¨Åcult to use the L-Sync mod-
els in streaming ASR, where block processing is used [10, 22]
and their decoding is generally complicated [23‚Äì25].
Both F-Sync and L-Sync decoding are complementary.
Several studies have attempted to combine F-Sync and L-Sync
decoding. Watanabe et al. proposed the joint model, where
AttDec and CTC are jointly trained, and CTC rescores the hy-
potheses generated by AttDec during inference. Sainath et al.
proposed two-pass decoding [17], in which the AttDec rescores
N-best list of the transducer‚Äôs hypotheses. Li et al. combined
separate F-Sync and L-Sync models where the former produces
a lattice and the latter rescores it [26]. Yan et al. introduce
AttDec score to F-Sync decoding of CTC in machine transla-
tion and speech translation tasks [27]. Additionally, [28, 29]
experimentally compared F-Sync and L-Sync decoding. How-
ever, in those studies, one is merely used for rescoring the hy-
potheses generated by the other beforehand, and none simulta-
neously considers the hypotheses from the individual decoding
methods.
This study exploits complementary attributes of the F-Sync
and L-Sync decoding, and proposes integrating both in a single
beam search scheme. To take advantage of F-Sync decoding,
which is easy to incorporate with block-wise streaming ASR,
the proposed beam search primarily runs in an F-Sync man-
ner. To mitigate the issue of unreliable partial F-Sync score
comparison, we employ the L-Sync beam search based on the
shortest token-length preÔ¨Åxes of the hypotheses. The selected
hypotheses by L-Sync are then preserved in the subsequent F-
Sync pruning steps and adjusted as the hypotheses expand, en-
suring that the most promising ones are maintained in the pro-
posed beam search. Experiments demonstrate that the proposed
search algorithm performs effectively pruning in the frame‚Äì
label grid search, and achieve lower error rates compared to the
other search methods in English and Japanese datasets. Evalu-
ation using various domains in each language indicates that the
proposed method is robust against out-of-domain situations.
2. F-Sync Decoding and L-Sync Decoding
for Streaming ASR
2.1. F-Sync decoding
F-Sync models include CTC [1‚Äì4] and transducer variants [5‚Äì
7], which are suitable for streaming ASR. In streaming ASR,
blockwise processing is widely used [10, 22]. Let Tb be the
last frame of b-th block. An acoustic representation sequence
HTb = {ht|1 ‚â§ t ‚â§ Tb} is extracted by an encoder from
speech input. CTC and transducers introduce a blank token,
œÜ, to align HTb with a different L-length of label sequence
YL = {yl|1 ‚â§ l ‚â§ L}.
The F-Sync model estimates œÜ-
augmented tokens zt ‚àà V ‚à™ {œÜ} at time frame t, where V is the
vocabulary. The estimated sequence ZT ‚Ä≤ = {zt|1 ‚â§ t ‚â§ T ‚Ä≤}
can be mapped to a label sequence using mapping function
F : ZT ‚Ä≤ ‚Üí YL. In the case of CTC, T ‚Ä≤ = Tb, and T ‚Ä≤ = Tb+L
for the transducers, which can emit tokens without consuming
frames. Based on ht and previous output label yl‚àí1, the F-sync
model, FSM(¬∑), calculates a posterior of zt.
qF(zt|ht, yl‚àí1) = FSM(ht, yl‚àí1)
(1)
For CTC, qF does not depend on the previous output. Probabil-
ity computation of Yl at t is efÔ¨Åciently performed as
pF(Yl|Ht) = Œ≥(Yl
(b), t) + Œ≥(Yl
(n), t)
(2)
Œ≥(Yl
(b), t) =
X
FF(Zt)=Yl:zt=œÜ
pF(Yl|Ht‚àí1)q(zt|ht, yl)
Œ≥(Yl
(n), t) =
X
FF(Zt)=Yl:zt=yl
pF(Yl‚àí1|Ht‚àí1)q(zt|ht, yl‚àí1),
where Y(b) is the label sequence ending with œÜ, and Y(n) is the
other [1].
F-Sync decoding is performed frame-by-frame.
At each
time t, probability (2) is used on a logarithmic scale as F-Sync
score Œ±F.
Œ±F(Yl, t) = log pF(Yl|Ht)
(3)
The hypotheses are copied by blank transition or expanded at
each time step with FSync : Yl ‚Üí Yl, Yl+1. In the beam
search with a Ô¨Åxed beam size B, the topB(¬∑) function returns a
set of the top B hypotheses at step t, denoted as ‚Ñ¶F,t:
‚Ñ¶F,t = topB(Œ±F(Y, t)|Y ‚àà FSync(Y))
(4)
Since the F-Sync decoding has no knowledge of future inputs
even when the block processing has certain look-ahead frames
[10, 25], i.e., the score is conditioned only on Ht at step t, it
sometimes incorrectly prune the correct hypothesis.
2.2. L-Sync models with blockwise streaming processing
We categorize AttDecs [11, 12] and LMs [13, 14] as L-Sync
models. At each step i, the L-Sync models predict the probabil-
ity distribution of the next label yi using previous output Yi‚àí1
and input HTb. L-Sync score Œ≤L is deÔ¨Åned as
Œ≤L(Yi) =
i
X
j=1
log pL(yj|Yj‚àí1, HTb).
(5)
For LM, pL(yj) does not depend on input HTb. All the hy-
potheses are simultaneously extended with LSync : Yi‚àí1 ‚Üí
Yi. Beam search at step i is performed to maintain top B hy-
potheses as in F-Sync decoding.
‚Ñ¶L,i = topB(Œ≤L(Yi)|Yi ‚àà LSync(Yi‚àí1))
(6)
This contextual dependency enables the L-Sync models to pro-
vide a richer representation of label sequences. However, L-
Sync models have the following two drawbacks:
‚Ä¢ Label bias problem: Once the model overestimates a label
biased toward its training domain, the following expansion
cannot easily recover it [19‚Äì21].
‚Ä¢ Endpoint problem: For streaming systems, the AttDec needs
to detect an endpoint, which is the point to stop decoding with
limited HTb. This requires additional mechanisms to detect
[23‚Äì25]. Once the decoding step i passes the endpoint, the
AttDec tends to emit unreliable tokens; thus it is better for the
AttDec to proceed decoding conservatively in the streaming
systems [30].
2.3. F-Sync and L-Sync score fusion
Because F-Sync and L-Sync models are complementary, the
aforementioned problems in F-Sync and L-Sync decoding can
be alleviated by fusing both scores in pruning.
2.3.1. Score fusion in F-Sync decoding
Typically, LMs are fused in the F-Sync decoding. As in [31,
32], the scores (e.g., Eqs. (3) and (5)) are simply combined with
weights Œª. In the case of CTC, the combined score is deÔ¨Åned as
sF(Yl, t) =ŒªctcŒ±ctc(Yl, t) + ŒªlmŒ≤lm(Yl)
+ ŒªattŒ≤att(Yl) + Œªlen|Yl|
(7)
The last term in (7) is a label reword, which uses the length
of hypothesis Yl. This mitigates unfair score comparison of
different length in the beam, which, however, requires careful
tuning.
2.3.2. Score fusion in L-Sync decoding
The joint model [33] has both CTC and AttDec modules, which
are trained in a multi-task learning framework. Its decoding is
based on L-Sync search led by the AttDec. During the decod-
ing, the score is a combination of the CTC, LM, and AttDec, as
follows:
sL(Yi, i) =ŒªctcŒ±ctc(Yi . . . , Tb) + ŒªlmŒ≤lm(Yi)
+ ŒªattŒ≤att(Yi) + Œªlen|Yi|
(8)
Œ±ctc(Yi . . . , Tb) =
X
yi+1‚ààV
Œ±ctc(Yi+1, Tb)
(9)
(9) is a CTC preÔ¨Åx score that accumulates probabilities of all
the sufÔ¨Åxes of Yi. The preÔ¨Åx scores are calculated over only
the hypotheses generated by the AttDec; thus, it is sometimes
difÔ¨Åcult to recover incorrect estimation of the AttDec by itself.
Note that when t = Tb and i = l = L, F-Sync score (3)
and the preÔ¨Åx score (9) become equivalent; thus F-Sync decod-
ing and L-Sync decoding eventually evaluate the same score,
sF(Y L, Tb) = sL(Y L, L).
3. Integrated Beam Search of the F-Sync
and L-Sync Decoding
As discussed in Sec. 2.1, the F-Sync beam search is likely to in-
correctly prune the hypothesis without considering future look-
ahead input, which can be prevented by the AttDec that uses all
Tb frames. Conversely, the label bias problems in the AttDec
(Sec. 2.2) can be mitigated by using the F-Sync models. There-
fore, we propose maintaining both the F-Sync and L-Sync hy-
potheses in extended beam B‚Ä≤ >= B. Those hypotheses are
generated individually from each decoding, and the step incre-
ment of t of the F-Sync decoding and i of the L-Sync decoding
are performed alternately in an integrated single search algo-
rithm, namely, FL-Sync beam search.
3.1. PreÔ¨Åx score fusion
In the FL-Sync beam search, We choose F-Sync to lead decod-
ing because it is suitable for streaming ASR. Thus, at each step
t, the hypotheses are copied and expanded based on F-Sync de-
coding as in Sec. 2.1. The score for hypothesis Yl is extended
from (7) and (8) to include both i and t as
sFL(Yl, i, t) =ŒªctcŒ±ctc(Yl, t) + ŒªlmŒ≤lm(Yi)
+ ŒªattŒ≤att(Yi) + Œªlen|Yi|.
(10)
# of token
time frame
L-Sync
beam search
F-Sync
beam search
Figure 1: F-Sync and L-Sync decoding process.

   
	
 

L-Sync
model
Figure 2: Prioritized L-Sync hypothesis (red box) and its suc-
cessors (blue circles). The score of the prioritized hypothesis is
compared to all scores of the successors to perform ancestor-
pruning.
While the Ô¨Åst term Œ±ctc deÔ¨Åned in (3) is computed for Yl, Œ≤lm
and Œ≤att are computed only for its preÔ¨Åx Yi, where i ‚â§ l is the
current L-Sync decoding step. The reason for using i instead of
l is that the token prediction of the AttDec becomes unreliable
when it exceeds the endpoint in streaming processing, as dis-
cussed in Sec. 2.2. Therefore, we avoid aggressively computing
the scores for the entire l labels for the score fusion and conser-
vatively use i instead. Thus, the integrated beam search aims to
perform efÔ¨Åcient pruning on this 2D grid space of (i, t).
Conversely, the L-Sync decoding expand all the i-length
preÔ¨Åx of the hypotheses in hypothesis set ‚Ñ¶FL,t, i.e., Yi ‚àà
Pfxi(‚Ñ¶FL,t), where Pfxi(¬∑) is an i‚àílength preÔ¨Åx extraction
function. Therefore, to perform L-Sync decoding, the minimum
length of the hypotheses needs to be min |Yl ‚àà ‚Ñ¶FL,t| ‚â• i.
The L-Sync decoding advances its step i ‚Üí i+1 once the min-
imum length becomes larger than i. A graphical explanation
of this is provided in Fig. 1. The red circles indicate that the
F-Sync beam search expands the hypotheses in each time step.
The L-Sync models add partial scores for the common length to
these hypotheses (blue bars). In the last step of F-Sync decod-
ing, Tb, the remaining steps of L-Sync for each hypothesis are
consumed, and Œ≤L(Yl) is applied to each hypothesis to deter-
mine the best candidate.
3.2. Prioritized hypotheses from L-Sync decoding
The L-Sync decoding generates hypotheses Yi
L based on (8) us-
ing all the input, including future look-ahead, which are use-
ful to complement the weakness of F-Sync decoding. How-
ever, the hypotheses are pruned based on (10), whose Ô¨Årst term,
Œ±ctc(Y l, t), uses only partial t-frame input unlike (8), and the
hypotheses can still be incorrectly pruned.
To prevent from
dropping L-Sync hypotheses Yi
L in the early stage, we priori-
tize them for survival. At step t, we Ô¨Årst preserve those priori-
tized hypotheses of L-Sync decoding, then F-Sync decoding is
performed to Ô¨Åll the remaining hypotheses in the beam B‚Ä≤, as
‚Ñ¶FL,t =topB(sL(Yi, i)|Yi ‚àà LSync(Pfxi‚àí1(‚Ñ¶FL,t‚àí1)))
‚à™ top(B‚Ä≤ ‚àí B)(sFL(Yl, i, t)|Yl ‚àà FSync(‚Ñ¶FL,t‚àí1))
(11)
Algorithm 1 FL-Sync beam search algorithm.
Input: last frame number of current block Tb, acoustic features HTb,
beam width for AttDec B, total beam width B‚Ä≤
Output: ‚Ñ¶FL,Tb: hypotheses for current block b
1: Initialize: y0 ‚Üê ‚ü®sos‚ü©, ‚Ñ¶FL,0 ‚Üê {y0}, t ‚Üê 1, i ‚Üê 1
2: while t < Tb do
3:
ÀÜ‚Ñ¶ ‚Üê {}
4:
if i < min(|Y ‚àà ‚Ñ¶FL,t‚àí1|) then
5:
i ‚Üê min(|Y ‚àà ‚Ñ¶FL,t‚àí1|)
6:
ÀÜ‚Ñ¶ ‚Üê LSync(Pfxi‚àí1(‚Ñ¶FL,t‚àí1)) ‚ä≤ PreÔ¨Åx L-Sync search
7:
ÀÜ‚Ñ¶ ‚Üê topB(sL(Yi, i)|Yi ‚àà ÀÜ‚Ñ¶)
‚ä≤ Keep top-B hyps
8:
end if
9:
ÀÜ‚Ñ¶ ‚Üê ÀÜ‚Ñ¶ ‚à™ FSync(‚Ñ¶FL,t‚àí1)
‚ä≤ FSync search
10:
ÀÜ‚Ñ¶ ‚Üê AncestorPruning(ÀÜ‚Ñ¶)
‚ä≤ Sec. 3.3
11:
‚Ñ¶FL,t ‚Üê OnlyPriority(ÀÜ‚Ñ¶)
‚ä≤ Sec. 3.1
12:
‚Ñ¶FL,t ‚Üê ‚Ñ¶FL,t ‚à™ top(B‚Ä≤ ‚àí |‚Ñ¶FL,t|)(sFL(Y, i, t)|Y ‚àà ÀÜ‚Ñ¶)
13:
t ‚Üê t + 1
14: end while
15: return ‚Ñ¶FL,Tb
3.3. Ancestor-pruning for the prioritized hypothesis
The priority of the hypothesis Yi
L is no longer valid once the L-
Sync decoding increase its step i ‚Üí i+1, as it generate new set
of hypotheses Yi+1
L
. Further, to prune unnecessary hypotheses
of L-Sync decoding, we propose to perform ancestor-pruning,
which is similar to the depth-pruning in [32]. As the F-Sync
decoding proceed, it expands the hypothesis Yi ‚Üí Yi+1, as
shown in Fig. 2. The prioritized root hypothesis Yi is indicated
by the red box and the expanded successors Yi+‚àó are indicated
by the blue circles. We compare the scores of all the succes-
sors and the root hypothesis, denoted as sFL in Fig. 2. When
all the scores of successors become greater than the score of
the root, sFL(Yi
L, i, t) < min(sFL(Yi+‚àó, i, t)), we prune away
prioritized root hypothesis Yi
L, because any new successor from
the root Yi
L can no longer achieve higher score than the current
successors due to the 0 ‚â§ q(zt|ht, yi) ‚â§ 1 restriction.
3.4. FL-Sync beam search algorithm
The proposed beam search is summarized in Algorithm 1. As
the FL-Sync decoding is performed based on the F-Sync frame-
wise pruning, t is increased at each step in the loop. First, L-
Sync decoding is performed if step i < min(|Y ‚àà ‚Ñ¶FL,t‚àí1|)
as descried in Sec. 3.1. The L-Sync decoding expands the pre-
Ô¨Åxes of the hypotheses (Line 6) and provides prioritized hy-
potheses Yi
L, which are maintained in temporary hypothesis
set ÀÜ‚Ñ¶ (Line 7). Then the F-Sync decoding is performed based
on (10), and merged with the prioritized hypotheses (Line 9).
Subsequently, in Line 10, the ancestor-pruning described in
Sec. 3.3 is performed, which removes unnecessary hypotheses.
The hypotheses from L-Sync decoding have priority for survival
(Line 11), then lastly the hypothesis set is Ô¨Ålled from the tempo-
rary set up to total beam size, B‚Ä≤ (Line 12). Note that, even the
total beam size increases from B to B‚Ä≤, computational impact
is marginal from B-beam L-Sync decoding, because the F-Sync
model, e.g., CTC, generally requires much less computation.
4. Experiments
To evaluate the effectiveness of the proposed FL-Sync beam
search, we evaluated cross-domain and intra-domain scenarios
in English and Japanese datasets.
Table 1: Comparison of searching algorithms of streaming ASR in English datasets in WER. All the models were trained with Lib-
rispeech. ID refers to in-domain. In all the decoding methods, the LM of the target domain is fused.
total
L-Sync
LS‚ÜíTEDLIUM3
LS‚ÜíSwitchboard
LS‚ÜíSTOP
Librispeech (ID)
beam size
beam size
dev
test
SW
CH
test-clean
test-other
Streaming L-Sync decoding [25]
5
13.7
13.1
30.2
35.7
14.5
3.0
8.1
CTC F-Sync decoding [32]
10
15.0
14.4
31.0
37.2
19.2
3.3
9.0
CTC+AttDec F-Sync decoding
10
14.6
14.0
31.4
36.6
19.4
3.1
8.6
Integ. FL-Sync decoding (proposed)
10
5
13.4
12.7
29.9
35.0
14.1
2.9
7.9
Table 2: Comparison of searching algorithms of streaming ASR in Japanese datasets in CER. All the models were trained with
CSJ+LaboroTV (LTV). ID refers to in-domain. In all the decoding methods, the general LM is fused.
total
L-Sync
CSJ+LTV
CSJ+LTV
CSJ+LTV
CSJ+LTV
CSJ (ID)
beam size
beam size
‚ÜíTEDxJP
‚ÜíNews
‚ÜíCMD1
‚ÜíCMD2
eval1
eval2
eval3
Streaming L-Sync decoding [25]
5
12.1
3.5
4.1
12.8
8.0
7.5
7.0
CTC F-Sync decoding [32]
10
12.4
4.9
5.1
10.1
8.7
7.8
7.3
CTC+AttDec F-Sync decoding
10
12.1
3.5
3.5
10.7
8.0
7.4
7.4
Integ. FL-Sync decoding (proposed)
10
5
11.9
3.0
3.2
9.6
7.9
7.3
7.1
4.1. Experimental setup
For the English evaluation, we trained a streaming encoder‚Äì
decoder ASR model following [25] with the Librispeech dataset
[34], a read speech corpus.
It was applied to three various
domains: The TED-LIUM 3 [35] dev/test set (a spontaneous
lecture style), Hub5‚Äô00 (telephony-style conversation) having
Switchboard (SWB) and CallHome (CHM) subsets, and voice-
command-style STOP dataset [36].
For Japanese, we trained a streaming ASR model with a
merged set of the lecture style CSJ [37] and LaboroTV cor-
pus of TV program [38]. We used three evaluation sets of the
CSJ dataset for intra-domain setup, and TEDxJP [38] for cross-
domain setup. To cover various range of domains, We also used
in-house evaluation data; 720 read news utterances (News) spo-
ken by four males and four females, 2,620 voice commands
(CMD1) and far-Ô¨Åeld 1,768 commands (CMD2), each spoken
by 10 hired speakers.
The input acoustic features were 80-dimensional Ô¨Ålter bank
features. The input features were applied mean normalization
with a sliding window. The encoder consisted of 12 blocks of
conformer [6]. The AttDec had six decoder blocks. Each block
consisted of four-head 256-unit attention layers and 2048-unit
feed-forward layers. Contextual block encoding [22] was ap-
plied to the encoder with a block size of 40, a shift size of 16,
and a look-ahead size of 16. The models were trained using
multitask learning with CTC loss [33], with a weight of 0.3.
For English evaluation, external LMs were trained using
each target dataset for adaptation. LMs were four-layer uni-
directional LSTM with 2048 units, using the byte-pair encod-
ing (BPE) subword tokenization with 5000 token classes. For
Japanese, we collected over 27 million sentences to train a gen-
eral LM with 10,000 BPE tokens, applied to all the tasks.
We compared our proposed FL-Sync decoding with stream-
ing L-Sync decoding [25], and CTC F-Sync decoding [32]. We
also evaluated AttDec score fusion for CTC F-Sync decoding
for comparison. For the CTC F-Sync decoding methods, Œªlm =
0.4 was used for English datasets, and Œªlm = 0.1 for Japanese,
instead. For the baseline L-Sync decoding and the proposed
FL-Sync decoding, we adopted (Œªlm, Œªctc) = (0.4, 0.4) for
English intra-domain, (0.6, 0.4) for English cross-domain, and
(0.3, 0.5) for Japanese experiments. Œªatt was set as (1 ‚àí Œªctc).
We adopted Œªlen = 1 for all the evaluation. We used B‚Ä≤ = 10
and B = 5 as a beam size for total FL-Sync decoding and L-
Sync decoding in it, respectively. For fair comparison, we used
the same beam sizes for the baseline approaches.
4.2. Results of English evaluation
The word error rate (WER) results are summarized in Table 1.
Streaming L-Sync decoding was generally better than CTC
F-Sync decoding, particularly in the voice-command STOP
dataset. This indicated that it is more effective in pruning to
use input information including future look-ahead frames. Al-
though the AttDec score fusion improved accuracy on CTC F-
Sync decoding, the proposed FL-Sync decoding showed large
improvement from those F-Sync decoding methods because it
also preserved hypotheses from the L-Sync decoding. When we
compare FL-Sync decoding with the L-Sync baseline decoding,
our proposed method performed robustly in the cross-domain
scenarios; for instance, WER was improved from 13.1% to
12.7% in TEDLIUM3 test set. Furthermore, we conÔ¨Årmed that
it did not degrade the intra-domain performance, or even ob-
served slight improvement from 8.1% to 7.9% in test-other, for
instance. Since the scores, (7), (8), and (10), eventually become
the same in all the search methods, the results show that our
method is effective in pruning because of maintaining hypothe-
ses from both L-Sync and F-Sync decoding.
4.3. Results of Japanese evaluation
We evaluated character error rates (CERs), which are listed in
Table 2. The results followed similar tendency to the English
evaluation.
As we used the general LM, it did not exactly
match to some of the target domains. As a result, we observed
that even in the source domain CSJ eval sets, CERs were rela-
tively higher than the other literature [25, 39]. Furthermore, the
baseline L-Sync decoding had difÔ¨Åculty in adapting to the tar-
get domains, in particular in CMD2 data, which tend not to be
grammatically structured. Since our proposed FL-Sync decod-
ing maintained both L-Sync and F-Sync decoding, it recovered
the domain bias and was robust to the cross-domain situation.
5. Conclusion
We have proposed a new FL-Sync beam search, which inte-
grates complementary F-Sync and Sync decoding to mitigate
the problems in both the decoding. The proposed beam search
primarily runs in an F-Sync manner to incorporate it with block-
wise streaming ASR. To overcome the unreliable partial F-Sync
score comparison in pruning, L-Sync decoding provide the pri-
oritized hypotheses with future look-ahead input frames, which
are also maintained in the beam to perform effective pruning.
6. References
[1]
A. Graves, S. Fern¬¥andez, F. Gomez, and J. Schmidhuber, ‚ÄúCon-
nectionist temporal classiÔ¨Åcation: Labelling unsegmented se-
quence data with recurrent neural networks,‚Äù in Proc. ICML,
2006, pp. 369‚Äì376.
[2]
A. Graves and N. Jaitly, ‚ÄúTowards end-to-end speech recog-
nition with recurrent neural networks,‚Äù in Proc. ICML, 2014,
pp. 1764‚Äì1772.
[3]
Y. Miao, M. Gowayyed, and F. Metze, ‚ÄúEESEN: End-to-end
speech recognition using deep RNN models and WFST-based
decoding,‚Äù in Proc. of ASRU Workshop, 2015, pp. 167‚Äì174.
[4]
D. Amodei et al., ‚ÄúDeep Speech 2: End-to-end speech recogni-
tion in English and Mandarin,‚Äù in Proc. ICML, vol. 48, 2016,
pp. 173‚Äì182.
[5]
A. Graves, A.-R. Mohamed, and G. Hinton, ‚ÄúSpeech recognition
with deep recurrent neural networks,‚Äù in Proc. ICASSP, 2013,
pp. 6645‚Äì6649.
[6]
A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu,
W. Han, S. Wang, Z. Zhang, Y. Wu, et al., ‚ÄúConformer:
Convolution-augmented transformer for speech recognition,‚Äù in
Proc. Interspeech, 2020, pp. 5036‚Äì5040.
[7]
Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo,
and S. Kumar, ‚ÄúTransformer transducer: A streamable speech
recognition model with transformer encoders and RNN-T loss,‚Äù
in Proc. ICASSP, 2020, pp. 7829‚Äì7833.
[8]
L. Dong, F. Wang, and B. Xu, ‚ÄúSelf-attention aligner: A latency-
control end-to-end model for ASR using self-attention network
and chunk-hopping,‚Äù in Proc. ICASSP, 2019, pp. 5656‚Äì5660.
[9]
J. Yu, C.-C. Chiu, B. Li, S.-y. Chang, T. N. Sainath, Y. He, A.
Narayanan, W. Han, A. Gulati, Y. Wu, et al., ‚ÄúFastEmit: Low-
latency streaming ASR with sequence-level emission regulariza-
tion,‚Äù in Proc. ICASSP, 2021, pp. 6004‚Äì6008.
[10]
Y. Shi, Y. Wang, C. Wu, C.-F. Yeh, J. Chan, F. Zhang, D. Le,
and M. Seltzer, ‚ÄúEmformer: EfÔ¨Åcient memory transformer based
acoustic model for low latency streaming speech recognition,‚Äù in
Proc. ICASSP, 2021, pp. 6783‚Äì6787.
[11]
J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Ben-
gio, ‚ÄúAttention-based models for speech recognition,‚Äù in Proc.
of NIPS, 2015, pp. 577‚Äì585.
[12]
W. Chan, N. Jaitly, Q. Le, and O. Vinyals, ‚ÄúListen, attend and
spell: A neural network for large vocabulary conversational
speech recognition,‚Äù in Proc. ICASSP, 2016, pp. 4960‚Äì4964.
[13]
T. Mikolov, M. KaraÔ¨Å¬¥at, L. Burget, J. Cernock`y, and S. Khudan-
pur, ‚ÄúRecurrent neural network based language model,‚Äù in Proc.
Interspeech, 2010, pp. 1045‚Äì1048.
[14]
K. Irie, A. Zeyer, R. Schl¬®uter, and H. Ney, ‚ÄúLanguage modeling
with deep transformers,‚Äù in Proc. Interspeech, 2019, pp. 3905‚Äì
3909.
[15]
J. Chorowski and N. Jaitly, ‚ÄúTowards better decoding and lan-
guage model integration in sequence to sequence models,‚Äù in
Proc. Interspeech, 2017, pp. 523‚Äì527.
[16]
A. Kannan, Y. Wu, P. Nguyen, T. N. Sainath, Z. Chen, and
R. Prabhavalkar, ‚ÄúAn analysis of incorporating an external lan-
guage model into a sequence-to-sequence model,‚Äù in Proc.
ICASSP, 2018, pp. 1‚Äì5828.
[17]
T. N. Sainath, R. Pang, D. Rybach, Y. He, R. Prabhavalkar, W.
Li, M. Visontai, Q. Liang, T. Strohman, Y. Wu, et al., ‚ÄúTwo-
pass end-to-end speech recognition,‚Äù in Proc. Interspeech, 2019,
pp. 2773‚Äì2777.
[18]
W. Zhou, S. Berger, R. Schl¬®uter, and H. Ney, ‚ÄúPhoneme based
neural transducer for large vocabulary speech recognition,‚Äù in
Proc. ICASSP, 2021, pp. 5644‚Äì5648.
[19]
J. Lafferty, A. McCallum, and F. C. Pereira, ‚ÄúConditional ran-
dom Ô¨Åelds: Probabilistic models for segmenting and labeling se-
quence data,‚Äù in Proc. ICML, 2001, pp. 282‚Äì289.
[20]
S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, ‚ÄúScheduled
sampling for sequence prediction with recurrent neural net-
works,‚Äù in Proc. NeurIPS, vol. 28, 2015.
[21]
K. Murray and D. Chiang, ‚ÄúCorrecting length bias in neural ma-
chine translation,‚Äù arXiv preprint arXiv:1808.10006, 2018.
[22]
E. Tsunoo, Y. Kashiwagi, T. Kumakura, and S. Watanabe,
‚ÄúTransformer ASR with contextual block processing,‚Äù in Proc.
of ASRU Workshop, 2019, pp. 427‚Äì433.
[23]
N. Moritz, T. Hori, and J. Le Roux, ‚ÄúTriggered attention for end-
to-end speech recognition,‚Äù in Proc. ICASSP, 2019, pp. 5666‚Äì
5670.
[24]
M. Li, C. ZorilÀòa, and R. Doddipatla, ‚ÄúHead-synchronous decod-
ing for transformer-based streaming ASR,‚Äù in Proc. ICASSP,
2021, pp. 5909‚Äì5913.
[25]
E. Tsunoo, C. Narisetty, M. Hentschel, Y. Kashiwagi, and
S. Watanabe, ‚ÄúRun-and-back stitch search: Novel block syn-
chronous decoding for streaming encoder-decoder ASR,‚Äù in
Proc. ICASSP, 2022, pp. 8287‚Äì8291.
[26]
Q. Li, C. Zhang, and P. C. Woodland, ‚ÄúCombining frame-
synchronous and label-synchronous systems for speech recog-
nition,‚Äù arXiv preprint arXiv:2107.00764, 2021.
[27]
B. Yan, S. Dalmia, Y. Higuchi, G. Neubig, F. Metze, A. W.
Black, and S. Watanabe, ‚ÄúCTC alignments improve autoregres-
sive translation,‚Äù arXiv preprint arXiv:2210.05200, 2022.
[28]
L. Dong, C. Yi, J. Wang, S. Zhou, S. Xu, X. Jia, and B. Xu,
‚ÄúA comparison of label-synchronous and frame-synchronous
end-to-end models for speech recognition,‚Äù arXiv preprint
arXiv:2005.10113, 2020.
[29]
W. Zhou, A. Zeyer, A. Merboldt, R. Schl¬®uter, and H. Ney,
‚ÄúEquivalence of segmental and neural transducer modeling: A
proof of concept,‚Äù in Proc. Interspeech, 2021, pp. 2891‚Äì2895.
[30]
E. Tsunoo, Y. Kashiwagi, and S. Watanabe, ‚ÄúStreaming trans-
former ASR with blockwise synchronous beam search,‚Äù in Proc.
SLT, 2021, pp. 22‚Äì29.
[31]
A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E.
Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates, et al.,
‚ÄúDeep speech: Scaling up end-to-end speech recognition,‚Äù arXiv
preprint arXiv:1412.5567, 2014.
[32]
K. Hwang and W. Sung, ‚ÄúCharacter-level incremental speech
recognition with recurrent neural networks,‚Äù in Proc. ICASSP,
2016, pp. 5335‚Äì5339.
[33]
S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and T. Hayashi,
‚ÄúHybrid CTC/attention architecture for end-to-end speech
recognition,‚Äù Journal of Selected Topics in Signal Processing,
vol. 11, no. 8, pp. 1240‚Äì1253, 2017.
[34]
V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, ‚ÄúLib-
riSpeech: An ASR corpus based on public domain audio books,‚Äù
in Proc. ICASSP, 2015, pp. 5206‚Äì5210.
[35]
F. Hernandez, V. Nguyen, S. Ghannay, N. Tomashenko, and Y.
Esteve, ‚ÄúTED-LIUM 3: Twice as much data and corpus repar-
tition for experiments on speaker adaptation,‚Äù in International
conference on speech and computer, 2018, pp. 198‚Äì208.
[36]
P. Tomasello, A. Shrivastava, D. Lazar, P.-C. Hsu, D. Le, A.
Sagar, A. Elkahky, J. Copet, W.-N. Hsu, Y. Adi, et al., ‚ÄúSTOP:
A dataset for spoken task oriented semantic parsing,‚Äù in Proc.
SLT, 2023, pp. 991‚Äì998.
[37]
K. Maekawa, H. Koiso, S. Furui, and H. Isahara, ‚ÄúSpontaneous
speech corpus of Japanese,‚Äù in Proc. of the International Con-
ference on Language Resources and Evaluation (LREC), 2000,
pp. 947‚Äì9520.
[38]
S. Ando and H. Fujihara, ‚ÄúConstruction of a large-scale Japanese
ASR corpus on TV recordings,‚Äù in Proc. ICASSP, 2021,
pp. 6948‚Äì6952.
[39]
S. Karita, Y. Kubo, M. A. U. Bacchiani, and L. Jones, ‚ÄúA
comparative study on neural architectures and training methods
for Japanese speech recognition,‚Äù in Proc. Interspeech, 2021,
pp. 2092‚Äì2096.

