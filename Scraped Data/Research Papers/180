Latent Positional Information is in the Self-Attention Variance of
Transformer Language Models Without Positional Embeddings
Ta-Chung Chi†
Carnegie Mellon University
Ting-Han Fan
Princeton University
Li-Wei Chen
Carnegie Mellon University
Alexander I. Rudnicky
Carnegie Mellon University
Peter J. Ramadge
Princeton University
Abstract
The use of positional embeddings in trans-
former language models is widely accepted.
However, recent research has called into ques-
tion the necessity of such embeddings. We
further extend this inquiry by demonstrating
that a randomly initialized and frozen trans-
former language model, devoid of positional
embeddings, inherently encodes strong posi-
tional information through the shrinkage of self-
attention variance. To quantify this variance,
we derive the underlying distribution of each
step within a transformer layer. Through empir-
ical validation using a fully pretrained model,
we show that the variance shrinkage effect still
persists after extensive gradient updates. Our
findings serve to justify the decision to discard
positional embeddings and thus facilitate more
efficient pretraining of transformer language
models.
1
Introduction & Related Work
Transformer models have become the backbone of
natural language processing applications (Vaswani
et al., 2017; Devlin et al., 2019; Radford et al.,
2019). Within the transformer architecture, there
are two main categories: 1) bidirectional models,
such as BERT (Devlin et al., 2019), that are trained
using the masked language modeling objective, and
2) (causal) language models, such as GPT (Radford
et al., 2019), that are trained using the traditional
language modeling objective. Both of these cate-
gories share the common feature of using positional
embeddings for encoding token distance.
Whether positional embeddings are truly essen-
tial has been a subject of ongoing research. While
they have been considered necessary for bidirec-
tional transformer models (Lee et al., 2019; Luo
et al., 2021; Sinha et al., 2021; Haviv et al., 2022),
the situation is different for transformer language
models (Irie et al., 2019; Yang et al., 2019; Tsai
†Correspondence to: tachungc@andrew.cmu.edu
Layer norm
Multi-Head
Attention
FFN
Addition
Input
Layer norm
Addition
Layer 2
Layer 3
Layer 4
Layer norm
Output
Input
Layer 1
Figure 1: The architecture of a Pre-LN transformer lan-
guage model. All the parameters are randomly initial-
ized and randomly sampled input is used in this work.
et al., 2019; Scao et al., 2022; Haviv et al., 2022).
In transformer language models, the removal of
positional embeddings results in only a marginal
decline in performance, while enabling more ef-
ficient training (Haviv et al., 2022). In addition
to empirical evidence, it has been proven (Bhat-
tamishra et al., 2020) that transformer language
models without positional embeddings are Turing-
complete and able to model sequences akin to re-
current neural networks (Rumelhart and McClel-
land, 1987; Jordan, 1986). Despite this, it remains
an open question where positional information is
stored in the absence of positional embeddings.
This motivates further investigation into individual
operations within a transformer layer.
The example architecture of a pre-LN (Xiong
et al., 2020) multi-layer transformer language
model with no positional embeddings used in this
arXiv:2305.13571v1  [cs.CL]  23 May 2023
40
50
60
70
80
90
100
0
5
10
15
20
25
MAE
Number of Layers
Figure 2: We plot the positions w.r.t their mean absolute
error (MAE) for input sequence length L = 512. A
naive baseline of predicting the middle point of L = 256
gives an MAE of 128. The numbers are the average of
5 seeds.
work is shown in Figure 1.1 We hereinafter refer
to this configuration as TLM. Our primary focus
is on the multi-head attention (MHA) module of a
randomly initialized TLM, as it is the only module
that allows inter-token information exchange. To
gain a deeper understanding, we compute the mean
and variance of MHA outputs. To our surprise, we
discover that the variance already encodes latent
positional information, with later tokens in a se-
quence displaying smaller variance. This motivates
us to quantify the variance by deriving the output
distribution after MHA operations. Finally, through
empirical validation using a fully pre-trained TLM,
we confirm thatthe same variance shrinkage effect
persists after extensive gradient updates.
To the best of our knowledge, we are the first
to identify and quantify the latent positional infor-
mation in TLMs. Our results provide theoretical
insights into the removal of positional embeddings,
enabling more efficient pretraining of future TLMs.
2
Probing Experiments
Given BERT and TLM (GPT) with positional em-
beddings removed, prior work (Haviv et al., 2022)
shows that only TLM is able to maintain the same
language modeling performance as its original ver-
sion with positional embeddings. The discrepancy
might be explained by the fact that only TLM en-
codes positional information within its layers, as
shown by the position probing experiment in Haviv
et al. (2022). Since both BERT and TLM have
access to the same semantic input and the only
difference is the use of causal attention masks in
TLM, we hypothesize that the positional informa-
1Post-LN places layer norm at different positions. It is the
configuration used in BERT (Devlin et al., 2019).
tion may be attributed to the interaction between
causal attention masks and the TLM architecture.
To further explore this hypothesis, we use a ran-
domly initialized and frozen TLM to eliminate any
semantic influence and focus solely on the archi-
tectural design. Additionally, to prevent the model
from memorizing the order of input sequences, we
do not perform embedding lookups and feed the
model with randomly sampled input vectors. A
trainable two-layer linear classifier with ReLU ac-
tivation in between was appended to the TLM to
probe the position of each token (further details can
be found in Appendix B). We plot the mean abso-
lute error (MAE) w.r.t the number of transformer
layers in Figure 2. The plot indicates a randomly
initialized and frozen TLM with randomly sampled
input vectors inherently provides positional infor-
mation, with an increase in the number of layers
resulting in higher probing performance. This sur-
prising outcome prompts further investigation into
the encoding of latent positional information inside
the TLM architecture.
3
Theoretical Analysis
We dissect the inner workings of a TLM by deriv-
ing the distribution of TLM operations in the hope
that they elucidate where the latent positional infor-
mation is stored. The derivation is made possible
thanks to the usage of a randomly initialized and
frozen TLM. We adopt the initialization settings
in accordance with those employed in GPT (Rad-
ford et al., 2019). WLOG, our derivation is limited
to the operations of the first layer in a TLM and
the FFN component is omitted (justified in §3.4).
The hyperparameters utilized in the simulations
are: hidden dimension d = 768, number of atten-
tion heads H = 12, head dimension d/H = 64,
sequence length L = 512, standard deviation for
initialization σ = 0.02. All proofs of lemmas are
deferred to Appendix A.
Given a sequence of randomly sampled input
embeddings {xm}L
m=1, where each element of
xm ∈ Rd is sampled i.i.d from N(0, σ2), a TLM
consists of the following operations:
3.1
Layer Normalization
For each input embedding xm, it computes the
sample mean and (biased) sample variance:
xm,: =
Pd
i=1 xmi
d
, S(xm,:) =
Pd
i=1(xmi − xm,:)2
d
0
100
200
300
400
500
Positions
0.0
0.2
0.4
0.6
0.8
1.0
Cumulative Attention Probability
Figure 3: We plot the positions w.r.t their cumulative
attention score for L = 512 averaged over 500 samples.
Then each entry i of xm, denoted as xmi, is nor-
malized by mean and variance to emi:
emi = xmi − xm,:
p
S(xm,:)
∗ γ + β
(∗)
≈ xmi − E[xmi]
p
V[xmi]
∼ N(0, 1),
where V[x] denotes the variance of x. Since the
initialization scheme sets γ = 1 and β = 0, (∗)
holds with sufficiently large d by the Law of large
numbers and the continuous mapping theorem.
3.2
Self Attention
Each attention head computes query, key, and value
vectors in R
d
H :
qm = Wqem, km = Wkem, vm = Wvem,
where Wq, Wk, Wv ∈ R
d
H ×d are matrices with
each element sampled i.i.d from N(0, σ2).
To be precise, most matrices (W (h)
q
, W (h)
k
,
W (h)
v
), vectors (q(h)
m , k(h)
m , v(h)
m ), and scalars (l(h)
mn,
a(h)
mn) are associated with a head number h. For
notation simplicity, we only show the dependency
on h when we need it.
Lemma 1. qm, km, and vm have zero mean and
(dσ2) · I covariance matrix.
The resulting vectors are processed by the self-
attention module for pre-Softmax logits:
lmn =
(
⟨qm, kn⟩,
if m ≥ n
− inf,
otherwise
0
1
2
3
4
5
6
Log Positions
8
7
6
5
4
3
Log Variance
Theoretical@Layer 0
Simulation@Layer 0
Simulation@Layer 5
Simulation@Layer 11
Figure 4: We plot the log positions (up to L = 512) w.r.t
their log variance under the assumption of Property 1.
The simulation aligns with the theoretical curve posited
by Lemma 3 at the 0th layer averaged over 500 samples.
followed by the scaled softmax normalization:
amn =
exp

lmn/
p
d/H

PL
i=1 exp

lmi/
p
d/H

Lemma 2. lmn has zero mean and d3σ4
H2 variance.
lmn/
p
d/H has d2σ4
H
variance.
The numerical variance of lmn/
p
d/H in our case
is 7682·0.024
12
≈ 0.0079. Lemma 2 suggests the
following approximation:
Property 1. When σ4 ≪ H
d2 , lm,: has small vari-
ance, making the attention weights am,: almost
evenly distributed among all positions.2
In Figure 3, we verify Property 1 by showing that
amn is almost evenly distributed in simulation.
Observe that the output vector om at position m
is:
om = Wo
 
⊕H
h=1
L
X
n=1
a(h)
mnv(h)
n
!
,
where ⊕ denotes the concatenation of vectors from
all H attention heads. Assume that Property 1
is valid and that Wo ∈ Rd×d has elements i.i.d
sampled from N(0, σ2), we derive the distribution
of om below.
Lemma 3. om has zero mean and d2σ4
m I covari-
ance matrix.
2This approximation was also used in Xiong et al. (2020)
except that they made a stronger assumption that Wq and Wk
have to be initialized as zero matrices.
0
1
2
3
4
5
6
Log Positions
15
10
5
0
5
Log Variance
Simulation@ =0.2
Theoretical@ =0.2
Simulation@ =0.02
Theoretical@ =0.02
Simulation@ =0.002
Theoretical@ =0.002
Figure 5: We vary the value of σ and show its effect at the 0th layer. As we can see, a smaller value of σ brings
Lemma 3 into alignment with the corresponding simulation more closely. Note that the two lines overlap completely
when σ = 0.002. Average of 500 samples.
Figure 4 is a simulation that verifies Lemma 3 un-
der the assumption of Property 1. We can see
that the variance of om already encodes the po-
sitional information m.
3.3
Residual Connection
As denoted by the Addition block of Figure 1, the
residual connection sets the output as ym = xm +
om. It allows the model to pass the first MHA
output to later MHA modules as well as the final
classifier. As the positional information has been
passed by the residual connection, we omit the FFN
part in our analysis.
3.4
The Final Layer Normalization
Layer normalization is an operation that might
eliminate the positional information derived in
Lemma 3, which happens before the MHA mod-
ules and position classifier. As mentioned in §3.1,
LN(ym) gives:
y′
mi ≈ ymi − E[ymi]
p
V[ymi]
≈ xmi + WoWv
Pm
n eni
m
q
σ2 + d2σ4
m
,
E[ymi] = 0, V[ymi] = V[xmi] + V[omi]
= σ2 + d2σ4
m
Lemma 4. The variance of the j-th dimension of
ym is:
mσ2 + P
i(Wo,j:Wv,:i)2
mσ2 + d2σ4
,
where Wo,j: ∈ R1×d is the j-th row of Wo.
Wv,:i ∈ Rd×1 is the i-th column of Wv. As long
as P
i(Wo,j:Wv,:i)2 ̸= d2σ4, the classifier should
be able to exploit the discrepancy to derive m.
Readers might wonder why Wo,j: and Wv,:i in
the numerator cannot be treated as random vari-
ables. The reason is that we only focus on one
dimension (j-th) at a time. This means we cannot
use the law of large numbers to approximate the
sample variance of ymj as we did for the denomi-
nator.
3.5
Relaxing the Assumptions
We discuss possible relaxation of the assumptions
used in §3.2.
What if Property 1 does not hold?
Or equiv-
alently, σ4 ̸≪
H
d2 . This prompts us to vary the
value of σ. In Figure 5, we see that smaller σ bet-
ter aligns Lemma 3 with the simulations, which is
unsurprising as Lemma 3 assumes small σ. Even
when σ is not too small (i.e., σ = 0.2, 0.02), the
variance still encodes the positional information as
the variance of om is negatively correlated with its
position m.
Other Initialization Schemes
So far we assume
the weight matrices (Wq, Wk, Wv, Wo) are ini-
tialized i.i.d from N(0, σ2). However, we can relax
the assumption to i.i.d. samples from a distribution
with zero mean and finite variance. This is because
the proof in Appendix A calculates the covariance.
The variance calculation relies on E[rir⊤
i ] = σ2I
where r⊤
i is the i-th row vector of a weight matrix.
This property holds for any distribution with zero
mean and σ2 variance.
4
Discussions
Why are the positions of later tokens in a se-
quence harder to be predicted in Figure 3 of Ha-
viv et al. (2022)?
Lemma 3 states the variance
is inversely proportional to the position m, so the
variance of later tokens (large m) plateaus, result-
ing in a harder numerical optimization problem.
This also suggests a potential downside of remov-
ing positional embeddings: It might be challenging
for the model to infer positional information of the
later tokens in extremely long input sequences.
Why do lower layers (closer to input) give worse
probing performances in both Figure 2 and Ha-
viv et al. (2022)?
This can be explained by Fig-
ure 4. Most of the positions at the 0th layer have
tiny variance (exp(−10) = 4.5e−5), which again
poses a difficult numerical optimization problem.
Why does BERT fail to converge without posi-
tional embeddings?
In a BERT model (Devlin
et al., 2019), each token has access to all the other
tokens, making the variance at all positions d2σ4
L .
Therefore, a BERT model cannot utilize variance
differences as its positional indicator.
5
Post-Training Results
Our derivations only apply to the initial stage where
the TLM and input embeddings are randomly ini-
tialized, which may not hold true after gradient up-
dates. It is essential to verify the existence of vari-
ance properties and lemmas on a fully pre-trained
TLM on OpenWebText2 (details in Appendix C).
We expect that the properties of lower layers of
a pre-trained TLM should align more closely with
the theoretical results for two reasons: 1) There
are more steps between the lower layers and the fi-
nal language modeling loss, resulting in smaller
gradients and thereby fewer parameter updates,
and 2) Lower layers typically encode more low-
level information dependent on positional infor-
mation (Vuli´c et al., 2020; de Vries et al., 2020).
Figures 6 and 7 demonstrate that the 0th (lowest)
layer exhibits highly similar cumulative attention
probability and decay-with-position variance as the
theoretical results. In contrast, higher layers deviate
from the analyses in § 3. We posit that the model
learns to rely more heavily on semantic rather than
positional information. This also explains why
0
100
200
300
400
500
Positions
0.2
0.4
0.6
0.8
1.0
Cumulative Attention Probability
Layer 0
Layer 6
Layer 11
Figure 6: We plot the positions w.r.t their cumulative
attention probability for L = 512 of a pre-trained TLM.
We average over all heads in a layer and 500 samples.
0
1
2
3
4
5
6
Log Positions
2
1
0
1
2
3
Log Variance
Layer 0
Layer 6
Layer 11
Figure 7: We plot the log positions w.r.t their log vari-
ance for L = 512 of a pre-trained TLM. We average
over 500 samples.
predicting positions using outputs of higher trans-
former layers is more challenging as demonstrated
in Figure 2 of Haviv et al. (2022).
6
Conclusion
We mathematically analyzed a randomly initialized
transformer language model without positional em-
beddings. We showed that the variance of the self-
attention output decreases as the position increases,
which serves as an indicator for positional infor-
mation. We validated that, after extensive gradient
updates, the low layers of a pretrained language
model still exhibit highly similar variance reduction
behaviors. Our results pave the way for the pretrain-
ing of more efficient and positional embedding-free
transformer language models.
Limitations
The limitations of this work mostly come from
our assumptions: 1) A randomly initialized and
frozen TLM, and 2) Input tokens are all different
and randomly sampled. These two assumptions
obviously do not hold true for human languages
and pre-trained TLMs. Therefore, we attempted
to empirically verify the existence of lemmas and
properties on a pre-trained TLM without positional
embeddings in §5.
That being said, several methods could be at-
tempted to remove these assumptions. Firstly, we
can analyze the training dynamics of a TLM to shed
light on the model parameter distribution after pre-
training. Secondly, Zipf’s law or a simple n-gram
language model could be used to quantify the de-
gree of input token duplication in human languages.
This might give us a more accurate estimate of the
variance at different positions. We leave these ideas
as future work.
Ethics Statement
Our work provides a deeper understanding of why
a transformer language model can still perform
well without positional embeddings, potentially
enabling the application of developing future trans-
formers that are greener and more cost-efficient.
Inappropriate usage of our technique might have
negative societal impacts though. These include
the ethical challenges of improper text generation
and privacy issues inherent in the data collection
process. These implications apply to any natural
language processing research and are not unique to
this specific work.
Acknowledgment
The authors acknowledge the support from Boeing
(2019-STU-PA-259), Amazon (CC ADV 00474341
2021 TR), NSF MRI Award 1919452, and Prince-
ton Research Computing.
References
Alex Andonian, Quentin Anthony, Stella Biderman, Sid
Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh
Levy-Kramer, Connor Leahy, Lucas Nestler, Kip
Parker, Michael Pieler, Shivanshu Purohit, Tri Songz,
Wang Phil, and Samuel Weinbach. 2021. GPT-NeoX:
Large Scale Autoregressive Language Modeling in
PyTorch.
Satwik Bhattamishra, Arkil Patel, and Navin Goyal.
2020. On the computational power of transformers
and its implications in sequence modeling. In Pro-
ceedings of the 24th Conference on Computational
Natural Language Learning, pages 455–475, Online.
Association for Computational Linguistics.
Wietse de Vries, Andreas van Cranenburgh, and Malv-
ina Nissim. 2020. What’s so special about BERT’s
layers? a closer look at the NLP pipeline in mono-
lingual and multilingual models. In Findings of the
Association for Computational Linguistics: EMNLP
2020, pages 4339–4350, Online. Association for
Computational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
ing, Travis Hoppe, Charles Foster, Jason Phang,
Horace He, Anish Thite, Noa Nabeshima, Shawn
Presser, and Connor Leahy. 2020.
The Pile: An
800gb dataset of diverse text for language modeling.
arXiv preprint arXiv:2101.00027.
Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer
Levy. 2022. Transformer language models without
positional encodings still learn positional information.
arXiv preprint arXiv:2203.16634.
Kazuki Irie, Albert Zeyer, Ralf Schlüter, and Hermann
Ney. 2019. Language modeling with deep transform-
ers. In INTERSPEECH.
M I Jordan. 1986. Serial order: a parallel distributed pro-
cessing approach. technical report, june 1985-march
1986.
Diederik P. Kingma and Jimmy Ba. 2014.
Adam:
A method for stochastic optimization.
Cite
arxiv:1412.6980Comment: Published as a confer-
ence paper at the 3rd International Conference for
Learning Representations, San Diego, 2015.
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Ko-
siorek, Seungjin Choi, and Yee Whye Teh. 2019.
Set transformer: A framework for attention-based
permutation-invariant neural networks. In Proceed-
ings of the 36th International Conference on Ma-
chine Learning, volume 97 of Proceedings of Ma-
chine Learning Research, pages 3744–3753. PMLR.
Ziyang Luo, Artur Kulmizev, and Xiaoxi Mao. 2021.
Positional artefacts propagate through masked lan-
guage model embeddings. In Proceedings of the 59th
Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers), pages 5312–5327, Online. Association
for Computational Linguistics.
Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Te-
jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,
Junjie Bai, and Soumith Chintala. 2019. Pytorch:
An imperative style, high-performance deep learning
library. In Advances in Neural Information Process-
ing Systems 32, pages 8024–8035. Curran Associates,
Inc.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
David E. Rumelhart and James L. McClelland. 1987.
Learning Internal Representations by Error Propa-
gation, pages 318–362.
Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile
Saulnier, Stas Bekman, M Saiful Bari, Stella Bider-
man, Hady Elsahar, Jason Phang, Ofir Press, Colin
Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika,
Jaesung Tae, Zheng Xin Yong, Julien Launay, and
Iz Beltagy. 2022. What language model to train if
you have one million GPU hours? In Challenges &
Perspectives in Creating Large Language Models.
Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle
Pineau, Adina Williams, and Douwe Kiela. 2021.
Masked language modeling and the distributional hy-
pothesis: Order word matters pre-training for little.
In Proceedings of the 2021 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2888–2913, Online and Punta Cana, Dominican Re-
public. Association for Computational Linguistics.
Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada,
Louis-Philippe Morency, and Ruslan Salakhutdinov.
2019. Transformer dissection: An unified under-
standing for transformer’s attention via the lens of
kernel. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
4344–4353, Hong Kong, China. Association for Com-
putational Linguistics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.
Ivan Vuli´c, Edoardo Maria Ponti, Robert Litschko,
Goran Glavaš, and Anna Korhonen. 2020. Probing
pretrained language models for lexical semantics. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 7222–7240, Online. Association for Computa-
tional Linguistics.
Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng,
Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan
Lan, Liwei Wang, and Tie-Yan Liu. 2020. On layer
normalization in the transformer architecture. In In-
ternational Conference on Machine Learning.
Baosong Yang, Longyue Wang, Derek F. Wong, Lidia S.
Chao, and Zhaopeng Tu. 2019. Assessing the ability
of self-attention networks to learn word order. In
Proceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 3635–
3644, Florence, Italy. Association for Computational
Linguistics.
A
Proofs
The proof of Lemma 1 and 2 are head-dependent
while that of Lemma 3 is head-independent. For
notation simplicity, at Lemma 1 and 2, we drop
the head dependency on matrices (W (h)
q
, W (h)
k
,
W (h)
v
), vectors (q(h)
m , k(h)
m , v(h)
m ), and scalars (l(h)
mn,
a(h)
mn).
Proof of Lemma 1
Here, we use r⊤
i to denote
the i-th row vector of Wv.
cov(vm, vn) = E[vmv⊤
n ]
= E[Wveme⊤
n W ⊤
v ]
= E




r⊤
1 em
...
r⊤
d
H em


h
e⊤
n r1
. . .
e⊤
n r d
H
i


=
h
E[r⊤
i eme⊤
n rj]
i d
H
i,j=1
=
h
E[Tr(rjr⊤
i eme⊤
n )]
i d
H
i,j=1
=
h
Tr(E[rjr⊤
i ]E[eme⊤
n ])
i d
H
i,j=1
(∗)
=

Tr((1i=jσ2) · Id · 1m=n · Id)
 d
H
i,j=1
=

1i=j1m=ndσ2 d
H
i,j=1
= (1m=ndσ2) · Id/H
(∗) holds because ri and rj are independent when
i ̸= j (similarly for em and en) and the covariance
of a Gaussian random vector is an identity matrix.
Id and Id/H denote d × d and
d
H × d
H identity
matrices.
Proof of Lemma 2
Here, we use r⊤
i to denote
the i-th row vector of Wq and Wk.
cov(lmn, lmp)
= E[(e⊤
mW ⊤
q Wken)(e⊤
mW ⊤
q Wkep)⊤]
= E[Tr(e⊤
mW ⊤
q Wkene⊤
p W ⊤
k Wqem)]
= E[Tr(eme⊤
mW ⊤
q Wkene⊤
p W ⊤
k Wq)]
= Tr(E[eme⊤
m]E[W ⊤
q Wkene⊤
p W ⊤
k Wq])
= E[Tr(ene⊤
p W ⊤
k WqW ⊤
q Wk)]
= Tr(E[ene⊤
p ]E[W ⊤
k WqW ⊤
q Wk)])
= (1n=p)Tr(E[WqW ⊤
q ]E[WkW ⊤
k ])
(∗)
= (1n=p)Tr(( d
H σ2 · I)( d
H σ2 · I))
= (1n=p)d3σ4
H2
(∗) holds since:
E[WqW ⊤
q ] = E




r⊤
1...
r⊤
d
H


h
r1
. . .
r d
H
i


=
h
E[r⊤
i rj]
i d
H
i,j=1 = d
H σ2 · I
Proof of Lemma 3
Because Wo ∈ Rd×d is ap-
plied on a concatenation of vectors at all heads, we
take vi = ⊕H
h=1v(h)
i
. vi here is head-independent
while vi at Lemma 1 is head-dependent. Here, we
use r⊤
i to denote the i-th row vector of Wo.
cov(om, om)
Property 1
≈
E
"
Wo
Pm
i=1 vi
m
Pm
j=1 v⊤
j
m
W ⊤
o
#
= 1
m2
m
X
i,j=1
E[Woviv⊤
j W ⊤
o ]
= 1
m2
m
X
i,j=1
E




r⊤
1 vi
...
r⊤
d vi


v⊤
j r1
. . .
v⊤
j rd



= 1
m2
m
X
i,j=1
h
E[r⊤
k viv⊤
j rl]
id
k,l=1
= 1
m2
m
X
i,j=1
h
E[Tr(rlr⊤
k viv⊤
j )]
id
k,l=1
= 1
m2
m
X
i,j=1
h
Tr(E[rlr⊤
k ]E[viv⊤
j ])
id
k,l=1
(∗)
=
1
m2
m
X
i,j=1

Tr((1k=lσ2) · I · (1i=jdσ2) · I)
d
k,l=1
= d2σ4
m I
(∗)
follows
from
Lemma
1:
because
cov(v(h)
i
, v(h)
j
) = (1i=jdσ2) · Id/H, a concatena-
tion for all h ∈ H gives E[viv⊤
j ] = (1i=jdσ2) · Id.
B
Probing Experiment Details
We train a randomly initialized and frozen TLM
with 12 layers, d = 768, H = 12, L = 512, and
σ = 0.02. We use the Adam optimizer (Kingma
and Ba, 2014) with learning rate 1e − 3 and 5000
gradient updates. The batch size is set to 32. We
implement our model using PyTorch (Paszke et al.,
2019).
# Layers
Hidden Size # Attention Heads Train Seq. Len. # Trainable Params.
12
64
12
512
162M
Optimizer
Batch Size
Train Steps
Precision
Dataset
Adam (lr 6e-4)
32
50,000
bfloat16
OpenWebText2
Table 1: Pre-trained Model Configurations.
C
Pre-trained Transformer Language
Model Details
We use the gpt-neox library (Andonian et al., 2021)
to train a TLM with no positional embeddings. De-
tailed hyperparameters are listed in Table 1. The
pretraining takes 5 hours on one NVIDIA A100-
40GB.
D
Scientific Artifacts
We use the gpt-neox library (Andonian et al., 2021)
under Apache-2.0 license. OpenWebText2 (Gao
et al., 2020) is released by the authors of gpt-neox.
The codebase and dataset are publicly released for
research purposes. The steps taken to protect pri-
vacy and anonymization are discussed in Section
6 and 7 of Gao et al. (2020). The distribution
and statistics of OpenWebext2 are also discussed
in Gao et al. (2020).

