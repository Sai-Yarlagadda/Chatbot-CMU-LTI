# End-to-End-NLP-System
## Abstract
  As a part of this assignment we worked on building a Retrieval Augmented Generation system that is capable of answering questions related to various facts about Carnegie Mellon University(CMU) and Language Technologies Institute (LTI). This is one particular topic that requires domain knowledge related to CMU and LTI for answering questions. Our RAG system addresses LLM's domain knowledge limitations by compiling documents relevant to the questions at hand. The core of our endeavor involved extensive data preparation and annotation phase, employing both automated tools and manual efforts. Incorporating both a multiquery retriever and a reranker into the RAG system yielded statistically significant results when compared to utilizing only a retriever in the RAG framework. This model had given a F1 score of 0.4161, which is 1.57 times better than the F1 score of a retriever only model.

## What each folder contains:
1. Annotations folder: This folder contains the initial annotations we collected based on the data scraped from sources related to CMU and LTI department.  
2. Embeddings folder: This folder contains the script we used to make the embeddings.
3. Final_Annotations folder: This folder contains the final annotations we used to test our RAG pipeline on. This contains a total of 176 questions.
4. Generated Answers folder: This folder contains all the generated answers for the test set, generated by the RAG models we constructed.
   1. closed_book_model_answers.txt: Answers generated by LLAMA2-70B-chat only, without any RAG (For the test set we generated)
   2. rag_system_answers.txt: Answers generated by LLAMA2-70B-chat model, when we performed RAG, where we passed the top 3 documents to the model as context (For the test set we generated)
   3. rag_with_reranker_answers.txt: Answers generated by LLAMA2-70B-chat model, when we performed RAG, followed by reranking of the documents retrieved by RAG, and then passing the top 3 documents to the model as context (For the test set we generated)
   4. rag_with_reranker_with_multiquery_answers.txt: Answers generated by LLAMA2-70B-chat model, when we performed Multiquery retrieval, followed by reranking of the documents generated and then passing the top 3 documents to the model as context (For the test set we generated)
5. Postprocessing folder: Contains the script used to format the answers generated into the correct format of answers.
6. RAG Pipeline folder: Contains the scripts used for running our RAG pipeline.
7. Reports folder: This folder contains the documents related to Inter Annotator Agreement. We sampled a total of 40 questions for Inter-Annotator Agreement (IAA) assessment.
   1. IAA Report: Comparison report of Annotator 1 and Annotator 2.
   2. IAA_Ann_Annotator1: Annotations of Annotator 1.
   3. IAA_Ann_Annotator2: Annotations of Annotator 2.
   4. get_IAA_data: Contains the script we used for sampling questions.
8. Scraped Data/Research papers folder: Contains the data of all the research papers generated by all LTI faculties in the year 2023.
9. Scraped data folder: Contains all the data we scraped from websites.
10. Scraped_pdf_file folder: Contains all the data we scraped from pdf files.
11. Significance Testing: Includes both the report for significance testing and the documents used for conducting the significance test.
12. data/test: Contains the all the test questions and its corresponding reference answers.
13. evaluation_metrics: Contains the script used for evaluating the models (F1, exact match, precision and recall)
14. py_files_for_scraping: Contains all the scripts we used for collecting data.
15. system_outputs: Contains outputs of our top 3 models. (Based on the test set we were provided (https://github.com/neubig/nlp-from-scratch-assignment-spring2024/tree/main/data)
16. Contributions_md: Contains the contributions of every person in this project
